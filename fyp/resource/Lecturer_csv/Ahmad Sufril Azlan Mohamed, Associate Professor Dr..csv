"Authors","Author full names","Author(s) ID","Title","Year","Source title","Volume","Issue","Art. No.","Page start","Page end","Page count","Cited by","DOI","Link","Affiliations","Authors with affiliations","Abstract","Author Keywords","Index Keywords","Molecular Sequence Numbers","Chemicals/CAS","Tradenames","Manufacturers","Funding Details","Funding Texts","References","Correspondence Address","Editors","Publisher","Sponsors","Conference name","Conference date","Conference location","Conference code","ISSN","ISBN","CODEN","PubMed ID","Language of Original Document","Abbreviated Source Title","Document Type","Publication Stage","Open Access","Source","EID"
"Ab Wahab M.N.; Nazir A.; Ren A.T.Z.; Noor M.H.M.; Akbar M.F.; Mohamed A.S.A.","Ab Wahab, Mohd Nadhir (57223397087); Nazir, Amril (24723004500); Ren, Anthony Tan Zhen (57290466200); Noor, Mohd Halim Mohd (36656106400); Akbar, Muhammad Firdaus (57195979493); Mohamed, Ahmad Sufril Azlan (57190968285)","57223397087; 24723004500; 57290466200; 36656106400; 57195979493; 57190968285","Efficientnet-Lite and Hybrid CNN-KNN Implementation for Facial Expression Recognition on Raspberry Pi","2021","IEEE Access","9","","","134065","134080","15","36","10.1109/ACCESS.2021.3113337","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115174722&doi=10.1109%2fACCESS.2021.3113337&partnerID=40&md5=92c9705ca03f0589b4a0244d4eecce85","School of Computer Sciences, Universiti Sains Malaysia, Penang, Minden, 11800, Malaysia; Department of Information Systems and Technology Management, College of Technological Innovation, Zayed University, Abu Dhabi, United Arab Emirates; School of Electrical and Electronic Engineering, Engineering Campus, Universiti Sains Malaysia, Nibong Tebal, Penang, 14300, Malaysia","Ab Wahab M.N., School of Computer Sciences, Universiti Sains Malaysia, Penang, Minden, 11800, Malaysia; Nazir A., Department of Information Systems and Technology Management, College of Technological Innovation, Zayed University, Abu Dhabi, United Arab Emirates; Ren A.T.Z., School of Computer Sciences, Universiti Sains Malaysia, Penang, Minden, 11800, Malaysia; Noor M.H.M., School of Computer Sciences, Universiti Sains Malaysia, Penang, Minden, 11800, Malaysia; Akbar M.F., School of Electrical and Electronic Engineering, Engineering Campus, Universiti Sains Malaysia, Nibong Tebal, Penang, 14300, Malaysia; Mohamed A.S.A., School of Computer Sciences, Universiti Sains Malaysia, Penang, Minden, 11800, Malaysia","Facial expression recognition (FER) is the task of determining a person's current emotion. It plays an important role in healthcare, marketing, and counselling. With the advancement in deep learning algorithms like Convolutional Neural Network (CNN), the system's accuracy is improving. A hybrid CNN and k-Nearest Neighbour (KNN) model can improve FER's accuracy. This paper presents a hybrid CNN-KNN model for FER on the Raspberry Pi 4, where we use CNN for feature extraction. Subsequently, the KNN performs expression recognition. We use the transfer learning technique to build our system with an EfficientNet-Lite model. The hybrid model we propose replaces the Softmax layer in the EfficientNet with the KNN. We train our model using the FER-2013 dataset and compare its performance with different architectures trained on the same dataset. We perform optimization on the Fully Connected layer, loss function, loss optimizer, optimizer learning rate, class weights, and KNN distance function with the k-value. Despite running on the Raspberry Pi hardware with very limited processing power, low memory capacity, and small storage capacity, our proposed model achieves a similar accuracy of 75.26% (with a slight improvement of 0.06%) to the state-of-the-art's Ensemble of 8 CNN model. © 2013 IEEE.","EfficientNet-Lite; emotion recognition; facial expression recognition; hybrid CNN-KNN; Raspberry Pi","Convolutional neural networks; Deep learning; Face recognition; Learning systems; Nearest neighbor search; Transfer learning; Distance functions; Expression recognition; Facial expression recognition; K nearest neighbours (k-NN); Learning techniques; Processing power; State of the art; Storage capacity; Learning algorithms","","","","","","","Panksepp J., Affective Neuroscience: The Foundations of Human and Animal Emotions, (2004); Izard C., The Psychology of Emotions, (1991); Ekman P., Friesen W., Facial Action Coding System, 1, (1978); Carlson N., Physiology of Behavior, (2012); Yu J., Bhanu B., Evolutionary feature synthesis for facial expression recognition, Pattern Recognit. Lett., 27, 11, pp. 1289-1298, (2006); Jabid T., Kabir M.H., Chae O., Robust facial expression recognition based on local directional pattern, Etri J., 32, 5, pp. 784-794, (2010); Shima Y., Omori Y., Image augmentation for classifying facial expression images by using deep neural network pre-trained with object image database, Proc. 3rd Int. Conf. Robot., Control Autom., pp. 140-146, (2018); Shan C., Gong S., McOwan P.W., Facial expression recognition based on local binary patterns: A comprehensive study, Image Vis. Com-put, 27, 6, pp. 803-816, (2009); Mehendale N., Facial emotion recognition using convolutional neural networks (FERC), Social Netw. Appl. Sci., 2, 3, pp. 1-8, (2020); Breuer R., Kimmel R., A Deep Learning Perspective on the Origin of Facial Expressions, (2017); Saeed S., Baber J., Bakhtyar M., Ullah I., Sheikh N., Dad I., Ali A., Empirical evaluation of SVM for facial expression recognition, Int. J. Adv. Comput. Sci. Appl., 9, 11, pp. 670-673, (2018); Goodfellow I.J., Erhan D., Carrier P.L., Courville A., Mirza M., Hamner B., Cukierski W., Tang Y., Thaler D., Lee D.H., Zhou Y., Challenges in representation learning: A report on three machine learning contests, Proc. Int. Conf. Neural Inf. Process, pp. 117-124, (2013); Pramerdorfer C., Kampel M., Facial Expression Recognition Using Convolutional Neural Networks: State of the Art, (2016); Sun Y., An Y., Research on the embedded system of facial expression recognition based on HMM, Proc. 2nd Ieee Int. Conf. Inf. Manage. Eng, pp. 727-731, (2010); Turabzadeh S., Meng H., Swash R.M., Pleva M., Juhar J., Real-time emotional state detection from facial expression on embedded devices, Proc. 7th Int. Conf. Innov. Comput. Technol. (INTECH), pp. 46-51, (2017); Loza-Alvarez A., Monroy-Meza A., Suarez-Rivera R.A., Perez-Soto G.I., Morales-Hernandez L.A., Camarillo-Gomez K.A., Facial expressions recognition with CNN and its application in an assistant humanoid robot, Proc. COMRob, pp. 1-6, (2018); Gallego A.-J., Pertusa A., Calvo-Zaragoza J., Improving convolu-tional neural networks' accuracy in noisy environments using k-nearest neighbors, Appl. Sci., 8, 11, (2018); Tan M., Le Q., EfficientNet: Rethinking model scaling for convolu-tional neural networks, Proc. 36th Int. Conf. Mach. Learn, pp. 6105-6114, (2019); FER-2013 Dataset, (2020); Srinivas B., Rao G., A hybrid CNN-KNN model for MRI brain tumor classification, Int. J. Recent Technol. Eng., 8, 2, pp. 20-25, (2019); Raheem K.R., Ali I.H., Facial expression recognition using hybrid CNN-SVM technique, Int. J. Adv. Sci. Technol., 29, 4, pp. 5528-5534, (2020); Gallego A.-J., Calvo-Zaragoza J., Rico-Juan J.R., Insights into efficient k-nearest neighbor classification with convolutional neural codes, Ieee Access, 8, pp. 99312-99326, (2020); Sun X., Park J., Kang K., Hur J., Novel hybrid CNN-SVM model for recognition of functional magnetic resonance images, Proc. Ieee Int. Conf. Syst., Man, Cybern. (SMC), pp. 1001-1006, (2017); Mohamed A.E., Comparative study of four supervised machine learning techniques for classification, Int. J. Appl. Sci. Technol., 7, 2, pp. 5-18, (2017); Miao S., Xu H., Han Z., Zhu Y., Recognizing facial expressions using a shallow convolutional neural network, Ieee Access, 7, pp. 78000-78011, (2019); Zahara L., Musa P., Prasetyo Wibowo E., Karim I., Bahri Musa S., The facial emotion recognition (FER-2013) dataset for prediction system of micro-expressions face using the convolutional neural network (CNN) algorithm based raspberry pi, Proc. 5th Int. Conf. Informat. Comput. (ICIC), pp. 1-9, (2020); Shirisha K., Buddha M., Facial emotion detection using convolutional neural network, Int. J. Sci. Eng. Res., 11, 3, pp. 51-54, (2020)","M.H.M. Noor; School of Computer Sciences, Universiti Sains Malaysia, Minden, Penang, 11800, Malaysia; email: halimnoor@usm.my; M.F. Akbar; School of Electrical and Electronic Engineering, Engineering Campus, Universiti Sains Malaysia, Penang, Nibong Tebal, 14300, Malaysia; email: firdaus.akbar@usm.my","","Institute of Electrical and Electronics Engineers Inc.","","","","","","21693536","","","","English","IEEE Access","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85115174722"
"Al Qudah M.; Mohamed A.; Lutfi S.","Al Qudah, Mustafa (57222567501); Mohamed, Ahmad (57190968285); Lutfi, Syaheerah (27567802400)","57222567501; 57190968285; 27567802400","Analysis of Facial Occlusion Challenge in Thermal Images for Human Affective State Recognition","2023","Sensors","23","7","3513","","","","1","10.3390/s23073513","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152352283&doi=10.3390%2fs23073513&partnerID=40&md5=51425677df6c40edb11a0b9b97668eb3","School of Computer Sciences, Universiti Sains Malaysia, Penang, Gelugor, 11800, Malaysia; Department of Computer Science, College of Science and Humanities in Al-Sulail, Prince Sattam bin Abdulaziz University, Kharj, 16278, Saudi Arabia","Al Qudah M., School of Computer Sciences, Universiti Sains Malaysia, Penang, Gelugor, 11800, Malaysia, Department of Computer Science, College of Science and Humanities in Al-Sulail, Prince Sattam bin Abdulaziz University, Kharj, 16278, Saudi Arabia; Mohamed A., School of Computer Sciences, Universiti Sains Malaysia, Penang, Gelugor, 11800, Malaysia; Lutfi S., School of Computer Sciences, Universiti Sains Malaysia, Penang, Gelugor, 11800, Malaysia","Several studies have been conducted using both visual and thermal facial images to identify human affective states. Despite the advantages of thermal facial images in recognizing spontaneous human affects, few studies have focused on facial occlusion challenges in thermal images, particularly eyeglasses and facial hair occlusion. As a result, three classification models are proposed in this paper to address the problem of thermal occlusion in facial images, with six basic spontaneous emotions being classified. The first proposed model in this paper is based on six main facial regions, including the forehead, tip of the nose, cheeks, mouth, and chin. The second model deconstructs the six main facial regions into multiple subregions to investigate the efficacy of subregions in recognizing the human affective state. The third proposed model in this paper uses selected facial subregions, free of eyeglasses and facial hair (beard, mustaches). Nine statistical features on apex and onset thermal images are implemented. Furthermore, four feature selection techniques with two classification algorithms are proposed for a further investigation. According to the comparative analysis presented in this paper, the results obtained from the three proposed modalities were promising and comparable to those of other studies. © 2023 by the authors.","affects; emotion; eyeglass; occlusion; recognition; thermal","Algorithms; Emotions; Face; Facial Expression; Humans; Mouth; Eyeglasses; Face recognition; Thermoanalysis; Affect; Affective state; Emotion; Facial images; Facial occlusions; Facial regions; Occlusion; Recognition; Thermal; Thermal images; algorithm; diagnostic imaging; emotion; face; facial expression; human; mouth; Emotion Recognition","","","","","Ministry of Higher Education, Malaysia, MOHE, (FRGS/1/2020/STG07/USM/02/12)","This work was supported by the Ministry of Higher Education Malaysia for Fundamental Research Grant Scheme with project code: FRGS/1/2020/STG07/USM/02/12.","Shouse E., Feeling, Emotion, Affect, M/C J, 8, (2005); Nayak S., Panda S.K., Uttarkabat S., A Non-contact Framework based on Thermal and Visual Imaging for Classification of Affective States during HCI, Proceedings of the 2020 4th International Conference on Trends in Electronics and Informatics (ICOEI), pp. 653-660; Desideri L., Ottaviani C., Malavasi M., di Marzio R., Bonifacci P., Emotional processes in human-robot interaction during brief cognitive testing, Comput. Human Behav, 90, pp. 331-342, (2019); Lee M.S., Cho Y.R., Lee Y.K., Pae D.S., Lim M.T., Kang T.K., PPG and EMG based emotion recognition using convolutional neural network, Proceedings of the 16th International Conference on Informatics in Control, Automation and Robotics, Prague, Czech Republic, 29 July 2019, pp. 595-600, (2019); Nancarrow A.F., Gilpin A.T., Thibodeau R.B., Farrell C.B., Knowing what others know: Linking deception detection, emotion knowledge, and Theory of Mind in preschool, Infant Child Dev, 27, (2018); Rooj S., Antesh U., Bhattacharya S., Routray A., Mandal M.K., Emotion Classification of Facial Thermal Images using Sparse Coded Filters, Proceedings of the IECON 2020 The 46th Annual Conference of the IEEE Industrial Electronics Society, pp. 453-458; Chen C.-H., Lee I.J., Lin L.-Y., Augmented reality-based self-facial modeling to promote the emotional expression and social skills of adolescents with autism spectrum disorders, Res. Dev. Disabil, 36, pp. 396-403, (2015); Usman M., Evans R., Saatchi R., Kingshott R., Elphick H., Non-invasive respiration monitoring by thermal imaging to detect sleep apnoea, Proceedings of the 32nd International Congress and Exhibition on Condition Monitoring and Diagnostic Engineering Management; Huang Y., Chen F., Lv S., Wang X., Facial expression recognition: A survey, Symmetry, 11, (2019); Mohd M.N.H., Kashima M., Sato K., Watanabe M., Mental stress recognition based on non-invasive and non-contact measurement from stereo thermal and visible sensors, Int. J. Affect. Eng, 14, pp. 9-17, (2015); Nguyen T., Tran K., Nguyen H., Towards thermal region of interest for human emotion estimation, Proceedings of the 2018 10th International Conference on Knowledge and Systems Engineering (KSE), pp. 152-157; Wang S., Pan B., Chen H., Ji Q., Thermal Augmented Expression Recognition, IEEE Trans Cybern, 48, pp. 2203-2214, (2018); Wang S., He M., Gao Z., He S., Ji Q., Emotion recognition from thermal infrared images using deep Boltzmann machine, Front. Comput. Sci, 8, pp. 609-618, (2014); Yan X., Andrews T.J., Jenkins R., Young A.W., Cross-cultural differences and similarities underlying other-race effects for facial identity and expression, Q. J. Exp. Psychol, 62, (2016); Wang S., Liu Z., Wang Z., Wu G., Shen P., He S., Wang X., Analyses of a Multimodal Spontaneous Facial Expression Database, IEEE Trans. Affect. Comput, 4, pp. 34-46, (2013); Bhowmik M.K., Saha P., Singha A., Bhattacharjee D., Dutta P., Enhancement of robustness of face recognition system through reduced gaussianity in Log-ICA, Expert Syst. Appl, 116, pp. 96-107, (2019); Zhang X., Yin L., Cohn J.F., Canavan S., Reale M., Horowitz A., Liu P., Girard J.M., Bp4d-spontaneous: A high-resolution spontaneous 3d dynamic facial expression database, Image Vis. Comput, 32, pp. 692-706, (2014); Al Qudah M.M., Mohamed A.S., Lutfi S.L., Affective State Recognition Using Thermal-Based Imaging: A Survey, Comput. Syst. Sci. Eng, 37, pp. 47-62, (2021); Wang Y., Jiang X., Yang M., Zhang D., Yi X., Nguyen H., Kotani K., Chen F., Le B., Estimation of human emotions using thermal facial information, Proceedings of the 5th International Conference on Graphic and Image Processing (ICGIP 2013); Nguyen H., Chen F., Kotani K., Le B., Fusion of visible images and thermal image sequences for automated facial emotion estimation, J. Mobile Multimed, 10, pp. 294-308, (2014); Abd Latif M., Yusof H.M., Sidek S., Rusli N., Thermal imaging based affective state recognition, Proceedings of the 2015 IEEE International Symposium on Robotics and Intelligent Sensors (IRIS), pp. 214-219; Saha P., Bhattacharjee D., De B.K., Nasipuri M., A Thermal Blended Facial Expression Analysis and Recognition System Using Deformed Thermal Facial Areas, Int. J. Image Graph, 22, (2022); Basu A., Routray A., Shit S., Deb A.K., Human emotion recognition from facial thermal image based on fused statistical feature and multi-class SVM, Proceedings of the 2015 Annual IEEE India Conference (INDICON), pp. 1-5; Goulart C., Valadao C., Delisle-Rodriguez D., Caldeira E., Bastos T., Emotion analysis in children through facial emissivity of infrared thermal imaging, PLoS ONE, 14, (2019); Khan M.M., Ward R.D., Ingleby M., Classifying pretended and evoked facial expressions of positive and negative affective states using infrared measurement of skin temperature, ACM Trans. Appl. Percept, 6, pp. 1-22, (2009); Cross C., Skipper J., Petkie D., Thermal imaging to detect physiological indicators of stress in humans, Proceedings of the SPIE Defense, Security, and Sensing; Wang S., Liu Z., Lv S., Lv Y., Wu G., Peng P., Chen F., Wang X., A Natural Visible and Infrared Facial Expression Database for Expression Recognition and Emotion Inference, IEEE Trans. Multimed, 12, pp. 682-691, (2010); Kopaczka M., Kolk R., Merhof D., A fully annotated thermal face database and its application for thermal facial expression recognition, Proceedings of the 2018 IEEE International Instrumentation and Measurement Technology Conference (I2MTC), pp. 1-6; Haamer R.E., Rusadze E., Lsi I., Ahmed T., Escalera S., Anbarjafari G., Review on emotion recognition databases, Hum. Robot Interact. Theor. Appl, 3, pp. 39-63, (2017); Pantic M., Valstar M., Rademaker R., Maat L., Web-based database for facial expression analysis, Proceedings of the 2005 IEEE International Conference on Multimedia and Expo; Liu P., Yin L., Spontaneous facial expression analysis based on temperature changes and head motions, Proceedings of the 2015 11th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG), pp. 1-6; Zhu X., Ramanan D., Face detection, pose estimation, and landmark localization in the wild, Proceedings of the 2012 IEEE Conference on Computer Vision and Pattern Recognition, pp. 2879-2886; Latif M., Yusof M.H., Sidek S., Rusli N., Texture descriptors based affective states recognition-frontal face thermal image, Proceedings of the 2016 IEEE EMBS Conference on Biomedical Engineering and Sciences (IECBES), pp. 80-85; Carrapico R., Mourao A., Magalhaes J., Cavaco S., A comparison of thermal image descriptors for face analysis, Proceedings of the 2015 23rd European Signal Processing Conference (EUSIPCO), pp. 829-833; Cruz-Albarran I.A., Benitez-Rangel J.P., Osornio-Rios R.A., Morales-Hernandez L.A., Human emotions detection based on a smart-thermal system of thermographic images, Infrared Phys. Technol, 81, pp. 250-261, (2017); Ioannou S., Gallese V., Merla A., Thermal infrared imaging in psychophysiology: Potentialities and limits, Psychophysiology, 51, pp. 951-963, (2014); Jian B.-L., Chen C.-L., Huang M.-W., Yau H.-T., Emotion-Specific Facial Activation Maps Based on Infrared Thermal Image Sequences, IEEE Access, 7, pp. 48046-48052, (2019); Kyal C.K., Poddar H., Reza M., Human Emotion Recognition from Spontaneous Thermal Image Sequence Using GPU Accelerated Emotion Landmark Localization and Parallel Deep Emotion Net, Proceedings of the International Conference on Innovative Computing and Communications: Proceedings of ICICC 2020, pp. 931-943; Perez-Rosas V., Narvaez A., Burzo M., Mihalcea R., Thermal imaging for affect detection, Proceedings of the 6th International Conference on PErvasive Technologies Related to Assistive Environments—PETRA ‘13, pp. 1-4; Latif M., Sidek S., Rusli N., Fatai S., Emotion detection from thermal facial imprint based on GLCM features, ARPN J. Eng. Appl. Sci, 11, pp. 345-350, (2016); Shaees S., Naeem H., Arslan M., Naeem M.R., Ali S.H., Aldabbas H., Facial emotion recognition using transfer learning, Proceedings of the 2020 International Conference on Computing and Information Technology (ICCIT-1441), pp. 1-5; Boccanfuso L., Wang Q., Leite I., Li B., Torres C., Chen L., Salomons N., Foster C., Barney E., Ahn Y.A., A thermal emotion classifier for improved human-robot interaction, Proceedings of the 2016 25th IEEE International Symposium on Robot and Human Interactive Communication (RO-MAN), pp. 718-723; Liu Y., Chen X., Wang Z., Wang Z.J., Ward R.K., Wang X., Deep learning for pixel-level image fusion: Recent advances and future prospects, Inf. Fusion, 42, pp. 158-173, (2018); Nayak S., Nagesh B., Routray A., Sarma M., A Human–Computer Interaction framework for emotion recognition through time-series thermal video sequences, Comput. Electr. Eng, 93, (2021); Kopaczka M., Schock J., Nestler J., Kielholz K., Merhof D., A combined modular system for face detection, head pose estimation, face tracking and emotion recognition in thermal infrared images, Proceedings of the 2018 IEEE International Conference on Imaging Systems and Techniques (IST), pp. 1-6; Sharma N., Dhall A., Gedeon T., Goecke R., Modeling Stress Using Thermal Facial Patterns: A Spatio-temporal Approach, Proceedings of the 2013 Humaine Association Conference on Affective Computing and Intelligent Interaction, pp. 387-392; Bian C., Zhang Y., Yang F., Bi W., Lu W., Spontaneous facial expression database for academic emotion inference in online learning, IET Comput. Vis, 13, pp. 329-337, (2019); Murtaza M., Sharif M., AbdullahYasmin M., Ahmad T., Facial expression detection using six facial expressions hexagon (SFEH) model, Proceedings of the 2019 IEEE 9th Annual Computing and Communication Workshop and Conference (CCWC), pp. 0190-0195; Saha P., Bhattacharjee D., De B.K., Nasipuri M., Characterization and recognition of mixed emotional expressions in thermal face image, Proceedings of the Infrared Imaging Systems: Design, Analysis, Modeling, and Testing xxvii, pp. 184-193; Ng E., Yee G.C., Hua T.J., Kagathi M., Analysis of normal human eye with different age groups using infrared images, J. Med. Syst, 33, pp. 207-213, (2009); Shi X., Wang S., Zhu Y., Expression recognition from visible images with the help of thermal images, Proceedings of the 5th ACM on International Conference on Multimedia Retrieval, pp. 563-566; Liew C.F., Yairi T., Facial expression recognition and analysis: A comparison study of feature descriptors, IPSJ Trans. Comput. Vis. Appl, 7, pp. 104-120, (2015); Alshamsi H., Meng H., Li M., Real time facial expression recognition app development on mobile phones, Proceedings of the 2016 12th International Conference on Natural Computation, Fuzzy Systems and Knowledge Discovery (ICNC-FSKD), pp. 1750-1755; Zhao X., Shi X., Zhang S., Facial expression recognition via deep learning, IETE Tech. Rev, 32, pp. 347-355, (2015); Akhand M.A.H., Roy S., Siddique N., Kamal M.A.S., Shimamura T., Facial Emotion Recognition Using Transfer Learning in the Deep CNN, Electronics, 10, (2021)","M. Al Qudah; School of Computer Sciences, Universiti Sains Malaysia, Penang, Gelugor 11800, Malaysia; email: m.alqudah@psau.edu.sa; A. Mohamed; School of Computer Sciences, Universiti Sains Malaysia, Gelugor, Penang, 11800, Malaysia; email: sufril@usm.my","","MDPI","","","","","","14248220","","","37050571","English","Sensors","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85152352283"
"Pearson S.J.; Ritchings T.; Mohamed A.S.A.","Pearson, Stephen J. (7201386436); Ritchings, Tim (15045654400); Mohamed, Ahmad S. A. (57190968285)","7201386436; 15045654400; 57190968285","The use of normalized cross-correlation analysis for automatic tendon excursion measurement in dynamic ultrasound imaging","2013","Journal of Applied Biomechanics","29","2","","165","173","8","14","10.1123/jab.29.2.165","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84877992010&doi=10.1123%2fjab.29.2.165&partnerID=40&md5=a92dccc2ab17fb48c0172f2ba5c0b20f","Centre for Health, Sport and Rehabilitation Sciences Research, University of Salford, Greater Manchester, United Kingdom; Control and Systems Engineering Research Centre, University of Salford, Greater Manchester, United Kingdom","Pearson S.J., Centre for Health, Sport and Rehabilitation Sciences Research, University of Salford, Greater Manchester, United Kingdom; Ritchings T., Control and Systems Engineering Research Centre, University of Salford, Greater Manchester, United Kingdom; Mohamed A.S.A., Control and Systems Engineering Research Centre, University of Salford, Greater Manchester, United Kingdom","The work describes an automated method of tracking dynamic ultrasound images using a normalized crosscorrelation algorithm, applied to the patellar and gastrocnemius tendon. Displacement was examined during active and passive tendon excursions using B-mode ultrasonography. In the passive test where two regions of interest (2-ROI) were tracked, the automated tracking algorithm showed insignificant deviations from relative zero displacement for the knee (0.01 ± 0.04 mm) and ankle (-0.02 ± 0.04 mm) (P > .05). Similarly, when tracking 1-ROI the passive tests showed no significant differences (P > .05) between automatic and manual methods, 7.50 ± 0.60 vs 7.66 ± 0.63 mm for the patellar and 11.28 ± 1.36 vs 11.17 ± 1.35 mm for the gastrocnemius tests. The active tests gave no significant differences (P > .05) between automatic and manual methods with differences of 0.29 ± 0.04 mm for the patellar and 0.26 ± 0.01 mm for the gastrocnemius. This study showed that automatic tracking of in vivo displacement of tendon during dynamic excursion under load is possible and valid when compared with the standardized method. This approach will save time during analysis and enable discrete areas of the tendon to be examined. © 2013 Human Kinetics, Inc.","Normalized cross-correlation; Speckle tracking; Tendon; Ultrasound","Ultrasonic imaging; Ultrasonics; Automated tracking; Automatic tracking; Normalized cross correlation; Regions of interest; Speckle tracking; Standardized methods; Ultrasound images; Ultrasound imaging; adult; algorithm; ankle; article; automation; B scan; biomechanics; correlation analysis; gastrocnemius muscle; human; human experiment; image analysis; in vivo study; knee; male; measurement; musculoskeletal function; normal human; patella tendon; tendon excursion; Tendons","","","","","","","Fukunaga T., Ito M., Ichinose Y., Kuno S., Kawakami Y., Fukashiro S., Tendinous movement of a human muscle during voluntary contractions determined by real-time ultrasonography, Journal of Applied Physiology, 81, 3, pp. 1430-1433, (1996); Kubo K., Kanehisa H., Kawakami Y., Fukunaga T., Elastic properties of muscle-tendon complex in long-distance runners, Eur J Appl Physiol, 81, pp. 181-187, (2000); Hansen P., Bojsen-Moller J., Aagaard P., Kjaer M., Magnusson S.P., Mechanical properties of the human patellar tendon, in vivo, Clin Biomech (Bristol, Avon), 21, pp. 54-58, (2006); Onambele G.N., Burgess K., Pearson S.J., Gender-specific in vivo measurement of the structural and mechanical properties of the human patellar tendon, J Orthop Res, 25, pp. 1635-1642, (2007); Dilley A., Greening J., Lynn B., Leary R., Morris V., The use of cross-correlation analysis between high-frequency ultrasound images to measure longitudinal median nerve movement, Ultrasound in Medicine and Biology, 27, 9, pp. 1211-1218, (2001); Lee S.S.M., Lewis G.S., Piazza S.J., An algorithm for automated analysis of ultrasound images to measure tendon excursion in vivo, Journal of Applied Biomechanics, 24, 1, pp. 75-82, (2008); Kim Y.S., Kim J.M., Bigliani L.U., Kim H.J., Jung H.W., In vivo strain analysis of the intact supraspinatus tendon by ultrasound speckles tracking imaging, J Orthop Res, 29, pp. 1931-1937, (2011); Farron J., Varghese T., Thelen D.G., Measurement of Tendon Strain During Muscle Twitch Contractions Using Ultrasonography, IEEE Transaction of Ultrasonics, Ferroelectrics, and Frequency Control, 56, pp. 27-35, (2009); Koga T., Iinuma K., Hirano A., Iijima Y., Ishiguro T., Motion Compensated Interframe Coding for Video Conferencing, Proc. Nat. Telcommun. Conf., New Orleans, LA. 1981;Nov; Arampatzis A., Stafilidis S., DeMonte G., Karaminidis K., Morey-Klapsing G., Bruggemann G.P., Strain and elongation of the human gastrocnemius tendon and aponeurosis during maximal plantarflexion effort, J Biomech, 38, pp. 883-1841, (2005); Revell M.M., McNally D., Computer vision elastography: Speckle adaptive motion estimation for elastography using ultrasound sequences, IEEE Trans Med Imaging, 24, (2005); Pearson S.J., Burgess K., Onambele G.N., Creep and the in vivo assessment of human patellar tendon mechanical properties, Clin Biomech (Bristol, Avon), 22, pp. 712-717, (2007); Maganaris C.N., Baltzopoulos V., Sergeant A.K., In vivo measurement-based estimations of the human Achilles tendon moment arm, Eur J Appl Physiol, 83, pp. 363-369, (2000); Magnusson S.P., Hansen P., Aagaard P., Et al., Differential strain patterns of the human gastrocnemius aponeurosis and free tendon, in vivo, Acta Physiol Scand, 177, pp. 185-195, (2003); Loram I.D., Maganaris C.N., Lakie M., Use of ultrasound to make noninvasive in vivo measurement of continuous changes in human muscle contractile length, J Appl Physiol, 100, pp. 1311-1323, (2006)","","","Human Kinetics Publishers Inc.","","","","","","10658483","","JABOE","","English","J. Appl. Biomech.","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-84877992010"
"Mohamed A.S.A.; Wahab M.N.A.; Krishnan S.R.; Arasu D.B.L.","Mohamed, Ahmad Sufril Azlan (57190968285); Wahab, Mohd Nadhir Ab (36471236100); Krishnan, Sanarthana Radha (57213595266); Arasu, Darshan Babu L. (57207817920)","57190968285; 36471236100; 57213595266; 57207817920","Facial recognition adaptation as biometric authentication for intelligent door locking system","2019","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11870 LNCS","","","257","267","10","2","10.1007/978-3-030-34032-2_24","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077907707&doi=10.1007%2f978-3-030-34032-2_24&partnerID=40&md5=b9b8b8f6caacfe5ddca2f8e13aafe27a","School of Computer Sciences, Universiti Sains Malaysia, Gelugor, 11800, Penang, Malaysia","Mohamed A.S.A., School of Computer Sciences, Universiti Sains Malaysia, Gelugor, 11800, Penang, Malaysia; Wahab M.N.A., School of Computer Sciences, Universiti Sains Malaysia, Gelugor, 11800, Penang, Malaysia; Krishnan S.R., School of Computer Sciences, Universiti Sains Malaysia, Gelugor, 11800, Penang, Malaysia; Arasu D.B.L., School of Computer Sciences, Universiti Sains Malaysia, Gelugor, 11800, Penang, Malaysia","As field of technology grows, security issues have gained high concern nowadays. Unfortunately, a good access authentication is high in price which had become less affordable. To overcome this scenario, Intelligent Door Locking System is proposed. This system can be divided into 3 parts, which are mobile application, server with web application and microcontroller. The mobile application will be the one in charge of having face recognition process. The face recognition will be carried out using Eigenfaces Algorithm. Users can lock the door using “Normal Lock” mode or “Secure Lock” mode. To unlock the “Normal Lock” mode, user just need to press on unlock button, while to unlock “Se- cure Lock” mode, user would need to pass biometric authentication and passcode authentication process. Once user successfully identified by the mobile application, data will be sent to microcontroller via Bluetooth. At the same time, the microcontroller will retrieve data from server database and check whether the user is having access to enter the room. If yes, the microcontroller will unlock the door. While for the server, it can be easily managed by administration using web application. Users can check their door lock condition from far distance through web application as well. They can lock the door if they realize the door is not locked wherever they are. This bring convenience to the user. © Springer Nature Switzerland AG 2019.","Eigenfaces and smart lock; Face recognition; Intelligent locking system; Security","Authentication; Biometrics; Controllers; Locks (fasteners); Microcontrollers; Mobile computing; Access authentications; Biometric authentication; Eigenfaces; Facial recognition; Intelligent locking system; Mobile applications; Recognition process; Security; Face recognition","","","","","University of Southern Maine, USM, (PKOMP/6315262, PKOMP/8014001); Universiti Sains Malaysia","Acknowledgement. This research is funded under USM RU Grant (PKOMP/8014001) and partly under USM Short Term Grant (PKOMP/6315262) and affiliated with Robotics, Computer Vision & Image Processing (RCVIP) Research Group Lab at School of Computer Sciences, Universiti Sains Malaysia.","Soyata T., Muraleedharan R., Funai C., Kwon M., Heinzelman W., Cloud-vision: Real-time face recognition using a mobile-cloudlet-cloud acceleration architecture, 2012 IEEE Symposium on Computers and Communications (ISCC), pp. 59-66, (2012); Januzaj Y., Luma A., Januzaj Y., Ramaj V., Real time access control based on face recognition, 2015 International Conference on Network Security & Computer Science, pp. 7-12, (2015); Young A.W., Burton A.M., Recognizing faces, Curr. Dir. Psychol. Sci., 26, 3, pp. 212-217, (2017); Mesni B., Authentication in door access control systems, A Treatise on Electricity and Magnetism, 2, pp. 68-73, (2013); Joseph J., Zacharia K.P., Automatic attendance management system using face recognition, Int. J. Sci. Res. (IJSR), 2, 11, pp. 327-330, (2013); Turk M.A., Pentland A.P., Face recognition using eigenfaces, Proceedings 1991 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, pp. 586-591, (1991); Chintalapati S., Raghunadh M.V., Automated attendance management system based on face recognition algorithms, 2013 IEEE International Conference on Computational Intelligence and Computing Research, Enathi, pp. 1-5, (2013); Dave G., Chao X., Sriadibhatla K., Face Recognition in Mobile Phones, pp. 1-7; Saini R., Saini A., Agarwal D., Analysis of different face recognition algorithms, Int. J. Eng. Res. Technol., 3, 11, pp. 1263-1267, (2014); Pabbaraju A., Puchakayala S., Face Recognition in Mobile Devices, pp. 1-9, (2010); Mohamed A.S.A., Face recognition using eigenfaces, MRG International Conference 2006, (2006); Turk M., Pentland A., Eigenfaces for recognition, J. Cogn. Neurosci., 3, 1, pp. 71-86, (1991); Belhumeur P.N., Hespanha J.P., Kriegman D.J., Eigenfaces vs. Fisherfaces: Recognition using class specific linear projection, IEEE Trans. Pattern Anal. Mach. Intell., 19, 7, pp. 711-719, (1997); Karande K.J., Talbar S.N., Simplified and modified approach for face recognition using PCA, IET-UK International Conference on Information and Communication Technology in Electrical Sciences (ICTES 2007, pp. 523-526, (2007); Pissarenko D., Eigenface-Based Facial Recognition, (2003); Zhao W., Chellappa R., Phillips P.J., Rosenfeld A., Face recognition: A literature survey, ACM Comput. Surv., 35, 4, pp. 399-458, (2003)","A.S.A. Mohamed; School of Computer Sciences, Universiti Sains Malaysia, Gelugor, 11800, Malaysia; email: sufril@usm.my","Badioze Zaman H.; Mohamad Ali N.; Ahmad M.N.; Smeaton A.F.; Shih T.K.; Velastin S.; Terutoshi T.","Springer","","6th International Conference on Advances in Visual Informatics, IVIC 2019","19 November 2019 through 21 November 2019","Bangi","235609","03029743","978-303034031-5","","","English","Lect. Notes Comput. Sci.","Conference paper","Final","","Scopus","2-s2.0-85077907707"
"Sukri S.S.; Ruhaiyem N.I.R.; Mohamed A.S.A.","Sukri, Syazwan Syafiqah (57197836300); Ruhaiyem, Nur Intan Raihana (57190964192); Mohamed, Ahmad Sufril Azlan (57190968285)","57197836300; 57190964192; 57190968285","Face recognition with real time eye lid movement detection","2017","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","10645 LNCS","","","352","363","11","2","10.1007/978-3-319-70010-6_33","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85035149981&doi=10.1007%2f978-3-319-70010-6_33&partnerID=40&md5=ac5ac86118d32da08b3b5b89e5155c08","School of Computer Sciences, Universiti Sains Malaysia (USM), Gelugor, 11800, Penang, Malaysia","Sukri S.S., School of Computer Sciences, Universiti Sains Malaysia (USM), Gelugor, 11800, Penang, Malaysia; Ruhaiyem N.I.R., School of Computer Sciences, Universiti Sains Malaysia (USM), Gelugor, 11800, Penang, Malaysia; Mohamed A.S.A., School of Computer Sciences, Universiti Sains Malaysia (USM), Gelugor, 11800, Penang, Malaysia","The enhancement of current face recognition system used in attendance system is proposed to fulfill the motivations for this project which are to encounter the shortcomings from the existing systems, to put an innovation into the existing system and to make the system smarter by using real-time functionality. There are three objectives in this project which are to make the system able to differentiate between real face and a photo, to make the system works on desired speed and important key is to make a user-friendly system in term of its interface and functions. Techniques that will be used to achieve the objectives are by using average standard deviation of depth or pulse magnification, using JAVA programming language and develop using simple and standard user interface components and functions. At the end, this system is expected to fulfill the objectives stated and can encounter the problem arise in existing system. As the conclusion, there is no perfect system and still need to be enhanced from time to time. © Springer International Publishing AG 2017.","Attendance system; Face recognition system; Innovation; Real-time","Computer programming; Eye movements; Innovation; Real time systems; User interfaces; Attendance systems; Desired speed; Existing systems; Face recognition systems; Movement detection; Real time; Standard deviation; User interface components; Face recognition","","","","","Universiti Sains Malaysia, (304/PKOMP/6313259)","Acknowledgments. The authors wish to thank Universiti Sains Malaysia for the support it has extended in the completion of the present research through Short Term University Grant No: 304/PKOMP/6313259.","Cornelissen F., Peters E., Palmer J., The Eyeblink Toolbox: Eye tracking with MATLAB and the Psychophysics Toolbox, Behav. Res. Methods Instrum. Comput., 34, 4, pp. 613-617, (2002); How Often and Why Do people’s Eyes Blink? - the Boston Globe; NEC Corporation of Malaysia Introduces Neoface® Facial Recognition Solutions for the First Time in Malaysia; (2016); Makwana H., Singh T., Comparison of different algorithm for face recognition, Glob. J. Comput. Sci. Technol. Graph. Vis., 13, 9, (2013); Ahonen T., Hadid A., Pietikainen M., Face recognition with local binary patterns, ECCV 2004. LNCS, 3021, pp. 469-481, (2004); Singh A., Comparison of face recognition algorithms on dummy faces, Int. J. Multimed. Appl., 4, 4, pp. 121-135, (2012); Mohamed A.S.A., Ritchings T., Pearson S., Image tracking using normalized cross-correlation to track and analyse mechanical tendon properties, Proceedings of the Salford Postgraduate Annual Research Conference (SPARC 2011), 2, pp. 10-11, (2011); Shanmugasundaram K., Mohamed A.S.A., Venkat I., An overview of multimodal biometrics using meta-heuristic optimization techniques for F2R system, Int. J. Soft Comput. Eng. (IJSCE, 5, 5, (2015); Ruhaiyem N.I.R., Mohamed A.S.A., Belaton B., Optimized segmentation of cellular tomography through organelles’ morphology and image features, J. Telecommun. Electron. Comput. Eng. (JTEC), 8, 3, pp. 79-83, (2016); Halim M.A.A., Ruhaiyem N.I.R., Fauzi E.R.I., Mohamed A.S.A., Automatic laser welding defect detection and classification using sobel-contour shape detection, J. Telecommun. Electron. Comput. Eng. (JTEC), 8, 6, pp. 157-160, (2016); Veeraputhara Thevar V., Ruhaiyem N.I.R., Concept, Theory and Application: Hybrid Watershed Classic and Active Contour for Enhanced Image Segmentation, (2016); Ruhaiyem N.I.R., Semi-automated cellular tomogram segmentation workflow (CTSW): Towards an automatic target-scoring system, Proceedings of the International Conference on Computer Graphics, Multimedia and Image Processing, pp. 38-48, (2014); Ruhaiyem N.I.R., Boundary-based versus region-based approaches for cellular tomography segmentation, Proceedings of 1St International Engineering Conference, pp. 260-267, (2014); Ruhaiyem N.I.R., Multiple, Object-Oriented Segmentation Methods of Mammalian Cell Tomograms, (2014)","N.I.R. Ruhaiyem; School of Computer Sciences, Universiti Sains Malaysia (USM), Gelugor, 11800, Malaysia; email: intanraihana@usm.my","Shih T.K.; Velastin S.; Robinson P.; Smeaton A.F.; Terutoshi T.; Badioze Zaman H.; Jaafar A.; Mohamad Ali N.","Springer Verlag","","5th International Visual Informatics Conference, IVIC 2017","28 November 2017 through 30 November 2017","Bangi","205759","03029743","978-331970009-0","","","English","Lect. Notes Comput. Sci.","Conference paper","Final","","Scopus","2-s2.0-85035149981"
"Mahdy K.; Zekry A.; Moussa M.; Mohamed A.; Mahdy H.; Elhabiby M.","Mahdy, Kamel (58756992500); Zekry, Ahmed (57226706468); Moussa, Mohamed (35422643300); Mohamed, Ahmed (57190968285); Mahdy, Hassan (55354235000); Elhabiby, Mohamed (34869463200)","58756992500; 57226706468; 35422643300; 57190968285; 55354235000; 34869463200","Pavement distress instance segmentation using deep neural networks and low-cost sensors","2024","Innovative Infrastructure Solutions","9","1","6","","","","0","10.1007/s41062-023-01308-1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85179312424&doi=10.1007%2fs41062-023-01308-1&partnerID=40&md5=394699e35b2c2122afe422bf0b31f44a","Public Works Department, Ain Shams University, Cairo, Egypt; Micro Engineering Tech. Inc., Calgary, AB, Canada","Mahdy K., Public Works Department, Ain Shams University, Cairo, Egypt; Zekry A., Micro Engineering Tech. Inc., Calgary, AB, Canada; Moussa M., Micro Engineering Tech. Inc., Calgary, AB, Canada; Mohamed A., Micro Engineering Tech. Inc., Calgary, AB, Canada; Mahdy H., Public Works Department, Ain Shams University, Cairo, Egypt; Elhabiby M., Public Works Department, Ain Shams University, Cairo, Egypt","Road degradation and deterioration cause more accidents in the transportation sector. Municipalities worldwide monitor and repair roads to solve this issue. However, this method is expensive. Recent research has focused on cost-effective alternatives, notably deep neural networks (DNNs), which can identify road distress kinds and locations from low-cost camera photographs. The lack of extensive road distress datasets, which DNN models mainly rely on, hinders their road condition monitoring performance. Because photographs typically contain noise that severely degrades DNN models, previous attempts to gather such datasets have failed. Our work presents a unique road distress dataset of 1040 photographs from a low-cost camera, covering six categories. For the training of DNN models, each incidence of distress in the dataset is rigorously annotated with bounding boxes, distress segmentation, and distress kind. We trained two DNN models to predict road distress instance segmentation and carefully compared their accuracy with 92.2% and 88.2% mean average precision for longitudinal and transverse cracks for a mere 17-ms inference time making it suitable for working with low-cost smartphone cameras with the highest frames per second (60 FPS). The distress instance segmentation estimation result is sent to a module that calculates the reliable Pavement Condition Index (PCI) of road health. Our unique approach to real-time online PCI computation uses instance segmentation on smartphone-collected road photographs. This application would access a cloud platform using DNN models. This strategy promises to change road monitoring and maintenance, making transport networks safer and more efficient. © 2023, Springer Nature Switzerland AG.","Deep learning; Instance segmentation; Low-cost sensors; Pavement distress dataset; Pavement distress detection; Pavement management; PMMS; Semantic segmentation","","","","","","","","Arya D., Maeda H., Ghosh S.K., Toshniwal D., Mraz A., Kashiyama T., Sekimoto Y., Transfer Learning-Based Road Damage Detection for Multiple Countries, (2020); Arya D., Maeda H., Ghosh S.K., Toshniwal D., Sekimoto Y., RDD2022: A Multi-National Image Dataset for Automatic Road Damage Detection, (2022); Augustaukas R., Lipnickas A., Pixel-Wise Road Pavement Defects Detection Using U-Net Deep Neural Network, 10Th IEEE International Conference on Intelligent Data Acquisition and Advanced Computing Systems: Technology and Applications (IDAACS), 1, pp. 468-471, (2019); Banharnsakun A., Hybrid ABC-ANN for pavement surface distress detection and classification, Int J Mach Learn Cybern, 8, 2, pp. 699-710, (2017); Chen Y., Fan H., Xu B., Yan Z., Kalantidis Y., Rohrbach M., Yan S., Feng J., Drop an octave: Reducing spatial redundancy in convolutional neural networks with octave convolution, Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 3435-3444, (2019); Chen H., Lin H., Yao M., Improving the efficiency of encoder-decoder architecture for pixel-level crack detection, IEEE Access, 7, pp. 186657-186670, (2019); Chollet F., Xception: Deep learning with depth-wise separable convolutions, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 1251-1258, (2017); Economy T.G., The Global Economy - Egypt Road Quality Index, (2023); Road Quality Ranking in Egypt Advances from 118 to 28, (2023); Fan R., Bocus M., Zhu Y., Jiao J., Wang L., Ma F., Cheng S., Liu M., Road crack detection using deep convolutional neural network and adaptive thresholding, IEEE Intelligent Vehicles Symposium (IV), pp. 474-479, (2019); Fan R., Liu Y., Bocus M.J., Wang L., Liu M., Real-time subpixel fast bilateral stereo, IEEE International Conference on Information and Automation (ICIA), IEEE, pp. 1058-1065, (2018); Fan L., Zhao H., Li Y., Li S., Zhou R., Chu W., RAO-UNet: a residual attention and octave UNet for road crack detection via balance loss, IET Intel Transport Syst, 16, 3, pp. 332-375, (2022); Feng H., Xu G.S., Guo Y., Multi-scale classification network for road crack detection, IET Intel Transport Syst, 13, 2, pp. 398-405, (2019); Girshick R., Fast R-CNN, IEEE International Conference on Computer Vision (ICCV), (2015); Gopalakrishnan K., Khaitan S.K., Choudhary A., Agrawal A., Deep convolutional neural networks with transfer learning for computer vision-based data-driven pavement distress detection, Constr Build Mater, 157, pp. 322-352, (2017); Gouveia B.G., Donato M., Da Silva M.A.V., Life cycle assessment in road pavement infrastructures: a review, Civ Eng J, 8, 6, pp. 1304-1315, (2022); He K., Gkioxari G., Dollar P., Girshick R., Mask r-cnn, Proceedings of the IEEE International Conference on Computer Vision, pp. 2961-2969, (2017); He K., Zhang X., Ren S., Sun J., Spatial pyramid pooling in deep convolutional networks for visual recognition, IEEE Trans Pattern Anal Mach Intell (TPAMI), 37, 9, pp. 1904-1916, (2015); Howard A.G., Zhu M., Chen B., Kalenichenko D., Wang W., Weyand T., Andreetto M., Adam H., Mobilenets: Efficient Convolutional Neural Networks for Mobile Vision Applications, (2017); Jenkins D., Mark T.A., Carr M.I., Iglesias T., Buggy G., A Deep Convolutional Neural Network for Semantic Pixel-Wise Segmentation of Road and Pavement Surface Cracks, 26Th European Signal Processing Conference (Eusipco), Ieee, pp. 2120-2144, (2018); Kamel M., Ahmed Z., Mohamed M., Ahmed M., Hassan M., Mohamed E., Deep learning techniques for efficient evaluation of asphalt pavement condition, Mansoura Eng J, 48, 2, (2023); Karaboga D., Gorkemli B., Ozturk C., Karaboga N., A comprehensive survey: artificial bee colony (ABC) algorithm and applications, Artif Intell Rev, 42, pp. 21-57, (2014); Lin T.Y., Maire M., Belongie S., Hays J., Perona P., Ramanan D., Dollar P., Zitnick C.L., Microsoft coco: Common objects in context, European conference on computer vision, pp. 740-755, (2014); Liu W., Anguelov D., Erhan D., Szegedy C., Reed S., Fu C.Y., Berg A.C., SSD: Single shot multibox detector, European conference on computer vision, pp. 21-37, (2016); Liu W., Huang Y., Li Y., Chen Q., Fpcnet: Fast Pavement Crack Detection Network Based on Encoder-Decoder Architecture, (2019); Liu S., Qi L., Qin H., Shi J., Jia J., Path aggregation network for instance segmentation, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 8759-8768, (2018); Ma D., Fang H., Wang N., Xue B., Dong J., Wang F., A real-time crack detection algorithm for pavement based on CNN with multiple feature layers, Road Mater Pavement Des, 23, 9, pp. 2115-2146, (2022); Maeda H., Sekimoto Y., Seto T., Kashiyama T., Omata H., Road damage detection using deep neural networks with images captured through smartphone, Comput Aided Civ Infrastruct Eng, 33, 12, pp. 1127-1168, (2018); Majidifard H., Adu-Gyamfi Y., Buttlar W.G., Deep machine learning approach to develop a new asphalt pavement condition index, Constr Build Mater, 247, (2020); Mohamed E., Shaker A., El-Sallab A., Hadhoud M., Insta-Yolo: Real-Time Instance Segmentation, (2021); Redmon J., Divvala S., Girshick R., Farhadi A., You only look once: Unified, real-time object detection, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 779-788, (2016); Redmon J., Farhadi A., Yolov3: An Incremental Improvement, (2018); Redmon J., Farhadi A., YOLO9000: Better, faster, stronger, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 7263-7271, (2017); Ren S., He K., Girshick R., Sun J., Faster r-cnn: Towards real-time object detection with region proposal networks, Advances in Neural Information Processing Systems, pp. 28-28, (2015); Ren Y., Huang J., Hong Z., Lu W., Yin J., Zou L., Shen X., Image- based concrete crack detection in tunnels using deep fully convolutional networks, Constr Build Mater, 234, (2020); Ronneberger O., Fischer P., Brox T., U-net: Convolutional networks for biomedical image segmentation, International conference on medical image computing and computer-assisted intervention, pp. 234-241, (2015); Shi Y., Cui L., Qi Z., Meng F., Chen Z., Automatic road crack detection using random structured forests, IEEE Trans Intell Transp Syst, 17, 12, pp. 3434-3445, (2016); Simonyan K., Zisserman A., Very Deep Convolutional Networks for Large-Scale Image Recognition, (2014); Szegedy C., Vanhoucke V., Ioffe S., Shlens J., Wojna Z., Rethinking the inception architecture for computer vision, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2818-2826, (2016); Yolo-V5, (2023); Walker D., Entine L., Kummer S., Pavement Surface Evaluation and Rating, Asphalt PASER Manual: Pavement Surface Evaluation and Rating, Transportation Information Center, (2002); Wang F., Jiang M., Qian C., Yang S., Li C., Zhang H., Wang X., Tang X., Residual attention network for image classification, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 3156-3164, (2017); Wang C.Y., Liao H.Y.M., Wu Y.H., Chen P.Y., Hsieh J.W., Yeh I.H., CSPNet: A new backbone that can enhance learning capability of CNN, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops, pp. 390-391, (2020); Wang W., Su C., Deep learning-based real-time crack segmentation for pavement images, KSCE J Civ Eng, 25, 12, pp. 4495-4506, (2021); Yu C., Wang J., Peng C., Gao C., Yu G., Sang N., Bisenet: Bilateral Segmentation Network for Real-Time Semantic Segmentation, pp. 325-341, (2018); Zhang L., Yang F., Zhang Y.D., Zhu Y.J., Road Crack Detection Using Deep Convolutional Neural Network, 2016 IEEE International Conference on Image Processing (ICIP), IEEE, pp. 3708-3720, (2016); Zhun F., Yuming W., Jiewei L., (2018); Zou Q., Zhang Z., Li Q., Qi X., Wang Q., Wang S., DeepCrack: learning hierarchical convolutional features for crack detection, IEEE Trans Image Process, 28, 3, pp. 1498-1512, (2019)","K. Mahdy; Public Works Department, Ain Shams University, Cairo, Egypt; email: kamel.essameldeen@eng.asu.edu.eg","","Springer Science and Business Media Deutschland GmbH","","","","","","23644176","","","","English","Innov. Infrastruct. Solut.","Article","Final","","Scopus","2-s2.0-85179312424"
"Yee Hui D.O.; Lutfi S.L.; Naim S.; Akhtar Z.; Azlan Mohamed A.S.; Siddique K.","Yee Hui, Deborah Ooi (57219472499); Lutfi, Syaheerah Lebai (27567802400); Naim, Syibrah (55413206700); Akhtar, Zahid (46661628200); Azlan Mohamed, Ahmad Sufril (57190968285); Siddique, Kamran (57191228422)","57219472499; 27567802400; 55413206700; 46661628200; 57190968285; 57191228422","The sound of trust: Towards modelling computational trust using voice-only cues at zero-acquaintance","2020","Advances in Science, Technology and Engineering Systems","5","4","","469","476","7","0","10.25046/AJ050456","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092894298&doi=10.25046%2fAJ050456&partnerID=40&md5=d554b99e76bddc28af3d0eb4ec95b67e","Universiti Sains Malaysia, School of Computer Sciences, Penang, 11800, Malaysia; Woosong University, Technology Department, Endicott College of International Studies, Daejeon, 34606, South Korea; University of Memphis, Tennessee, 38152, United States; Xiamen University Malaysia, Sepang, 43900, Malaysia","Yee Hui D.O., Universiti Sains Malaysia, School of Computer Sciences, Penang, 11800, Malaysia; Lutfi S.L., Universiti Sains Malaysia, School of Computer Sciences, Penang, 11800, Malaysia; Naim S., Woosong University, Technology Department, Endicott College of International Studies, Daejeon, 34606, South Korea; Akhtar Z., University of Memphis, Tennessee, 38152, United States; Azlan Mohamed A.S., Universiti Sains Malaysia, School of Computer Sciences, Penang, 11800, Malaysia; Siddique K., Xiamen University Malaysia, Sepang, 43900, Malaysia","Trust is essential in many interdependent human relationships. Trustworthiness is measured via the effectiveness of the relationships involving human perception. The decision to trust others is often made quickly (even at zero acquaintance). Previous research has shown the significance of voice in perceived trustworthiness. However, the listeners' characteristics were not considered. A system has yet to be produced that can quantitatively predict the degree of trustworthiness in a voice. This research aims to investigate the relationship between trustworthiness and different vocal features while considering the listener's physical characteristics, towards modelling a computational trust model. This study attempts to predict the degree of trustworthiness in voice by using an Artificial Neural Network (ANN) model. A set of 30 audio clips of white males were obtained, acoustically analyzed and then distributed to a large group of untrained Malaysian respondents who rated their degree of trust in the speakers of each audio clip on a scale of 0 to 10. The ANOVA test showed a statistically significant difference of trust ratings across different types and intensities of emotion, duration of audio clip, average fundamental frequencies, speech rates, articulation rates, average loudness, ethnicity of listener and ages of listener (p <.01). The findings conclude that Malaysians tend to trust white males who talk faster and longer, speak louder, have an f0 between 132.03Hz & 149.52Hz, and show a neutral emotion or rather stoic (arousal<.325). Results suggest that Indians are the most trusting Malaysian ethnic group, followed by Bumiputera from East Malaysia and then followed by Malays. Chinese are the least trusting Malaysian ethnic group. The data was fed into an ANN model to be evaluated, which yielded a perfect percentage accuracy (100%) in degree of trustworthiness 39.70% of the time. Given a threshold of two-point deviation, the ANN had a prediction accuracy of 76.86%. © 2020 ASTES Publishers. All rights reserved.","Artificial neural network; True; Voice; Zero acquaintance","","","","","","Xiamen University Malaysia, (XMUMRF/2019-C3/IECE/0006); Universiti Sains Malaysia, (1001/PKOMP/8014001, 304/PKOMP/6315137)","This work was supported by the Research Management Center, Xiamen University Malaysia under the XMUM Research Program Cycle 3 (Grant XMUMRF/2019-C3/IECE/0006). The authors also thank Universiti Sains Malaysia for the partial funding of this work from the grant no. 304/PKOMP/6315137 and 1001/PKOMP/8014001.","Ambady N., Krabbenhoft M. A., Hogan D., The 30-sec sale: Using thin-slice judgments to evaluate sales effectiveness, J. Consum. Psychol, 16, 1, pp. 4-13, (2006); Stirrat M., Perrett D. I., Valid Facial Cues to Cooperation and Trust: Male Facial Width and Trustworthiness, Psychol. Sci, 21, 3, pp. 349-354, (2010); Kenny D. A., West T. V, Zero acquaintance: Definitions, statistical model, findings, and process, First impressions, pp. 129-146, (2008); Ambady N., Rosenthal R., Half a minute: Predicting teacher evaluations from thin slices of nonverbal behavior and physical attractiveness, J. Pers. Soc. Psychol, 64, 3, (1993); Eisenkraft N., Accurate by way of aggregation: Should you trust your intuition-based first impressions?, J. Exp. Soc. Psychol, 49, 2, pp. 277-279, (2013); Hecht M. A., LaFrance M., How (Fast) Can I Help You? Tone of Voice and Telephone Operator Efficiency in Interactions 1, J. Appl. Soc. Psychol, 25, 23, pp. 2086-2098, (1995); Ambady N., Bernieri F. J., Richeson J. A., Toward a histology of social behavior: Judgmental accuracy from thin slices of the behavioral stream, 32, pp. 201-271, (2000); Mohammad S. M., Kiritchenko S., Using nuances of emotion to identify personality, Proc. ICWSM, (2013); Hohmann H. H., Malieva E., The concept of trust: Some notes on definitions, forms and sources, Trust Entrep.), pp. 7-23, (2005); Edwards C., Edwards A., Stoll B., Lin X., Massey N., Evaluations of an artificial intelligence instructor's voice: Social Identity Theory in human-robot interactions, Comput. Human Behav, 90, pp. 357-362, (2019); Goel A., Creeden B., Kumble M., Salunke S., Shetty A., Wiltgen B., Using watson for enhancing human-computer co-creativity, 2015 AAAI Fall Symposium Series, (2015); Folstad A., Nordheim C. B., Bjorkli C. A., What makes users trust a chatbot for customer service? An exploratory interview study, International Conference on Internet Science, pp. 194-208, (2018); Ciechanowski L., Przegalinska A., Magnuski M., Gloor P., In the shades of the uncanny valley: An experimental study of human-chatbot interaction, Futur. Gener. Comput. Syst, 92, pp. 539-548, (2019); Fenwick J., Barclay L., Schmied V., Chatting: an important clinical tool in facilitating mothering in neonatal nurseries, J. Adv. Nurs, 33, 5, pp. 583-593, (2001); Bos N., Olson J., Gergle D., Olson G., Wright Z., Effects of Four Computer-mediated Communications Channels on Trust Development, Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, pp. 135-140, (2002); Qiu L., Benbasat I., Online Consumer Trust and Live Help Interfaces: The Effects of Text-to-Speech Voice and Three-Dimensional Avatars, Int. J. Human-Computer Interact, 19, 1, pp. 75-94, (2005); Greenspan S., Goldberg D., Weimer D., Basso A., Interpersonal Trust and Common Ground in Electronically Mediated Communication, Proceedings of the 2000 ACM Conference on Computer Supported Cooperative Work, pp. 251-260, (2000); Schirmer A., Feng Y., Sen A., Penney T. B., Angry, old, male-and trustworthy? How expressive and person voice characteristics shape listener trust, PLoS One, 14, 1, (2019); Hughes S. M., Harrison M. A., Your Cheatin' Voice Will Tell on You: Detection of Past Infidelity from Voice, EPsychol, 15, 2, (2017); Belin P., Boehme B., McAleer P., The sound of trustworthiness: Acoustic-based modulation of perceived voice personality, PLoS One, 12, 10, pp. 1-9, (2017); Doney P. M., Cannon J. P., Mullen M. R., Understanding the influence of national culture on the development of trust, Acad. Manag. Rev, 23, 3, pp. 601-620, (1998); Gefen D., Heart T. H., On the need to include national culture as a central issue in e-commerce trust beliefs, J. Glob. Inf. Manag, 14, 4, pp. 1-30, (2006); Zaheer S., Zaheer A., Trust across borders, J. Int. Bus. Stud, 37, 1, pp. 21-29, (2006); Niebuhr O., Vosse J., Brem A., What makes a charismatic speaker? A computer-based acoustic-prosodic analysis of Steve Jobs tone of voice, Comput. Human Behav, 64, pp. 366-382, (2016); Anolli L., Ciceri R., The voice of deception: Vocal strategies of naive and able liars, J. Nonverbal Behav, 21, 4, pp. 259-284, (1997); Zuckerman M., DeFrank R. S., Hall J. A., Larrance D. T., Rosenthal R., Facial and vocal cues of deception and honesty, J. Exp. Soc. Psychol, 15, 4, pp. 378-396, (1979); Kirchhubel C., Howard D. M., Detecting suspicious behaviour using speech: Acoustic correlates of deceptive speech-An exploratory investigation, Appl. Ergon, 44, 5, pp. 694-702, (2013); Torre I., White L., Goslin J., Behavioural mediation of prosodic cues to implicit judgements of trustworthiness, (2016); Korovaiko N., Thomo A., Trust prediction from user-item ratings, Soc. Netw. Anal. Min, 3, 3, pp. 749-759, (2013); DuBois T., Golbeck J., Srinivasan A., Predicting trust and distrust in social networks, 2011 IEEE third international conference on privacy, security, risk and trust and 2011 IEEE third international conference on social computing, pp. 418-424, (2011); Zong B., Xu F., Jiao J., Lv J., A broker-assisting trust and reputation system based on artificial neural network, 2009 IEEE International Conference on Systems, Man and Cybernetics, pp. 4710-4715, (2009); Bejou D., Wray B., Ingram T. N., Determinants of relationship quality: an artificial neural network analysis, J. Bus. Res, 36, 2, pp. 137-143, (1996); Lee J. J., Knox B., Breazeal C., Modeling the dynamics of nonverbal behavior on interpersonal trust for human-robot interactions, 2013 AAAI Spring Symposium Series, (2013); Ekonomou L., Greek long-term energy consumption prediction using artificial neural networks, Energy, 35, 2, pp. 512-517, (2010); Imhof M., Listening to Voices and Judging People, Int. J. List, 24, 1, pp. 19-33, (2010); Smith B. L., Brown B. L., Strong W. J., Rencher A. C., Effects of Speech Rate on Personality Perception, Lang. Speech, 18, 2, pp. 145-152, (1975); Ismail M. N., Chee S. S., Nawawi H., Yusoff K., Lim T. O., James W. P. T., Obesity in Malaysia, Obes. Rev, 3, 3, pp. 203-208, (2002); Swami V., Furnham A., Self-assessed intelligence: Inter-ethnic, rural-urban, and sex differences in Malaysia, Learn. Individ. Differ, 20, 1, pp. 51-55, (2010); Barros P., Churamani N., Lakomkin E., Siqueira H., Sutherland A., Wermter S., The OMG-Emotion Behavior Dataset, Proceedings of the International Joint Conference on Neural Networks, pp. 2018-July, (2018); Ekman P., Friesen W. V., Constants across cultures in the face and emotion, J. Pers. Soc. Psychol, 17, 2, pp. 124-129, (1971); Trouvain J., Schmidt S., Schroder M., Schmitz M., Barry W. J., Modelling personality features by changing prosody in synthetic speech, (2006); Quene H., On the just noticeable difference for tempo in speech, J. Phon, 35, 3, pp. 353-362, (2007); Yulin G., The Spectrum of Trust and Distrust, Jiangsu Soc. Sci, 1, (2012)","S.L. Lutfi; Universiti Sains Malaysia, School of Computer Sciences, Penang, 11800, Malaysia; email: syahherah@usm.my; K. Siddique; Xiamen University Malaysia, Sepang, 43900, Malaysia; email: kamran.siddique@xmu.edu.my","","ASTES Publishers","","","","","","24156698","","","","English","Adv.  Sci., Technol.  Eng.  Syst.","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85092894298"
"Bello R.-W.; Talib A.Z.H.; Mohamed A.S.A.B.","Bello, Rotimi-Williams (57209469141); Talib, Abdullah Zawawi Hj (35570816900); Mohamed, Ahmad Sufril Azlan Bin (57190968285)","57209469141; 35570816900; 57190968285","Deep learning-based architectures for recognition of cow using cow nose image pattern","2020","Gazi University Journal of Science","33","3","","831","844","13","13","10.35378/gujs.605631","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090096917&doi=10.35378%2fgujs.605631&partnerID=40&md5=f63289f0e46e042875b36a64bbd523e2","School of Computer Sciences, Universiti Sains Malaysia, Pulau, 11800, Pinang, Malaysia","Bello R.-W., School of Computer Sciences, Universiti Sains Malaysia, Pulau, 11800, Pinang, Malaysia; Talib A.Z.H., School of Computer Sciences, Universiti Sains Malaysia, Pulau, 11800, Pinang, Malaysia; Mohamed A.S.A.B., School of Computer Sciences, Universiti Sains Malaysia, Pulau, 11800, Pinang, Malaysia","Stacked denoising auto-encoder and deep belief network are proposed as methods of deep learning for cow nose image texture feature extraction, and for learning the extracted features for better representation. While stacked denoising auto-encoder is applied for encoding and decoding of the extracted features, a deep belief network is applied for learning the extracted features and representing the cow nose image in feature space. Stacked denoising auto-encoder and deep belief network help in animal biometrics. Biometrics emanated from computer vision and pattern recognition and it plays an important role in the automated animal registration and identification process. Using the visual attributes of cow, and for the fact that the existing visual feature extraction and representation methods are not capable of handling cow recognition; deep belief network and stacked denoising auto-encoder are proposed. An experiment performed under different conditions of identification indicated that deep belief network outshines other methods with approximately 98.99% accuracy. 4000 cow nose images from an existing database of 400 individual cows contribute to the community of research especially in the animal biometrics for identification of individual cow. © 2020, Gazi University Eti Mahallesi. All rights reserved.","Animal biometrics; Cow nose image; DBN; Deep learning; SDAE","Animals; Deep learning; Extraction; Feature extraction; Image texture; Learning systems; Signal encoding; Textures; Animal biometric; Auto encoders; Cow nose image; DBN; De-noising; Deep belief networks; Deep learning; Image patterns; SDAE; Texture feature extraction; Biometrics","","","","","","","Kumar S., Singh S.K., Datta T., Gupta H.P., A fast cattle recognition system using smart devices, Proceedings of the 2016 ACM Conference on Multimedia, pp. 742-743, (2016); Noviyanto A., Arymurthy A.M., Automatic cattle identification based on muzzle photo using speed-up robust features approach, Proceedings of the 3rd European Conference of Computer Science, ECCS, 110, (2012); Kohl H.S., Burkhart T., Animal biometrics: quantifying and detecting phenotypic Appearance, Trends Ecol. Evol, 28, 7, pp. 432-441, (2013); Duyck J., Finn C., Hutcheon A., Vera P., Salas J., Ravela S., Sloop: a pattern retrieval engine for individual animal identification, Pattern Recogn, 48, 4, pp. 1059-1073, (2015); Nasirahmadi A., Richter U., Hensel O., Edwards S., Sturm B., Using machine vision for investigation of changes in pig group lying patterns, Computers and Electronics in Agriculture, 119, pp. 184-190, (2015); Wang Z., Fu Z., Chen W., Hu J., A rfid-based traceability system for cattle breeding in china, Proceedings of 2010 IEEE International Conference on Computer Application and System Modeling (ICCASM), 2, pp. V2-567, (2010); Krizhevsky A., Sutskever I., Hinton G., ImageNet classification with deep convolutional neural networks, Advances in Neural Information Processing Systems, pp. 1097-1105, (2012); Incetas M. O., Demirci R., Yavuzcan H. G., Automatic Color Edge Detection with Similarity Transformation, Gazi University Journal of Science, 32, 2, pp. 458-469, (2019); Sun Y., Wang X., Tang X., Deep convolutional network cascade for facial point detection, Proc. IEEE Conf. Comput. Vis. Pattern Recognit, pp. 3476-3483, (2013); Kumar S., Singh S.K., Visual animal biometrics: survey, IET Biometrics, 6, 3, pp. 139-156, (2016); Barron U.G., Butler F., McDonnell K., Ward S., The end of the identity crisis? Advances in biometric markers for animal identification, Irish Veterinary J, 62, 3, pp. 204-208, (2009); Reiter S., Sattlecker G., Lidauer L., Kickinger F., Ohlschuster M., Auer W., Iwersen M., Evaluation of an ear-tag-based accelerometer for monitoring rumination in dairy cows, Journal of Dairy Science, 101, 4, pp. 3398-3411, (2018); Seijas C., Montilla G., Frassato L., Identification of Rodent Species Using Deep Learning, Computación y Sistemas, 23, 1, (2019); Hansen M.F., Smith M.L., Smith L.N., Salter M.G., Baxter E.M., Farish M., Grieve B., Towards on-farm pig face recognition using convolutional neural networks, Computers in Industry, 98, pp. 145-152, (2018); Kumar S., Singh S.K., Cattle Recognition: A New Frontier in Visual Animal Biometrics Research, Proceedings of the National Academy of Sciences, India Section A: Physical Sciences, pp. 1-20, (2019); Norouzzadeh M.S., Nguyen A., Kosmala M., Swanson A., Palmer M.S., Packer C., Clune J., Automatically identifying, counting, and describing wild animals in camera-trap images with deep learning, Proceedings of the National Academy of Sciences, 115, 25, pp. E5716-E5725, (2018); Zin T.T., Phyo C.N., Tin P., Hama H., Kobayashi I., Image technology based cow identification system using deep learning, Proceedings of the International MultiConference of Engineers and Computer Scientists, 1, (2018); Kumar S., Pandey A., Satwik K.S.R., Kumar S., Singh S.K., Singh A.K., Mohan A., Deep learning framework for recognition of cattle using muzzle point image pattern, Measurement, 116, pp. 1-17, (2018); Iswanto I.A., Li B., Visual object tracking based on mean-shift and particle-Kalman filter, Procedia Computer Science, 116, pp. 587-595, (2017); Minagawa H., Fujimura T., Ichiyanagi M., Tanaka K., Fangquan M., Identification of beef cattle by analyzing images of their muzzle patterns lifted on paper, Proceedings of the 3rd Asian Conference for Information Technology in Asian Agricultural Information Technology & Management, pp. 596-600, (2002); Barry B., Gonzales-Barron U., McDonnell K., Butler F., Ward S., Using muzzle pattern recognition as a biometric approach for cattle identification, Trans. ASABE, 50, 3, pp. 1073-1080, (2007); Dalal N., Triggs B., Histograms of oriented gradients for human detection, Proc. CVPR, 1, pp. 886-893, (2005); Awad A.I., Zawbaa H.M., Mahmoud H.A., Nabi E.H.H.A., Fayed R.H., Hassanien A.E., A robust cattle identification scheme using muzzle print images, Proceedings of IEEE Federated Conference on Computer Science and Information Systems (FedCSIS), pp. 529-534, (2013); Noviyanto A., Arymurthy A.M., Beef cattle identification based on muzzle pattern using a matching refinement technique in the sift method, Comp. Electr. Agr, 99, pp. 77-84, (2013); Kumar S., Tiwari S., Singh S.K., Face recognition for cattle,  IEEE International Conference on Image Information Processing (ICIIP), pp. 65-72, (2015); Ehsani K., Bagherinezhad H., Redmon J., Mottaghi R., Farhadi A., Who let the dogs out? modeling dog behavior from visual data, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 4051-4060, (2018); Gaber T., Tharwat A., Hassanien A.E., Snasel V., Biometric cattle identification approach based on webers local descriptor and AdaBoost classifier, Comp. Electr. Agr, 122, pp. 55-66, (2016); Risha K.P., Chempak K.A., Sindhu C.S., Difference of Gaussian on Frame Differenced Image, International Journal of Innovative Research in Electrical, Electronics, Instrumentation and Control Engineering, 3, 1, pp. 92-95, (2016); Vincent P., Larochelle H., Lajoie I., Bengio Y., Manzagol P.-A., Stacked denoising auto-encoders: learning useful representations in a deep network with a local denoising criterion, JMLR, 11, pp. 3371-3408, (2010); Vincent P., Larochelle H., Bengio Y., Manzagol P.A., Extracting and composing robust features with denoising autoencoders, Proceedings of the 25th International Conference on Machine Learning, pp. 1096-1103, (2008); Bengio Y., Learning deep architectures for AI, Foundations and Trends in Machine Learning, 2, 1, pp. 1-127, (2009); Bengio Y., Courville A., Vincent P., Representation learning: A review and new perspectives, IEEE Transactions on Pattern Analysis and Machine Intelligence, 35, 8, pp. 1798-1828, (2013)","R.-W. Bello; School of Computer Sciences, Universiti Sains Malaysia, Pulau, 11800, Malaysia; email: sirbrw@yahoo.com","","Gazi Universitesi","","","","","","21471762","","","","English","GU J. Sci.","Article","Final","All Open Access; Bronze Open Access; Green Open Access","Scopus","2-s2.0-85090096917"
"Bello R.W.; Olubummo D.A.; Seiyaboh Z.; Enuma O.C.; Talib A.Z.; Mohamed A.S.A.","Bello, R.W. (57209469141); Olubummo, D.A. (57216345013); Seiyaboh, Z. (57218909817); Enuma, O.C. (57221132880); Talib, A.Z. (35570816900); Mohamed, A.S.A. (57190968285)","57209469141; 57216345013; 57218909817; 57221132880; 35570816900; 57190968285","Cattle identification: The history of nose prints approach in brief","2020","IOP Conference Series: Earth and Environmental Science","594","1","012026","","","","14","10.1088/1755-1315/594/1/012026","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098322994&doi=10.1088%2f1755-1315%2f594%2f1%2f012026&partnerID=40&md5=0d3041d52593cec60c8070445fce1db5","School of Computer Sciences, Universiti Sains Malaysia, Pulau, Pinang, 11800, Malaysia; Department of Mathematical Sciences, University of Africa, Toru-Orua, Bayelsa, Nigeria; Department of Computer and Information Systems, Robert Morris University, Moon-Township, PA, United States; Institute of Research in Applicable Computing (IRAC), University of Bedfordshire, LU1 3JU, United Kingdom; Department of Planning, Research and Statistics, Ministry of Health, Bayelsa, Nigeria","Bello R.W., School of Computer Sciences, Universiti Sains Malaysia, Pulau, Pinang, 11800, Malaysia, Department of Mathematical Sciences, University of Africa, Toru-Orua, Bayelsa, Nigeria; Olubummo D.A., Department of Computer and Information Systems, Robert Morris University, Moon-Township, PA, United States; Seiyaboh Z., Institute of Research in Applicable Computing (IRAC), University of Bedfordshire, LU1 3JU, United Kingdom; Enuma O.C., Department of Planning, Research and Statistics, Ministry of Health, Bayelsa, Nigeria; Talib A.Z., School of Computer Sciences, Universiti Sains Malaysia, Pulau, Pinang, 11800, Malaysia; Mohamed A.S.A., School of Computer Sciences, Universiti Sains Malaysia, Pulau, Pinang, 11800, Malaysia","Petersen was the first published paper to address cattle biometrics and identification problem by suggesting a permanent cattle identification method based on nose print principles widely accepted today. His major concern was on proper identification of cattle for registration and of cattle on an official test so that the possibility of swapping, false insurance claims, and ownership disputes can be guarded against. It was with this identification problem in the mind of every breeder that the practicable suggestion of using nose print as means of identification was made by O. H. Baker of the American Jersey Cattle Club in Petersen's paper entitled ""The identification of the bovine by means of nose-prints"". Before the advent of the nose print method, cattle identification has been by conventional constructs such as tattoo, tags, photographs, descriptions, branding (hot and freeze), ear notching, and sketching (drawings) the color markings on them on paper for registration and identification purposes. These classical methods of identification cause trouble among the breeders especially when their cattle are sold or are on an official test due to lack of artistic ability on the part of the breeders which makes the matching of the sketches and the markings on the cattle disagree. Presented in this paper are the various cattle biometrics and identification methods, most especially from the classical methods to the modern methods.  © Published under licence by IOP Publishing Ltd.","","Biometrics; Insurance; Mammals; Classical methods; Color markings; Identification method; Identification problem; Insurance claims; Petersen; Agriculture","","","","","","","Petersen W E, The identification of the bovine by means of nose-prints, J. Dairy Sci, 5, pp. 249-258, (1922); Neary M, Yager A, Methods of livestock identification, (2002); Blancou J, A history of the traceability of animals and animal products, Revue Scientifique et Technique-Office International des Epizooties, 20, pp. 420-425, (2001); Ghirardi J J, Caja G, Garin D, Casellas J, Hernandez-Jover M, Evaluation of the retention of electronic identification boluses in the forestomachs of cattle, J. Anim. Sci, 84, pp. 2260-2268, (2006); Bello R W, Abubakar S, Development of a software package for cattle identification in Nigeria, J. Appl. Sci. Environ. Manag, 23, pp. 1825-1828, (2019); Bello R W, Moradeyo O M, Monitoring cattle grazing behavior and intrusion using global positioning system and virtual fencing, Asian J. Mathemat. Sci, 3, pp. 4-14, (2019); Bello R W, Abubakar S, Framework for Modeling Cattle Behavior through Grazing Patterns, Asian J. Mathemat. Sci, 4, pp. 75-79, (2020); Bello R W, Talib A Z H, Mohamed A S A B, A framework for real-Time cattle monitoring using multimedia networks, Intern. J. Recent Technol. Engin, 8, pp. 974-979, (2020); Marchant J, Secure animal identification and source verification, J M Communications UK, pp. 1-28, (2002); Bello R W, Talib A Z, Mohamed A S A, Olubummo D A, Otobo F N, Image-based Individual Cow Recognition using Body Patterns, Intern. J. Adv. Comp. Sci. Appl, 11, pp. 92-98, (2020); Bello R, Talib A, Mohamed A, Deep learning-based architectures for recognition of cow using cow nose image pattern Gazi Uni, J. Sci, pp. 1-1, (2020); Cai C, Li J, Cattle face recognition using local binary pattern descriptor, 2013 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference, pp. 1-4, (2013); Zin T T, Phyo C N, Tin P, Hama H, Kobayashi I, Image technology based cow identification system using deep learning, Lecture Notes in Engineering and Computer Science In Proceedings of the International MultiConference of Engineers and Computer Scientists, 1, pp. 320-323, (2018); Kumar S, Singh S K, Automatic identification of cattle using muzzle point pattern: A hybrid feature extraction and classification paradigm Multim, Tools Appl, 76, pp. 26551-26580, (2017); Kumar S, Pandey A, Satwik K S R, Kumar S, Singh S K, Singh A K, Mohan A, Deep learning framework for recognition of cattle using muzzle point image pattern Measurement, J. Intern. Measurem. Confeder, 116, pp. 1-17, (2018); Kumar S, Singh S K, Cattle recognition: A new frontier in visual animal biometrics research, Proceedings of the National Academy of Sciences India Section A Physical Sciences, pp. 1-20, (2019); Mahmoud H A, Hadad H M R E, Automatic cattle muzzle print classification system using multiclass support vector machine, International Journal of Image Mining (IJIM), 1, pp. 126-140, (2015); Ahmed S, Gaber T, Tharwat A, Hassanien A E, Snael V, Muzzle-based cattle identification using speed up robust feature approach 2015, International Conference on Intelligent Networking and Collaborative Systems, pp. 99-104, (2015); Evans J, Van Eenennaam A, Livestock identification. An introduction to electronic animal identifications systems and comparison of technologies Emerging management systems in animal identification, pp. 1-12, (2005); Allen A, Golden B, Taylor M, Patterson D, Henriksen D, Skuce R, Evaluation of retinal imaging technology for the biometric identification of bovine animals in northern Ireland, Livest Sci, 116, pp. 42-52, (2008)","R.W. Bello; School of Computer Sciences, Universiti Sains Malaysia, Pulau, Pinang, 11800, Malaysia; email: sirbrw@yahoo.com","Carranca C.; Feng X.","IOP Publishing Ltd","","6th International Conference on Agricultural and Biological Sciences, ABS 2020","23 August 2020 through 26 August 2020","Virtual, Online","165910","17551307","","","","English","IOP Conf. Ser. Earth Environ. Sci.","Conference paper","Final","All Open Access; Bronze Open Access; Green Open Access","Scopus","2-s2.0-85098322994"
"Ibrahim N.; Hassan F.H.; Azlan Mohamed A.S.; Khader A.T.","Ibrahim, Najihah (57198067619); Hassan, Fadratul Hafinaz (36809487100); Azlan Mohamed, Ahmad Sufril (57190968285); Khader, Ahamad Tajudin (24724794600)","57198067619; 36809487100; 57190968285; 24724794600","The Impact of Spatial Layout Design on the Pedestrian Movement during Panic Situation: Pedestrian Survival Prediction","2019","Journal of Physics: Conference Series","1201","1","012066","","","","0","10.1088/1742-6596/1201/1/012066","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067638748&doi=10.1088%2f1742-6596%2f1201%2f1%2f012066&partnerID=40&md5=232c96ad074a8672b576c6dac40ed732","School of Computer Sciences, Universiti Sains Malaysia, Pulau Pinang, 11800, Malaysia","Ibrahim N., School of Computer Sciences, Universiti Sains Malaysia, Pulau Pinang, 11800, Malaysia; Hassan F.H., School of Computer Sciences, Universiti Sains Malaysia, Pulau Pinang, 11800, Malaysia; Azlan Mohamed A.S., School of Computer Sciences, Universiti Sains Malaysia, Pulau Pinang, 11800, Malaysia; Khader A.T., School of Computer Sciences, Universiti Sains Malaysia, Pulau Pinang, 11800, Malaysia","Crowd management has become the global issue due to the fatal incidents happened that involve the high casualties of pedestrians. During panic situation, the survival rate of the pedestrians are depends on the several parameter values that are hard to discover due to the catastrophic case scenarios and almost impossible for the scene to be created due to the human ethical conduct. Hence, with the advance of computation technique and the improvement of Artificial Intelligence (AI) process automation, the panic situations are able to be visualized with the computer simulation. However, based on the previous simulation researches, there are a lot of features that had been found that are able to influence the pedestrian movement and caused the clogging region on the spatial layout. Hence, this research has been conducted to justify and analyze the findings of influential features by simulating the near-realistic pedestrian movement using Cellular Automata (CA) approach to re-enact the real behavioral actions of the pedestrians during panic situation. Based on the near-realistic simulation and the construction of the spatial layout design based on the features that promotes the clogging region experiments, the results had shown that the structural arrangement of a space is the proper solution to increase the pedestrian survival rate and decrease the clogging region. However, the existing structural also can enhance the safety quality of the space by rearranging the obstacles in the spatial layout. This research had introduced the granularity based approach and proves that the implementation of the fine-grain obstacles' arrangement while accommodating the interior design standard may reduce the time (10 seconds, 0.1%) taken for evacuation process and increase the probability of movement directions for the pedestrians to ensure the balance usage of the egress points. © Published under licence by IOP Publishing Ltd.","","Physics; Computation techniques; Evacuation process; Pedestrian movement; Process automation; Realistic simulation; Simulation research; Structural arrangement; Survival prediction; Architectural design","","","","","Ministry of Higher Education, Malaysia, MOHE","Research experiments reported were pursued under the Bridging Grant by Universiti Sains Malaysia [304.PKOMP.6316019], Research University Grant by Universiti Sains Malaysia [1001.PKOMP.8014073] and Fundamental Research Grant Scheme (FRGS) by Ministry of Education Malaysia [203.PKOMP.6711534] and [203.PKOMP.6711713].","Four in a Family Killed in Fire, The Star Online, (2017); Lu X., Luh P.B., Tucker A., Gifford T., Astur R.S., Olderman N., Impacts of Anxiety in Building Fire and Smoke Evacuation: Modeling and Validation, IEEE Robotics and Automation Letters, 2, 1, pp. 255-260, (2017); Henderson B., Graham C., The Telegraph, (2017); Jay B.N., Tahfiz did not have fire exit; Bodies found piled on top of each other, New Straits Times, (2017); Zong X., Jiang Y., Pedestrian-vehicle mixed evacuation model based on multi-particle swarm optimization, 2016 11th International Conference on Computer Science & Education (ICCSE), pp. 568-572, (2016); Konstantara K., Dourvas N.I., Georgoudas I.G., Sirakoulis G.C., Parallel Implementation of a Cellular Automata-Based Model for Simulating Assisted Evacuation of Elderly People, 2016 24th Euromicro International Conference on Parallel, Distributed, and Network-Based Processing (PDP), pp. 702-709, (2016); Ruiz S., Hernandez B., A Parallel Solver for Markov Decision Process in Crowd Simulations, 2015 Fourteenth Mexican International Conference on Artificial Intelligence (MICAI), pp. 107-116, (2015); Hall J.R., High-Rise Building Fires, (2011); Yan Z., Han X., Li M., Accurate Assessment of RSET for Building Fire Based on Engineering Calculation and Numerical Simulation, MATEC Web of Conferences, (2016); Huixian J., Shaoping Z., Navigation system design of fire disaster evacuation path in buildings based on mobile terminals, 2016 11th International Conference on Computer Science & Education (ICCSE), pp. 327-331, (2016); Xueling J., Simulation Model of Pedestrian Evacuation in High-Rise Building: Considering Group Behaviors and Real-Time Fire, International Journal of Smart Home, 9, 2, (2015); Chen Y., Cai Y., Li P., Zhang G., Study on Evacuation Evaluation in Subway Fire Based on Pedestrian Simulation Technology, Mathematical Problems in Engineering, 2015, (2015); Sime J.D., Crowd psychology and engineering, Safety Science, 21, 1, pp. 1-14, (1995); Tcheukam A., Djehiche B., Tembine H., Evacuation of multi-level building: Design, control and strategic flow, 2016 35th Chinese Control Conference (CCC), (2016); Wang H., Simulation research based on evacuation ability estimation method, 2016 12th World Congress on Intelligent Control and Automation (WCICA), pp. 645-649, (2016); Hassan F.H., Using microscopic pedestrian simulation statistics to find clogging regions, 2016 SAI Computing Conference (SAI), pp. 156-160, (2016); Miao Q., Lv Y., Zhu F., A cellular automata based evacuation model on GPU platform, 2012 15th International IEEE Conference on Intelligent Transportation Systems, pp. 764-768, (2012); Helbing D., Johansson A., Encyclopedia of Complexity and Systems Science, pp. 1-28, (2009); Helbing D., Farkas I., Vicsek T., Simulating dynamical features of escape panic, Nature, 407, 6803, pp. 487-490, (2000); Helbing D., Farkas I.J., Molnar P., Vicsek T., Simulation of pedestrian crowds in normal and evacuation situations, Pedestrian and Evacuation Dynamics, 21, pp. 21-58, (2002); Yamin M., Al-Ahmadi H.M., Muhammad A.A., Integrating social media and mobile apps into Hajj management, 2016 3rd International Conference on Computing for Sustainable Global Development (INDIACom), pp. 1368-1372, (2016); Doshi Eloise Stevens V., Stampede at Indian train station kills at least 22, The Washington Post, (2017); Zhang L., Lai D., Miranskyy A.V., The impact of position errors on crowd simulation, Simulation Modelling Practice and Theory, 90, pp. 45-63; Kihlstrom J.F., The person-situation interaction, The Oxford Handbook of Social Cognition, pp. 786-805, (2013); Winkens A., Rupprecht T., Seyfried A., Klingsch W., Empirical study of pedestrians' characteristics at bottlenecks, Pedestrian and Evacuation Dynamics 2008, pp. 263-268, (2010); Forell B., Seidenspinner R., Hosser D., Quantitative comparison of international design standards of escape routes in assembly buildings 2010, Pedestrian and Evacuation Dynamics, pp. 791-801, (2008)","F.H. Hassan; School of Computer Sciences, Universiti Sains Malaysia, Pulau Pinang, 11800, Malaysia; email: fadratul@usm.my","Wibowo F.W.","Institute of Physics Publishing","Hemispheres","International Conference on Electronics Representation and Algorithm 2019, ICERA 2019","29 January 2019 through 30 January 2019","Yogyakarta","148736","17426588","","","","English","J. Phys. Conf. Ser.","Conference paper","Final","All Open Access; Bronze Open Access","Scopus","2-s2.0-85067638748"
"Wahab M.N.A.; Mohamed A.S.A.; Sukor A.S.A.; Teng O.C.","Wahab, Mohd Nadhir Ab (36471236100); Mohamed, Ahmad Sufril Azlan (57190968285); Sukor, Abdul Syafiq Abdull (57209073616); Teng, Ong Chia (57209690411)","36471236100; 57190968285; 57209073616; 57209690411","Text Reader for Visually Impaired Person","2021","Journal of Physics: Conference Series","1755","1","012055","","","","7","10.1088/1742-6596/1755/1/012055","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102407044&doi=10.1088%2f1742-6596%2f1755%2f1%2f012055&partnerID=40&md5=c2c4db8d3189c3a5299630e0f2f124a9","School of Computer Sciences, Universiti Sains Malaysia, Penang, 11800, Malaysia; Centre of Advanced Sensor Technology (CEASTech), Universiti Malaysia Perlis, Perlis, Arau, 02600, Malaysia","Wahab M.N.A., School of Computer Sciences, Universiti Sains Malaysia, Penang, 11800, Malaysia; Mohamed A.S.A., School of Computer Sciences, Universiti Sains Malaysia, Penang, 11800, Malaysia; Sukor A.S.A., Centre of Advanced Sensor Technology (CEASTech), Universiti Malaysia Perlis, Perlis, Arau, 02600, Malaysia; Teng O.C., School of Computer Sciences, Universiti Sains Malaysia, Penang, 11800, Malaysia","There are approximately 1.3 billion people in the world have visual impairment issue. They usually have to read printed material using Braille. However, there are limitations for these people when the material is not printed in Braille. Although there is much electronic equipment that can help them to read, the prices are too expensive to afford. Thus, this paper proposes an affordable mobile application which is designed for the visually impaired person. The mobile application is able to capture the image of printed material with a mobile camera. The captured image is then converted to text by using image-to-text conversion in Optical Character Recognition (OCR) framework. Finally, the text will be read out into speech format using text-to-speech conversion in Text to Speech (TTS) framework. As a result, a person who has visual impairment can understand the printed material which is not written in Braille through listening instead of touching. Some alert sound is provided to allow the users to know what exactly happened in the mobile application. It is user friendly for the visually impaired person since the designed system has sound for guideline so they can always get to know the process of the application. © 2021 Published under licence by IOP Publishing Ltd.","","Electronic equipment; Ophthalmology; Optical character recognition; Mobile applications; Optical character recognition (OCR); Printed materials; Text conversion; Text to speech; Text-to-speech conversion; Visual impairment; Visually impaired persons; Mobile computing","","","","","Universiti Malaysia Perlis; Centre of Advanced Sensor and Technology; Universiti Sains Malaysia","This project is supported by USM Short Term Grant (PKOMP/6315262) and part of the collaboration project under Robotics, Computer Vision, and Image Processing (RCVIP) Research Group of Universiti Sains Malaysia (USM) and Centre of Advanced Sensor and Technology (CEASTech), University Malaysia Perlis (UniMAP).","(2014); Organization W. H., International Ststistical Classification of Diseases and Related Health Problems, (2011); I. S. for the E. of Eyesight 2006 20/20 Vision Activity-Eye Chart; Bowen M., Et al., The Prevalence of Visual Impairment in People with Dementia (the PrOVIDe study): a cross-sectional study of people aged 60-89 years with dementia and qualitative exploration of individual, carer and professional perspectives, Heal. Serv. Deliv. Res, 4, pp. 1-200, (2016); Saba T., Sulong G., Rehman A., A Survey on Methods and Strategies on Touched Characters Segmentation, Int. J. Res. Rev. Comput. Sci, 1, pp. 103-114, (2010); Vijayabharathi K., Mahalakshmi V., Implementation of OCR Using Raspberry Pi for Visually Impaired Person, Int. J. Pure Appl. Math, 119, pp. 111-117, (2018); Dimitrova D., Students with Visual Impairments: Braille Reading Rate, Int. J. Cogn. Res. Sci. Eng. Educ, 3, pp. 1-6, (2015); Vader L. A., Measuring Vision and Vision Loss Nurs, Clin. North Am, 27, pp. 705-714, (2009); Ashrafi E., Et al., National and sub-national burden of visual impairment in Iran 1990-2013, Study protocol Arch. Iran. Med, 17, pp. 810-815, (2014); Singla S. K., Yadav R. K., Optical character recognition based speech synthesis system using LabVIEW, J. Appl. Res. Technol, 12, pp. 919-926, (2014); Jondhale N., Gupta S., Reading text extracted from an image using OCR and android Text to Speech, Int. J. Latest Eng. Manag. Res. (IJLEMR), (2018); Esmaeel H., Apply Android Studio (SDK) Tools, Int. J. Adv. Res. Comput. Sci. Softw. Eng, 5, pp. 88-92, (2019)","M.N.A. Wahab; School of Computer Sciences, Universiti Sains Malaysia, Penang, 11800, Malaysia; email: mohdnadhir@usm.my","","IOP Publishing Ltd","","5th International Conference on Electronic Design, ICED 2020","19 August 2020","Perlis, Virtual","167462","17426588","","","","English","J. Phys. Conf. Ser.","Conference paper","Final","All Open Access; Bronze Open Access","Scopus","2-s2.0-85102407044"
"Zhonglin T.; Wahab M.N.A.; Akbar M.F.; Mohamed A.S.A.; Noor M.H.M.; Rosdi B.A.","Zhonglin, Tian (57222069782); Wahab, Mohd Nadhir Ab (36471236100); Akbar, Muhammad Firdaus (57195979493); Mohamed, Ahmad Sufril Azlan (57190968285); Noor, Mohd Halim Mohd (36656106400); Rosdi, Bakhtiar Affendi (14525363000)","57222069782; 36471236100; 57195979493; 57190968285; 36656106400; 14525363000","SFFSORT Multi-Object Tracking by Shallow Feature Fusion for Vehicle Counting","2023","IEEE Access","11","","","76827","76841","14","0","10.1109/ACCESS.2023.3297190","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85165387856&doi=10.1109%2fACCESS.2023.3297190&partnerID=40&md5=29ad760996cb557ed5beed6e2c931a0d","Universiti Sains Malaysia, School of Computer Sciences, Minden, 11800, Malaysia; Universiti Sains Malaysia, Engineering Campus, Nibong Tebal, School of Electrical and Electronic Engineering, Penang, 14300, Malaysia","Zhonglin T., Universiti Sains Malaysia, School of Computer Sciences, Minden, 11800, Malaysia; Wahab M.N.A., Universiti Sains Malaysia, School of Computer Sciences, Minden, 11800, Malaysia; Akbar M.F., Universiti Sains Malaysia, Engineering Campus, Nibong Tebal, School of Electrical and Electronic Engineering, Penang, 14300, Malaysia; Mohamed A.S.A., Universiti Sains Malaysia, School of Computer Sciences, Minden, 11800, Malaysia; Noor M.H.M., Universiti Sains Malaysia, School of Computer Sciences, Minden, 11800, Malaysia; Rosdi B.A., Universiti Sains Malaysia, Engineering Campus, Nibong Tebal, School of Electrical and Electronic Engineering, Penang, 14300, Malaysia","Standard Multi-Object Tracking (MOT) frameworks are currently categorised into three categories: tracking-by-detection, joint detection, and tracking and attention mechanisms. Infrequently, the latter two frameworks require substantial computing resources. The difficulty of implementing real-time tracking does not apply to vehicle detection at traffic crossings. Not only is it essential to meet real-time requirements for vehicle tracking and detection at traffic intersections, but it is also necessary to address common MOT issues such as target occlusion, repetition technology, error detection, etc. Detection-based tracking has a great deal of potential. This study proposed a shallow feature fusion algorithm based on SORT, called SFFSORT and developed an innovative architecture for vehicle monitoring and counting based on detection tracking. This tracking method is more efficient than both SORT and DeepSORT. It achieved 60.9% MOTA and 65.5% IDF1 in MOT16 while MOTA achieved 60.1% and 64.7% IDF1 in MOT17. Utilizing this tracking method as a foundation, we have developed a vehicle counting framework and successfully implemented it on road traffic videos sourced from the Malaysian transportation department. The tracking algorithm presented here effectively addresses the tracking challenges arising from both detection errors and inaccuracies, providing a robust solution. The experimental findings demonstrate that the deep learning framework is capable of achieving lane-level vehicle counting even in scenarios with limited labelled data.  © 2013 IEEE.","deep learning; Intelligent transportation; multi-object tracking; transfer learning; video tracking","Deep learning; Feature extraction; Intelligent systems; Interactive computer systems; Multiobjective optimization; Object detection; Pareto principle; Target tracking; Vehicles; Deep learning; Intelligent transportation; Intelligent transportation systems; Multi-object tracking; Pareto-optimization; Real - Time system; Targets tracking; Transfer learning; Vehicles detection; Video-tracking; Real time systems","","","","","","","Mandellos N.A., Keramitsoglou I., Kiranoudis C.T., A background subtraction algorithm for detecting and tracking vehicles, Expert Syst. Appl., 38, 3, pp. 1619-1631, (2011); Lin H., Yuan Z., He B., Kuai X., Li X., Guo R., A deep learning framework for video-based vehicle counting, Frontiers Phys, 10; Kasturi R., Goldgof D., Soundararajan P., Manohar V., Garofolo J., Bowers R., Boonstra M., Korzhova V., Zhang J., Framework for performance evaluation of face, text, and vehicle detection and tracking in video: Data, metrics, and protocol, IEEE Trans. Pattern Anal. Mach. Intell., 31, 2, pp. 319-336, (2009); Wang W., Gee T., Price J., Qi H., Real time multi-vehicle tracking and counting at intersections from a fisheye camera, Proc. IEEE Winter Conf. Appl. Comput. Vis., pp. 17-24, (2015); Dai Z., Song H., Wang X., Fang Y., Yun X., Zhang Z., Li H., Video-based vehicle counting framework, IEEE Access, 7, pp. 64460-64470, (2019); Rosin P., Ellis T., Image difference threshold strategies and shadow detection, Proc. Brit. Mach. Vis. Conf., (1995); Chen Z., Cao J., Tang Y., Tang L., Tracking of moving object based on optical flow detection, Proc. Int. Conf. Comput. Sci. Netw. Technol., 2, pp. 1096-1099, (2011); Redmon J., Divvala S., Girshick R., Farhadi A., You only look once: Unified, real-time object detection, Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), pp. 779-788, (2016); Redmon J., Farhadi A., YOLOv3: An incremental improvement, (2018); Bochkovskiy A., Wang C.-Y., Mark Liao H.-Y., YOLOv4: Optimal speed and accuracy of object detection; Liu W., Anguelov D., Erhan D., Szegedy C., Reed S., Fu C.-Y., Berg A.C., SSD: Single shot MultiBox detector, Proc. Eur. Conf. Comput. Vis., pp. 21-37, (2016); Ren S., He K., Girshick R., Sun J., Faster R-CNN: Towards realtime object detection with region proposal networks, IEEE Trans. Pattern Anal. Mach. Intell., 39, 6, pp. 1137-1149, (2017); Bewley A., Ge Z., Ott L., Ramos F., Upcroft B., Simple online and realtime tracking, Proc. IEEE Int. Conf. Image Process. (ICIP), pp. 3464-3468, (2016); Wojke N., Bewley A., Paulus D., Simple online and realtime tracking with a deep association metric, (2017); Zhou X., Koltun V., Krahenbuhl P., Tracking objects as points; Zhang Y., Wang C., Wang X., Zeng W., Liu W., FairMOT: On the fairness of detection and re-identification in multiple object tracking, Int. J. Comput. Vis., 129, 11, pp. 3069-3087; Peng J., Wang C., Wan F., Wu Y., Wang Y., Tai Y., Wang C., Li J., Huang F., Fu Y., Chained-tracker: Chaining paired attentive regression results for end-to-end joint multiple-object detection and tracking; Meinhardt T., Kirillov A., Leal-Taixe L., Feichtenhofer C., Track-Former: Multi-object tracking with transformers, Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), pp. 8834-8844, (2022); Sun P., Cao J., Jiang Y., Zhang R., Xie E., Yuan Z., Wang C., Luo P., TransTrack: Multiple object tracking with transformer,; Yang T., Liang R., Huang L., Vehicle counting method based on attention mechanism SSD and state detection, Vis. Comput., 38, 8, pp. 2871-2881, (2022); Liang H., Song H., Li H., Dai Z., Vehicle counting system using deep learning and multi-object tracking methods, Transp. Res. Rec., J. Transp. Res. Board, 2674, 4, pp. 114-128; Bui K.N., Yi H., Cho J., A vehicle counts by class framework using distinguished regions tracking at multiple intersections, Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. Workshops (CVPRW), pp. 2466-2474, (2020); Milan A., Leal-Taixe L., Reid I., Roth S., Schindler K., MOT16: A benchmark for multi-object tracking, (2016); Benjumea A., Teeti I., Cuzzolin F., Bradley A., YOLO-Z: Improving small object detection in YOLOv5 for autonomous vehicles; Girshick R., Donahue J., Darrell T., Malik J., Rich feature hierarchies for accurate object detection and semantic segmentation, Proc. IEEE Conf. Comput. Vis. Pattern Recognit., pp. 580-587, (2014); Wang H., Yu Y., Cai Y., Chen X., Chen L., Liu Q., A comparative study of state-of-the-art deep learning algorithms for vehicle detection, IEEE Intell. Transp. Syst. Mag., 11, 2, pp. 82-95, (2019); Chen X., Fang H., Lin T.-Y., Vedantam R., Gupta S., Dollar P., Zitnick C.L., Microsoft COCO captions: Data collection and evaluation server, (2015); Yu J., Jiang Y., Wang Z., Cao Z., Huang T., UnitBox: An advanced object detection network, Proc. 24th ACM Int. Conf. Multimedia, pp. 516-520, (2016); Zheng Z., Wang P., Liu W., Li J., Ye R., Ren D., Distance-IoU loss: Faster and better learning for bounding box regression, (2019); Zheng Z., Wang P., Ren D., Liu W., Ye R., Hu Q., Zuo W., Enhancing geometric factors in model learning and inference for object detection and instance segmentation; Rezatofighi H., Tsoi N., Gwak J., Sadeghian A., Reid I., Savarese S., Generalized intersection over union: A metric and a loss for bounding box regression, Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), pp. 658-666, (2019); Zhang Y., Sun P., Jiang Y., Yu D., Weng F., Yuan Z., Luo P., Liu W., Wang X., ByteTrack: Multi-object tracking by associating every detection box; Leal-Taixe L., Milan A., Reid I., Roth S., Schindler K., MOTChallenge 2015: Towards a benchmark for multi-target tracking, (2015); Dendorfer P., Rezatofighi H., Milan A., Shi J., Cremers D., Reid I., Roth S., Schindler K., Leal-Taixe L., MOT20: A benchmark for multi object tracking in crowded scenes; Bergmann P., Meinhardt T., Leal-Taixe L., Tracking without bells and whistles, Proc. IEEE/CVF Int. Conf. Comput. Vis. (ICCV), pp. 941-951, (2019); Dai P., Weng R., Choi W., Zhang C., He Z., Ding W., Learning a proposal classifier for multiple object tracking, Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), pp. 2443-2452, (2021); Yang J., Ge H., Yang J., Tong Y., Su S., Online multi-object tracking using multi-function integration and tracking simulation training, Appl. Intell., 52, 2, pp. 1268-1288, (2022); Kim C., Fuxin L., Alotaibi M., Rehg J.M., Discriminative appearance modeling with multi-track pooling for real-time multi-object tracking, Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), pp. 9548-9557, (2021); He Y., Wei X., Hong X., Ke W., Gong Y., Identity-quantity harmonic multi-object tracking, IEEE Trans. Image Process., 31, pp. 2201-2215; Kalman R.E., A new approach to linear filtering and prediction problems, J. Basic Eng., 82, 1, pp. 35-45, (1960); Jagannathan P., Rajkumar S., Frnda J., Divakarachari P.B., Subramani P., Moving vehicle detection and classification using Gaussian mixture model and ensemble deep learning technique, Wireless Commun. Mobile Comput., 2021, pp. 1-15, (2021); Zhou A., Xie W., Pei J., Background modeling combined with multiple features in the Fourier domain for maritime infrared target detection, IEEE Trans. Geosci. Remote Sens., 60; Yin G., Yu M., Wang M., Hu Y., Zhang Y., Research on highway vehicle detection based on faster R-CNN and domain adaptation, Int. J. Speech Technol., 52, 4, pp. 3483-3498; Lin C.-J., Jhang J.-Y., Intelligent traffic-monitoring system based on YOLO and convolutional fuzzy neural networks, IEEE Access, 10, pp. 14120-14133; Khalkhali M.B., Vahedian A., Yazdi H.S., Situation assessment-augmented interactive Kalman filter for multi-vehicle tracking, IEEE Trans. Intell. Transp. Syst., 23, 4, pp. 3766-3776, (2022); Hassaballah M., Kenk M.A., Muhammad K., Minaee S., Vehicle detection and tracking in adverse weather using a deep learning framework, IEEE Trans. Intell. Transp. Syst., 22, 7, pp. 4230-4242; Hou X., Wang Y., Chau L.-P., Vehicle tracking using deep SORT with low confidence track filtering, Proc. 16th IEEE Int. Conf. Adv. Video Signal Based Surveill. (AVSS), pp. 1-6, (2019); Valdivieso Tituana D.E., Yoo S.G., Andrade R.O., Vehicle counting using computer vision: A survey, Proc. IEEE 7th Int. Conf. Converg. Technol. (I2CT), pp. 1-8, (2022); Najm M., Ali Y.H., Automatic vehicles detection, classification and counting techniques/survey, Iraqi J. Sci., 61, 7, pp. 1811-1822; Mandal V., Adu-Gyamfi Y., Object detection and tracking algorithms for vehicle counting: A comparative analysis, J. Big Data Anal. Transp., 2, 3, pp. 251-261; Li H., Wang P., Huang C., Comparison of deep learning methods for detecting and counting sorghum heads in UAV imagery, Remote Sens, 14, 13, (2022); Berwo M.A., Khan A., Fang Y., Fahim H., Javaid S., Mahmood J., Abideen Z.U., Deep learning techniques for vehicle detection and classification from images/videos: A survey, Sensors, 23, 10, (2023)","M.N.A. Wahab; Universiti Sains Malaysia, School of Computer Sciences, Minden, 11800, Malaysia; email: mohdnadhir@usm.my; M.F. Akbar; Universiti Sains Malaysia, Engineering Campus, Nibong Tebal, School of Electrical and Electronic Engineering, Penang, 14300, Malaysia; email: firdaus.akbar@usm.my","","Institute of Electrical and Electronics Engineers Inc.","","","","","","21693536","","","","English","IEEE Access","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85165387856"
"Halim M.A.A.; Ruhaiyem N.I.R.; Fauzi E.R.I.; Jamil M.S.C.; Mohamed A.S.A.","Halim, M.A.A. (57192171962); Ruhaiyem, N.I.R. (57190964192); Fauzi, E.R.I. (56641781800); Jamil, M.S.C. (36727346400); Mohamed, A.S.A. (57190968285)","57192171962; 57190964192; 56641781800; 36727346400; 57190968285","Automatic laser welding defect detection and classification using sobel-contour shape detection","2016","Journal of Telecommunication, Electronic and Computer Engineering","8","6","","157","160","3","6","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84999143636&partnerID=40&md5=defe81e1e55fd4dea02b6484d23f303e","School of Computer Sciences, Universiti Sains Malaysia, Penang, 11800, Malaysia; School of Mechanical Engineering, Universiti Sains Malaysia, Engineering Campus, Nibong Tebal, Penang, 14300, Malaysia","Halim M.A.A., School of Computer Sciences, Universiti Sains Malaysia, Penang, 11800, Malaysia; Ruhaiyem N.I.R., School of Computer Sciences, Universiti Sains Malaysia, Penang, 11800, Malaysia; Fauzi E.R.I., School of Mechanical Engineering, Universiti Sains Malaysia, Engineering Campus, Nibong Tebal, Penang, 14300, Malaysia; Jamil M.S.C., School of Mechanical Engineering, Universiti Sains Malaysia, Engineering Campus, Nibong Tebal, Penang, 14300, Malaysia; Mohamed A.S.A., School of Computer Sciences, Universiti Sains Malaysia, Penang, 11800, Malaysia","This paper describes a detection of common defects in laser welding of structural aluminum alloy. To overcome these problems, a technique has been proposed to detect defects automatically and effectively using the image segmentation technique. Although, this technique has been well developed, it does suffer from several disadvantages of radiographic images taken to be poor in quality, as well as the microscopic size of the defects together with poor orientation relatively to the small size and thickness of the evaluated parts. Using image segmentation algorithm allows the defects to be automatically inspected and measured within the welded surface such as cracks, porosity and foreign inclusions, which may be weakening the welded parts. This paper proposes a system to automatically identifies and classifies the faults from the welding process by using the existing image segmentation algorithms. The output of the developed system produces a measured analysis which can be then used to describe the mechanical properties of welded part of the alloy such as its tensile and force. The benefits of this project will improve the welding process to reduce faults and defects for both constructing and manufacturing fields.","Automatic laser welding; Classification; Defect detection; Sobel-contour shape","","","","","","","","Li C.-T., Chiao R., Multiresolution genetic clustering algorithm for texture segmentation, Image and Vision Computing, 21, 11, pp. 955-966, (2003); Richardson I.E., H.264 and MPEG-4 Video Compression: Video Coding for Next-generation Multimedia, (2003); Nejatpour R., Sadabad A.A., Automated weld defects detection using image processing and cad methods, ASME International Mechanical Engineering Congress and Exposition, (2008); Parvati K., Prakasa Rao B.S., Mariya Das M., Image segmentation using gray-scale morphology and marker-controlled watershed transformation, Discrete Dynamics in Nature and Society, (2008); Singh K., Singh A., A study of image segmentation algorithms for different types of images, IJCSI International Journal of Computer Science, 7, (2010); Bala A., An improved watershed image segmentation technique using matlab, International Journal of Scientific & Engineering Research, 3, (2012); Shen J., Application of Image Segmentation In Inspection Of Welding-Practical research in MATLAB, (2012); Dass R., Devi P.S., Image segmentation techniques, IJECT, 3, 1, (2012); Chaudhary A., Gulati T., Segmenting digital images using edge detection, International Journal of Application of Innovation in Engineering & Management, 2, 5, (2013); WELDER'S Visual Inspection, pp. 32-33, (2013); Narsimhachary D., Effect of Laser Welding Parameters on 6061 Aluminium Alloy, (2014); Nand G.K., Neogi N.N., Defect detection of steel surface using entropy segmentation, Annual IEEE India Conference, (2014); Alam M.A., Ali M.M.N., Syed M.A.A., Sorif N., Rahaman M.A., An Algorithm to Detect and Identify Defects of Industrial Pipes Using Image Processing, (2014); Cogranne R., Statistical detection of defects in radiographic images using an adaptive parametric model, Signal Processing, 96, 8, pp. 173-189, (2014); Madani S., Azizi M., Detection of Weld Defects in Radiography Films Using Image Processing, 9, (2015); Che Jamil M.S., Imam Fauzi E.R., Juinn C.S., Sheikh M.A., Laser bending of pre-stressed thin-walled nickel microtubes, Journal of Optics & Laser Technology, 73, pp. 105-117, (2015)","","","Universiti Teknikal Malaysia Melaka","","","","","","21801843","","","","English","J. Telecommun. Electron. Comput. Eng.","Article","Final","","Scopus","2-s2.0-84999143636"
"Chin D.J.Y.; Mohamed A.S.A.; Shariff K.A.; Wahab M.N.A.; Ishikawa K.","Chin, Daniel Jie Yuan (57355532800); Mohamed, Ahmad Sufril Azlan (57190968285); Shariff, Khairul Anuar (57190617705); Wahab, Mohd Nadhir Ab (36471236100); Ishikawa, Kunio (7404554763)","57355532800; 57190968285; 57190617705; 36471236100; 7404554763","Effects of different parameter settings for 3d data smoothing and mesh simplification on near real-time 3d reconstruction of high resolution bioceramic bone void filling medical images","2021","Sensors","21","23","7955","","","","2","10.3390/s21237955","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120047479&doi=10.3390%2fs21237955&partnerID=40&md5=7b562c5d6537f7c0019d75d559914951","School of Computer Sciences, Universiti Sains Malaysia, Gelugor, Penang, 11800, Malaysia; School of Materials and Mineral Resources Engineering, Universiti Sains Malaysia, Engineering Campus, Nibong Tebal, Penang, 14300, Malaysia; Dental Materials Science and Technology Division, Faculty of Dental Medicine, Airlangga University, JI. Prof. Dr. Moestopo No. 47, East Java, Surabaya, 60132, Indonesia; Department of Biomaterials, Faculty of Dental Science, Kyushu University, 3-1-1 Maidashi, Higashi-ku, Fukuoka, 812-8582, Japan","Chin D.J.Y., School of Computer Sciences, Universiti Sains Malaysia, Gelugor, Penang, 11800, Malaysia; Mohamed A.S.A., School of Computer Sciences, Universiti Sains Malaysia, Gelugor, Penang, 11800, Malaysia; Shariff K.A., School of Materials and Mineral Resources Engineering, Universiti Sains Malaysia, Engineering Campus, Nibong Tebal, Penang, 14300, Malaysia, Dental Materials Science and Technology Division, Faculty of Dental Medicine, Airlangga University, JI. Prof. Dr. Moestopo No. 47, East Java, Surabaya, 60132, Indonesia; Wahab M.N.A., School of Computer Sciences, Universiti Sains Malaysia, Gelugor, Penang, 11800, Malaysia; Ishikawa K., Department of Biomaterials, Faculty of Dental Science, Kyushu University, 3-1-1 Maidashi, Higashi-ku, Fukuoka, 812-8582, Japan","Three-dimensional reconstruction plays a vital role in assisting doctors and surgeons in diagnosing the healing progress of bone defects. Common three-dimensional reconstruction methods include surface and volume rendering. As the focus is on the shape of the bone, this study omits the volume rendering methods. Many improvements have been made to surface rendering methods like Marching Cubes and Marching Tetrahedra, but not many on working towards real-time or near real-time surface rendering for large medical images and studying the effects of different parameter settings for the improvements. Hence, this study attempts near real-time surface rendering for large medical images. Different parameter values are experimented on to study their effect on reconstruction accuracy, reconstruction and rendering time, and the number of vertices and faces. The proposed improvement involving three-dimensional data smoothing with convolution kernel Gaussian size 5 and mesh simplification reduction factor of 0.1 is the best parameter value combination for achieving a good balance between high reconstruction accuracy, low total execution time, and a low number of vertices and faces. It has successfully increased reconstruction accuracy by 0.0235%, decreased the total execution time by 69.81%, and decreased the number of vertices and faces by 86.57% and 86.61%, respectively. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","3D data smoothing; 3D reconstruction; High resolution micro-CT images; Mesh simplification","Algorithms; Imaging, Three-Dimensional; Normal Distribution; Prostheses and Implants; Surgical Mesh; Diagnosis; Graphic methods; Image enhancement; Image reconstruction; Medical imaging; Mesh generation; Volume rendering; 3D data; 3d data smoothing; 3D reconstruction; CT Image; Data smoothing; High resolution; High resolution micro-CT image; Mesh simplifications; Micro CT; Near-real time; algorithm; normal distribution; prostheses and orthoses; surgical mesh; three-dimensional imaging; Computerized tomography","","","","","Ministry of Higher Education, Malaysia, MOHE, (FRGS/1/2020/STG07/USM/02/12)","Funding: This research was funded by the Ministry of Higher Education Malaysia for Fundamental Research Grant Scheme with Project Code: FRGS/1/2020/STG07/USM/02/12.","Alasal S.A., Alsmirat M., Al-Mnayyis A., Baker Q.B., Al-Ayyoub M., Improving radiologists’ and orthopedists’ QoE in diagnosing lumbar disk herniation using 3D modeling, Int. J. Electr. Comput. Eng, 11, pp. 4336-4344, (2021); Al-Mnayyis A., Alasal S.A., Alsmirat M., Baker Q.B., AlZu'bi S., Lumbar disk 3D modeling from limited number of MRI axial slices, Int. J. Electr. Comput. Eng, 10, pp. 4101-4108, (2020); Stein D., Assaf Y., Dar G., Cohen H., Slon V., Kedar E., Medlej B., Abbas J., Hay O., Barazany D., Et al., 3D virtual reconstruction and quantitative assessment of the human intervertebral disc’s annulus fibrosus: A DTI tractography study, Sci. Rep, 11, (2021); Bao L., Rong S., Shi Z., Wang J., Zhang Y., Measurement of femoral posterior condylar offset and posterior tibial slope in normal knees based on 3D reconstruction, BMC Musculoskelet. Disord, 22, (2021); Tuecking L.-R., Ettinger M., Nebel D., Welke B., Schwarze M., Windhagen H., Savov P., 3D-surface scan based validated new measurement technique of femoral joint line reconstruction in total knee arthroplasty, J. Exp. Orthop, 8, (2021); Wu W., Wu Y., Shen G., Zhang G., Preoperative virtual simulation for synchronous multiple primary lung cancers using three-dimensional computed tomography lung reconstruction: A case report, J. Cardiothorac. Surg, 16, (2021); Bosc R., Tortolano L., Hersant B., Oudjhani M., Leplay C., Woerther P.L., Aguilar P., Leguen R., Meningaud J.-P., Bacteriological and mechanical impact of the Sterrad sterilization method on personalized 3D printed guides for mandibular reconstruction, Sci. Rep, 11, (2021); Wang S., Leng H., Tian Y., Xu N., Liu Z., A novel 3D-printed locking cage for anterior atlantoaxial fixation and fusion: Case report and in vitro biomechanical evaluation, BMC Musculoskelet. Disord, 22, (2021); van Ooijen P.M.A., van Geuns R.J.M., Rensing B.J.W.M., Bongaerts A.H.H., de Feyter P.J., Oudkerk M., Noninvasive coronary imaging using electron beam CT: Surface rendering versus volume rendering, AJR Am. J. Roentgenol, 180, pp. 223-226, (2003); Udupa J.K., Hung H.-M., Chuang K.-S., Surface and volume rendering in three-dimensional imaging: A comparison, J. Digit. Imaging, 4, (1991); Lorensen W.E., Cline H.E., Marching cubes: A high resolution 3D surface construction algorithm, Comput. Graph, 21, pp. 163-169, (1987); Kim S., Sohn D., Im S., Construction of polyhedral finite element meshes based upon marching cube algorithm, Adv. Eng. Softw, 128, pp. 98-112, (2019); Wang J., Huang Z., Yang X., Jia W., Zhou T., Three-dimensional Reconstruction of Jaw and Dentition CBCT Images Based on Improved Marching Cubes Algorithm, Procedia CIRP, 89, pp. 239-244, (2020); Garland M., Heckbert P.S., Surface simplification using quadric error metrics, Proceedings of the 24th Annual Conference on Computer Graphics and Interactive Techniques (SIGGRAPH’97), pp. 209-216, (1997); Chernyaev E.V., Marching Cubes 33: Construction of Topologically Correct Isosurfaces, Proceedings of the GRAPHICON’95, (1995); Custodio L., Pesco S., Silva C., An extended triangulation to the Marching Cubes 33 algorithm, J. Braz. Comput. Soc, 25, (2019); Wi W., Park S.M., Shin B.S., Computed Tomography-Based Preoperative Simulation System for Pedicle Screw Fixation in Spinal Surgery, J. Korean Med. Sci, 35, (2020); Masala G.L., Golosio B., Oliva P., An improved Marching Cube algorithm for 3D data segmentation, Comput. Phys. Commun, 184, pp. 777-782, (2013); Wang M., Luo H., Cui Q., Three-Dimensional Reconstruction Based On Improved Marching Cubes Algorithm, J. Mech. Med. Biol, 20, (2020); Doi A., Koide A., An Efficient Method of Triangulating Equi-Valued Surfaces by Using Tetrahedral Cells, IEICE Trans. Inf. Syst, E74-D, pp. 214-224, (1991); Lu T., Chen F., Quantitative analysis of molecular surface based on improved Marching Tetrahedra algorithm, J. Mol. Graph, 38, pp. 314-323, (2012); Bagley B., Sastry S.P., Whitaker R.T., A Marching-tetrahedra Algorithm for Feature-preserving Meshing of Piecewise-smooth Implicit Surfaces, Procedia Eng, 163, pp. 162-174, (2016); Guo D., Li C., Wu L., Yang J., Improved marching tetrahedra algorithm based on hierarchical signed distance field and multi-scale depth map fusion for 3D reconstruction, J. Vis. Commun. Image Represent, 48, pp. 491-501, (2017); Ren P., Wang H., Zhou G., Li J., Cai Q., Yu J., Yuan Y., Solid rocket motor propellant grain burnback simulation based on fast minimum distance function calculation and improved marching tetrahedron method, Chin. J. Aeronaut, 34, pp. 208-224, (2021); Marching Cubes–File Exchange–MATLAB Central; stlwrite–Write ASCII or Binary STL Files–File Exchange–MATLAB Central; Polygonising a Scalar Field (Marching Cubes); Wang Z., Simoncelli E.P., Bovik A.C., Multiscale structural similarity for image quality assessment, Proceedings of the Thrity-Seventh Asilomar Conference on Signals, Systems & Computers, pp. 1398-1402, (2003); Smooth 3-D Data—MATLAB Smooth3; Reduce Number of Patch Faces—MATLAB Reducepatch; Array Stored on GPU—MATLAB; Execute For-Loop Iterations in Parallel on Workers—MATLAB Parfor; Vectorization–MATLAB & Simulink","A.S.A. Mohamed; School of Computer Sciences, Universiti Sains Malaysia, Gelugor, Penang, 11800, Malaysia; email: sufril@usm.my","","MDPI","","","","","","14248220","","","34883959","English","Sensors","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85120047479"
"Shanmugasundaram K.; Mohamed A.S.A.; Ruhaiyem N.I.R.","Shanmugasundaram, K. (57193491154); Mohamed, A.S.A. (57190968285); Ruhaiyem, N.I.R. (57190964192)","57193491154; 57190968285; 57190964192","Hybrid improved bacterial swarm (HIBS) optimization algorithm","2017","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","10645 LNCS","","","71","78","7","2","10.1007/978-3-319-70010-6_7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85035117123&doi=10.1007%2f978-3-319-70010-6_7&partnerID=40&md5=37c714551e6804df5c55ed62d488fd8d","School of Computer Sciences, Universiti Sains Malaysia (USM), Gelugor, 11800, Penang, Malaysia","Shanmugasundaram K., School of Computer Sciences, Universiti Sains Malaysia (USM), Gelugor, 11800, Penang, Malaysia; Mohamed A.S.A., School of Computer Sciences, Universiti Sains Malaysia (USM), Gelugor, 11800, Penang, Malaysia; Ruhaiyem N.I.R., School of Computer Sciences, Universiti Sains Malaysia (USM), Gelugor, 11800, Penang, Malaysia","This paper proposed a hybrid improved bacterial swarm optimization (HIBS) algorithm by combining bacterial foraging optimization algorithm (BFO) with particle swarm optimization (PSO) to improve the performance of the classical BFO algorithm. Adaptive step size is introduced instead of fixed step size by random walk of the Fire Fly Algorithm (FFA) in the tumble move of the bacterium at the chemo-taxis stage of BFO. So that, the slow convergence of the BFO algorithm is mitigated. PSO algorithm is acted as mutation operator to attain the global best. So, the trapping out in the local optima by PSO is being avoided. BFO algorithm is used to attain the local best optimality. The new algorithm is tested on a set of benchmark functions. The proposed hybrid algorithm is compared with the original BFO and PSO algorithm. It has been proved that the proposed algorithm shows the significance than the classical BFO and PSO algorithms. © Springer International Publishing AG 2017.","Adaptive step size; Bacterial foraging optimization; Fire fly algorithm; Particle swarm optimization","Particle swarm optimization (PSO); Adaptive step size; Bacterial foraging optimization; Bacterial foraging optimization algorithms; Bacterial swarm optimizations; Benchmark functions; Mutation operators; Optimization algorithms; Slow convergences; Optimization","","","","","Universiti Sains Malaysia, (304/PKOMP/6313280)","Acknowledgements. The author wish to thank Universiti Sains Malaysia for the support it has extended in the completion of the present research through Short Term University Grant No. 304/PKOMP/6313280.","Alostaz A., Alhanjouri M., A new adaptive BFO based on PSO for learning neural network. I-Manager’s, J. Comput. Sci, 1, (2013); Bakwad K.M., Patnaik S.S., Et al., Hybrid bacterial foraging with parameter free PSO, IEEE World Congress on Nature and Biologically Inspired Computing, (2009); Kevin M., Biomimicry of bacterial foraging for distributed optimization and control, IEEE Control Syst. Mag., (2002); Biswas A., Das S., Abraham A., Synergy of PSO and bacterial foraging optimization: A comparative study on numerical benchmarks, Innovations in Hybrid Intelligent Systems. ASC, 44, pp. 255-263, (2007); Jarraya Y., Bouaziz S., Alimi A.M., Abraham A., A hybrid computational chemotaxis in bacterial foraging optimization algorithm for global numerical optimization, IEEE International Conference on Cybernetics, pp. 213-218, (2003); Kora P., Kalva S.R., Hybrid bacterial foraging and particle swarm optimization for detecting Bundle Branch Block, Springerplus, 4, 1, (2015); Kumar S., Sing S.K., Hybrid BFO and PSO Swarm Intelligence Approach for Biometric Feature Optimization, Nature-Inspired Computing Concepts, Methodologies, Tools, and Applications. IGI Global, Hershey, (2017); Yan X., Zhu Y., Chen H., Zhang H., Improved bacterial foraging optimization with social cooperation and adaptive step size, ICIC 2012. LNCS, 7389, pp. 634-640, (2012); Daas M.S., Chikhi S., Batouche M., Bacterial foraging optimization with double role of reproduction and step adaptation, Proceedings of International Conference on Intelligent Information Processing, Security and Advanced Communication, 71, (2015); Hanmandlu M., Kumar A., Madasu V.K., Yarlagadda P., Fusion of hand based biometrics using particle swarm optimization, 5Th International Conference on Information Technology: New Generations, pp. 783-788, (2008); Cherifi, Et al., Multimodal score-level fusion using hybrid GA-PSO for multibiometric system, Informatica, 39, pp. 209-216, (2015); Datta T., Et al., Improved adaptive bacteria foraging algorithm in optimization of antenna array for faster convergence, Prog. Electromagn. Res., 1, pp. 143-157, (2008); Chen C.-H., Et al., Hybrid of bacterial foraging optimization and particle swarm optimization for evolutionary neural fuzzy classifier, Int. J. Fuzzy Syst, 16, pp. 422-433, (2014); Yang X.-S., Firefly algorithms for multimodal optimization, SAGA 2009. LNCS, 5792, pp. 169-178, (2009); Mao L., Et al., Particle swarm and bacterial foraging inspired hybrid artificial bee colony algorithm for numerical function optimization, Math. Probl. Eng., (2016); Kumar A., Et al., A new framework for adaptive multimodal biometrics management, IEEE Trans. Inf. Forensics Secur., 5, pp. 92-102, (2010); Kumar A., Et al., Adaptive management of multimodal biometrics fusion using ant colony optimization, Inf. Fusion, 32, pp. 49-63, (2016); Kennedy J., Kennedy J.F., Eberhart R.C., Shi Y., Swarm Intelligence, (2001); Hanmandlu M., Kumar A., Madasu V.K., Yarlagadda P., Fusion of hand based biometrics using particle swarm optimization, Fifth International Conference on Information Technology: New Generations, pp. 783-788, (2008); Kora P., Krishna K.S.R., Hybrid firefly and particle swarm optimization algorithm for the detection of Bundle Branch Block, Int. J. Cardiovasc. Acad., 2, pp. 44-48, (2016); Ruhaiyem N.I.R., Mohamed A.S.A., Belaton B., Optimized segmentation of cellular tomography through organelles’ morphology and image features, J. Telecommun. Electron. Comput. Eng. (JTEC), 8, 3, pp. 79-83, (2016); Thevar V.V., Ruhaiyem N.I.R., Concept, theory and application: Hybrid watershed classic and active contour for enhanced image segmentation, Visual Informatics International Seminar, (2016); Ruhaiyem N.I.R., Semi-automated cellular tomogram segmentation workflow (CTSW): Towards an automatic target-scoring system, Proceedings of International Conference on Computer Graphics, Multimedia and Image Processing (CGMIP 2014), pp. 38-48, (2014); Ruhaiyem N.I.R., Boundary-based versus region-based approaches for cellular tomography segmentation, Proceedings of 1St International Engineering Conference (IEC 2014), pp. 260-267, (2014); Ruhaiyem N.I.R., Multiple, Object-Oriented Segmentation Methods of Mammalian Cell Tomograms, (2014)","A.S.A. Mohamed; School of Computer Sciences, Universiti Sains Malaysia (USM), Gelugor, 11800, Malaysia; email: sufril@usm.my","Shih T.K.; Velastin S.; Robinson P.; Smeaton A.F.; Terutoshi T.; Badioze Zaman H.; Jaafar A.; Mohamad Ali N.","Springer Verlag","","5th International Visual Informatics Conference, IVIC 2017","28 November 2017 through 30 November 2017","Bangi","205759","03029743","978-331970009-0","","","English","Lect. Notes Comput. Sci.","Conference paper","Final","","Scopus","2-s2.0-85035117123"
"Adamu H.; Lutfi S.L.; Malim N.H.A.H.; Hassan R.; Di Vaio A.; Mohamed A.S.A.","Adamu, Hassan (57222538575); Lutfi, Syaheerah Lebai (27567802400); Malim, Nurul Hashimah Ahamed Hassain (35090139100); Hassan, Rohail (56575985100); Di Vaio, Assunta (55922628500); Mohamed, Ahmad Sufril Azlan (57190968285)","57222538575; 27567802400; 35090139100; 56575985100; 55922628500; 57190968285","Framing twitter public sentiment on Nigerian government COVID-19 palliatives distribution using machine learning","2021","Sustainability (Switzerland)","13","6","3497","","","","25","10.3390/su13063497","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103101474&doi=10.3390%2fsu13063497&partnerID=40&md5=c68d161ce91902586e2c7e3bdb8b5a07","School of Computer Sciences, Universiti Sains Malaysia, Penang, 11800, Malaysia; Othman Yeop Abdullah Graduate School of Business (OYAGSB), Universiti Utara Malaysia (UUM), Kuala Lumpur, 50300, Malaysia; Department of Law, University of Naples “Parthenope”, Naples, 80132, Italy","Adamu H., School of Computer Sciences, Universiti Sains Malaysia, Penang, 11800, Malaysia; Lutfi S.L., School of Computer Sciences, Universiti Sains Malaysia, Penang, 11800, Malaysia; Malim N.H.A.H., School of Computer Sciences, Universiti Sains Malaysia, Penang, 11800, Malaysia; Hassan R., Othman Yeop Abdullah Graduate School of Business (OYAGSB), Universiti Utara Malaysia (UUM), Kuala Lumpur, 50300, Malaysia; Di Vaio A., Department of Law, University of Naples “Parthenope”, Naples, 80132, Italy; Mohamed A.S.A., School of Computer Sciences, Universiti Sains Malaysia, Penang, 11800, Malaysia","Sustainable development plays a vital role in information and communication technology. In times of pandemics such as COVID-19, vulnerable people need help to survive. This help includes the distribution of relief packages and materials by the government with the primary objective of lessening the economic and psychological effects on the citizens affected by disasters such as the COVID-19 pandemic. However, there has not been an efficient way to monitor public funds’ accountability and transparency, especially in developing countries such as Nigeria. The understanding of public emotions by the government on distributed palliatives is important as it would indicate the reach and impact of the distribution exercise. Although several studies on English emotion classification have been conducted, these studies are not portable to a wider inclusive Nigerian case. This is because Informal Nigerian English (Pidgin), which Nigerians widely speak, has quite a different vocabulary from Standard English, thus limiting the applicability of the emotion classification of Standard English machine learning models. An Informal Nigerian English (Pidgin English) emotions dataset is constructed, pre-processed, and annotated. The dataset is then used to classify five emotion classes (anger, sadness, joy, fear, and disgust) on the COVID-19 palliatives and relief aid distribution in Nigeria using standard machine learning (ML) algorithms. Six ML algorithms are used in this study, and a comparative analysis of their performance is conducted. The algorithms are Multinomial Naïve Bayes (MNB), Support Vector Machine (SVM), Random Forest (RF), Logistics Regression (LR), K-Nearest Neighbor (KNN), and Decision Tree (DT). The conducted experiments reveal that Support Vector Machine outperforms the remaining classifiers with the highest accuracy of 88%. The “disgust” emotion class surpassed other emotion classes, i.e., sadness, joy, fear, and anger, with the highest number of counts from the classification conducted on the constructed dataset. Additionally, the conducted correlation analysis shows a significant relationship between the emotion classes of “Joy” and “Fear”, which implies that the public is excited about the palliatives’ distribution but afraid of inequality and transparency in the distribution process due to reasons such as corruption. Conclusively, the results from this experiment clearly show that the public emotions on COVID-19 support and relief aid packages’ distribution in Nigeria were not satisfactory, considering that the negative emotions from the public outnumbered the public happiness. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","COVID-19 palliatives; Machine learning; Nigerian Pidgin English Twitter dataset; Relief aid; Sentiment analysis; Social media","Nigeria; Varanidae; accountability; algorithm; COVID-19; developing world; government; machine learning; spatiotemporal analysis","","","","","RCMO; UNIVERSITI, (1001/PKOMP/8014001, 304/PKOMP/6315137)","Funding: This research was funded jointly by RCMO, UNIVERSITI SAINS MALAYSIA under the, grants number 1001/PKOMP/8014001 and 304/PKOMP/6315137 and Research University Grant (1001/PKOMP/8014001).","Siddiqui Abdul Hameed, A Sustainable Society: Its Meaning and Objectives, Int. J. Res. Sci. Innov, (2018); Impact of COVID-19 on the Sustainable Development Goals: Pursuing the Sustainable Development Goals (SDGs) in a World Reshaped by COVID-19, (2020); Szabo S., Nhau B., Tsusaka T.W., Kadigi R.M.J., Payne T., Kangile J.R., Park K.S., Couto M., Runsten L., Burgess N.D., Et al., Towards a Successful Post COVID-19 Transition of Monitoring, Evaluation, and Learning in Complex Sustainability Science Research-to-Policy Projects, Sustainability, 13, (2021); Rutkowska A., Kacperak K., Rutkowski S., Cacciante L., Kiper P., Szczegielniak J., The Impact of Isolation Due to COVID-19 on Physical Activity Levels in Adult Students, Sustainability, 13, (2021); Vaz E., COVID-19 in Toronto: A Spatial Exploratory Analysis, Sustainability, 13, (2021); COVID-19 Dashboard by the Center for Systems Science and Engineering (CSSE) at Johns Hopkins University (JHU); NCDC Coronavirus COVID-19 Microsite; Kemp S., Digital 2020: Nigeria. DataReportal-Global Digital Insights; Qiu M., Sha J., Utomo S., Listening to Forests: Comparing the Perceived Restorative Characteristics of Natural Soundscapes before and after the COVID-19 Pandemic, Sustainability, 13, (2021); Tetrevova L., Vavra J., Munzarova S., Communication of Socially-Responsible Activities by Higher Education Institutions, Sustainability, 13, (2021); Marinello S., Lolli F., Gamberini R., The Impact of the COVID-19 Emergency on Local Vehicular Traffic and Its Consequences for the Environment: The Case of the City of Reggio Emilia (Italy), Sustainability, 13, (2021); Ricciardelli A., Governance, Local Communities, and Citizens Participation, Global Encyclopedia of Public Administration, Public Policy, and Governance, (2018); Loshin D., Text Data Analytics: In Service of Smart Government; Barns S., Smart cities and urban data platforms: Designing interfaces for smart governance, City Cult. Soc, 12, pp. 5-12, (2018); Kumar A., Sharma A., Systematic literature review on opinion mining of big data for government intelligence, Webology, 14, pp. 6-47, (2017); Lennerholt C., van Laere J., Soderstrom E., Implementation Challenges of Self Service Business Intelligence: A Literature Review, Proceedings of the 51st Hawaii International Conference on System Sciences, 9, pp. 5055-5063, (2018); Khan K., Baharudin B., Khan A., Ullah A., Mining opinion components from unstructured reviews: A review, J. King Saud Univ. Comput. Inf. Sci, 26, pp. 258-275, (2014); Joshi S., Deshpande D., Twitter Sentiment Analysis System, Int. J. Comput. Appl, 180, pp. 35-39, (2018); Varrella S., Nigeria: Leading Social Media Platforms; Pidgin-West African Lingua Franca. BBC News, (2016); Desai R.D., Sentiment Analysis of Twitter Data, Proceedings of the 2nd International Conference on Intelligent Computing and Control Systems, pp. 114-117, (2018); Huang C.H., Hsieh S.H., Predicting BIM labor cost with random forest and simple linear regression, Autom. Constr, 118, (2020); Reddy D.M., Twitter Sentiment Analysis using Distributed Word and Sentence Representation, (2019); Meng L., Dong Z.S., Christenson L., Fulton L., Mining Public Opinion on Twitter about Natural Disaster Response Using Machine Learning Techniques, (2017); Squicciarini A., Tapia A., Stehle S., Sentiment analysis during Hurricane Sandy in emergency response, Int. J. Disaster Risk Reduct, 21, pp. 213-222, (2017); Rathee N., Joshi N., Kaur J., Sentiment Analysis Using Machine Learning Techniques on Python, Proceedings of the 2018 Second International Conference on Intelligent Computing and Control Systems (ICICCS), (2018); Kanish S., Henil P., Devanshi S., Manan S., A Comparative Analysis of Logistic Regression, Random Forest and KNN Models for the Text Classification, Augment. Hum. Res, 5, (2020); Oyewusi W.F., Adekanmbi O., Akinsande O., Semantic Enrichment of Nigerian Pidgin English for Contextual Sentiment Classification; Suh A., Li M., Digital Tracing during the COVID-19 Pandemic: User Appraisal, Emotion, and Continuance Intention, Sustainability, 13, (2021); Manguri K.N., Ramadhan R., Mohammed A.P., Twitter Sentiment Analysis on Worldwide COVID-19 Outbreaks, Kurd. J. Appl. Res, pp. 54-65, (2020); Bento A.I., Thuy N., Coady W., Felipe L.R., Yong Y.A., Kosali S., Evidence from internet search data shows information-seeking responses to news of local COVID-19 cases, Natl. Acad. Sci, 117, pp. 11220-11222, (2020); Hasan A., Moin S., Karim A., Shamshirband S., Machine Learning-Based Sentiment Analysis for Twitter Accounts, Math. Comput. Appl, 23, (2018); Ozturk N., Ayvaz S., Sentiment analysis on Twitter: A text mining approach to the Syrian refugee crisis, Telemat. Inform, 35, pp. 136-147, (2018); Yin H., Cui B., Lu H., Huang Y., Yao J., A unified model for stable and temporal topic detection from social media data, Proceedings of the International Conference Data Engineering, pp. 661-672, (2013); Sidarenka U., Sentiment Analysis of German Twitter, (2019); Sonawane S.S., Sentiment Analysis of Twitter Data: A Survey of Techniques, Int. J. Comput. Appl, 139, pp. 5-15, (2016); Nakov P., Ritter A., Rosenthal S., Sebastiani F., Stoyanov V., SemEval-2016 task 4: Sentiment analysis in twitter, Proceedings of the SemEval 2016-10th International Workshop on Semantic Evaluation, pp. 1-18, (2016); Chakriswaran P., Vincent D.R., Srinivasan K., Sharma V., Chang C.Y., Reina D.G., Emotion AI-driven sentiment analysis: A survey, future research directions, and open issues, Appl. Sci, 9, (2019); Balogun T.A., In defense of Nigerian pidgin, J. Lang. Cult, 4, pp. 90-98, (2013); Osoba J.B., Analysis of Discourse in Nigerian Pidgin, J. Univers. Lang, 16, pp. 131-159, (2015); Idegbekwe D., Anthropomorphisms and the Nigerian Pidgin Proverbs: A Linguistic Conceptual Metaphorical Analysis, EBSU J. Soc. Sci. Rev, 10, pp. 71-76, (2020); Bigi B., Caron B., Abiola O., Developing Resources for Automated Speech Processing of the African Language Naija (Nigerian Pidgin), Proceedings of the 8th Language & Technology Conference: Human Language Technologies as a Challenge for Computer Science and Linguistics, pp. 441-445, (2017); Sung Y.A., Kim K.W., Kwon H.J., Big Data Analysis of Korean Travelers’ Behavior in the Post-COVID-19 Era, Sustainability, 13, (2021); Zhao F., Zhu N., Hamalainen J., Protection of Children in Difficulty in China during the COVID-19 Pandemic, Sustainability, 13, (2021); Radulescu C.V., Ladaru G.R., Burlacu S., Constantin F., Ioanas C., Petre I.L., Impact of the COVID-19 Pandemic on the Romanian Labor Market, Sustainability, 13, (2021); Awwalu J., Umar N.A., Ibrahim M.S., Nonyelum O.F., A multinomial Naïve Bayes decision support system for COVID-19 detection, FUDMA J. Sci, 4, pp. 704-711, (2020); Kaklamanis M.M., Filippakis M., Touloupos M., Christodoulou K., An experimental comparison of machine learning classification algorithms for breast cancer diagnosis, Proceedings of the 16th European, Mediterranean, and Middle Eastern Conference, EMCIS 2019, 381, pp. 18-30, (2019); Jianqiang Z., Xiaolin G., Comparison research on text pre-processing methods on twitter sentiment analysis, IEEE Access, 5, pp. 2870-2879, (2017); Sahoo D., Liu C., Hoi S.C.H., Malicious URL Detection using Machine Learning: A Survey, (2017); Rohini J., Ambesh B., Animesh S., Abhishek K.S., A Survey on Various Approaches for Sentiment Analysis and Performance Optimization, Int. J. Eng. Res. Technol, 6, pp. 716-720, (2017); Khanvilkar G., Deepali V.P., Sentiment Analysis for Product Recommendation Using Random Forest, Int. J. Eng. Technol, 7, (2018); Joshi A.M., Prabhune S., Random forest: A hybrid implementation for sarcasm detection in public opinion mining, Int. J. Innov. Technol. Explor. Eng, 8, pp. 5022-5025, (2019); Pathan M., Patel N., Yagnik H., Shah M., Artificial cognition for applications in smart agriculture: A comprehensive review, Artif. Intell. Agric, 4, pp. 81-95, (2020); Varathan K.D., Anastasia G., Crestani F., Comparative Opinion Mining: A Review, J. Assoc. Inf. Sci. Technol, 64, pp. 811-829, (2017); Samuel J., Ali G.G., Rahman M.M., Esawi E., Samuel Y., COVID-19 public sentiment insights and machine learning for tweets classification, Information, 11, (2020); Karami A., Shah V., Vaezi R., Bansal A., Twitter speaks: A case of national disaster situational awareness, J. Inf. Sci, 46, pp. 313-324, (2020); Delizo J.P.D., Abisado M.B., De Los Trinos M.I.P., Philippine twitter sentiments during COVID-19 Pandemic using Multinomial Naïve Bayes, Int. J. Adv. Trends Comput. Sci. Eng, 64, pp. 408-412, (2020); Karisani N., Karisani P., Mining Coronavirus (COVID-19) Posts in Social Media, (2020); Emil E., Jozef B., Analysis of Online Consumer Behavior-Design of CRISP-DM Process Model, Agris On-Line Pap. Econ. Inform, 9, pp. 13-22, (2020); Population, total-Nigeria | Data; Kabir A.I., Karim R., Newaz S., Hossain M.I., The power of social media analytics: Text analytics based on sentiment analysis and word clouds on R, J. Inform. Econ, 22, pp. 25-38, (2018); Danisman T., Alpkocak A., Feeler: Emotion classification of text using vector space model, Conv. Commun. Interact. Soc. Intell, 1, (2008); Thomas B., Vinod P., Dhanya K.A., Multiclass emotion extraction from sentences, Int. J. Sci. Eng. Res, 5, pp. 12-15, (2014)","S.L. Lutfi; School of Computer Sciences, Universiti Sains Malaysia, Penang, 11800, Malaysia; email: syaheerah@usm.my","","MDPI AG","","","","","","20711050","","","","English","Sustainability","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85103101474"
"Al Qudah M.M.M.; Mohamed A.S.A.; Lutfi S.L.","Al Qudah, Mustafa M. M. (57222567501); Mohamed, Ahmad S. A. (57190968285); Lutfi, Syaheerah L. (27567802400)","57222567501; 57190968285; 27567802400","Affective state recognition using thermal-based imaging: A survey","2021","Computer Systems Science and Engineering","37","1","","47","62","15","8","10.32604/CSSE.2021.015222","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103239312&doi=10.32604%2fCSSE.2021.015222&partnerID=40&md5=bef868e22b0f14d169bcdc8c900d2900","School of Computer Sciences, Universiti Sains Malaysia, Penang, 11800, Malaysia","Al Qudah M.M.M., School of Computer Sciences, Universiti Sains Malaysia, Penang, 11800, Malaysia; Mohamed A.S.A., School of Computer Sciences, Universiti Sains Malaysia, Penang, 11800, Malaysia; Lutfi S.L., School of Computer Sciences, Universiti Sains Malaysia, Penang, 11800, Malaysia","The thermal-based imaging technique has recently attracted the attention of researchers who are interested in the recognition of human affects due to its ability to measure the facial transient temperature, which is correlated with human affects and robustness against illumination changes. Therefore, studies have increasingly used the thermal imaging as a potential and supplemental solution to overcome the challenges of visual (RGB) imaging, such as the variation of light conditions and revealing original human affect. Moreover, the thermal-based imaging has shown promising results in the detection of psychophysiological signals, such as pulse rate and respiration rate in a contactless and noninvasive way. This paper presents a brief review on human affects and focuses on the advantages and challenges of the thermal imaging technique. In addition, this paper discusses the stages of thermal-based human affective state recognition, such as dataset type, preprocessing stage, region of interest (ROI), feature descriptors, and classification approaches with a brief performance analysis based on a number of works in the literature. This analysis could help beginners in the thermal imaging and affective recognition domain to explore numerous approaches used by researchers to construct an affective state system based on thermal imaging. © 2021 CRL Publishing. All rights reserved.","Affective state recognition; Feature extraction and classification; Spontaneous emotion; Thermal-based imaging","Classification (of information); Image segmentation; State estimation; Affective recognition; Classification approach; Feature descriptors; Illumination changes; Performance analysis; Psychophysiological signals; Region of interest; Transient temperature; Infrared imaging","","","","","Universiti Sains Malaysia","Funding Statement: This research was funded by the research university grant by Universiti Sains Malaysia [1001.PKOMP.8014001].","Greenblatt S., Toward a universal language of motion: Reflections on a seventeenth-century muscle man, LiNQ (Literature in North Queensland), 21, 2, pp. 56-62, (1994); Darwin C., Introduction to The First Edition, The Expression of Emotions in Man and Animals, (1872); Duchenne G. B., de Boulogne G. B. D., Cuthbertson R. A., Manstead A. S. R., Oatley K., A review of previous work on muscle action in facial expression, The Mechanism of Human Facial Expression, pp. 14-16, (1990); Ekman P., Friesen W. V., Constants across cultures in the face and emotion, Journal of Personality and Social Psychology, 17, 2, pp. 124-129, (1971); Russell J. A., A circumplex model of affect, Journal of Personality and Social Psychology, 39, 6, (1980); Chen C.H., Lee I.J., Lin L.Y., Augmented reality-based self-facial modeling to promote the emotional expression and social skills of adolescents with autism spectrum disorders, Research in Developmental Disabilities, 36, pp. 396-403, (2015); Fendri E., Boukhriss R. R., Hammami M., Fusion of thermal infrared and visible spectra for robust moving object detection, Pattern Analysis and Applications, 20, 4, pp. 907-926, (2017); Desideri L., Ottaviani C., Malavasi M., di Marzio R., Bonifacci P., Emotional processes in human-robot interaction during brief cognitive testing, Computers in Human Behavior, 90, 1, pp. 331-342, (2019); Lee M. S., Cho Y. R., Lee Y. K., Pae D. S., Lim M. T., Et al., PPG and EMG based emotion recognition using convolutional neural network, ICINCO 2019-Proc. of the 16th Int. Conf. on Informatics in Control, Automation and Robotics, pp. 595-600, (2019); Miller G. E., Cohen S., Ritchey A. K., Chronic psychological stress and the regulation of pro-inflammatory cytokines: A glucocorticoid-resistance model, Health Psychology, 21, 6, pp. 531-541, (2002); Vitetta L., Anton B., Cortizo F., Sali A., Mind-body medicine: Stress and its impact on overall health and longevity, Annals of the New York Academy of Sciences, 1057, 1, pp. 492-505, (2005); Gradus J. L., Posttraumatic stress disorder and death from suicide, Current Psychiatry Reports, 20, 11, (2018); Mowrer O. H., Introduction: Historical review and perespective, Learning Theory and Behavior, (1960); Frijda N. H., Emotional behavior, The Emotions, pp. 9-109, (1986); Ekman P., Universals and cultural differences in facial expressions of emotion, Nebraska Sym. on Motivation, 19, pp. 209-282, (1971); Scherer K. R., Schorr A., Johnstone T., Appraisal theory, Appraisal Processing in Emotion, (2001); Ortony A., Clore G., Appraisal theories: How cognition shapes affect into emotion, Handbook of Emotions, pp. 628-642, (2008); Fasel B., Luettin J., Automatic facial expression analysis: A survey, Pattern Recognition, 36, 1, pp. 259-275, (2003); Devito J. A., Silence and paralanguage as communication, ETC: A Review of General Semantics, 74, 3- 4, pp. 482-487, (2017); Knapp M. L., Hall J. A., Horgan T. G., Nonverbal communication: Basic perspectives, Nonverbal Communication in Human Interaction, pp. 3-19, (2013); Johar S., Emotion, affect and personality in speech: The bias of language and paralanguage, pp. 1-6, (2016); Khatri N. N., Shah Z. H., Patel S. A., Facial expression recognition: A survey, Int. Journal of Computer Science and Information Technologies (IJCSIT), 5, 1, pp. 149-152, (2014); Smith C., Scott H., A Componential approach to the meaning of facial expressions, The Psychology of Facial Expression (Studies in Emotion and Social Interaction), pp. 229-254, (1997); Kihlstrom J. F., Mulvaney S., Tobias B. A., Tobis I. P., The emotional unconscious, Cognition and Emotion, pp. 30-86, (2000); Friesen E., Ekman P., Facial action coding system: A technique for the measurement of facial movement, Consulting Psychologists Press, 3, (1978); Khalid B., Khan A. M., Akram M. U., Batool S., Person detection by fusion of visible and thermal images using convolutional neural network, Int. Conf. on Communication, Computing and Digital Systems (C-CODE), pp. 143-148, (2019); Shu L., Xie J., Yang M., Li Z., Li Z., Et al., A review of emotion recognition using physiological signals, Sensors, 18, 7, (2018); Kreibig S. D., Autonomic nervous system activity in emotion: A review, Biological Psychology, 84, 3, pp. 394-421, (2010); Samadiani N., Huang G., Cai B., Luo W., Chi C.H., Et al., A review on automatic facial expression recognition systems assisted by multimodal sensor data, Sensors (Basel), 19, 8, (2019); Jaimes A., Sebe N., Multimodal human-computer interaction: A survey, Computer Vision and Image Understanding, 108, 1-2, pp. 116-134, (2007); Mehta D., Siddiqui M. F. H., Javaid A. Y., Facial emotion recognition: A survey and real-world user experiences in mixed reality, Sensors, 18, 2, (2018); Gunes H., Schuller B., Pantic M., Cowie R., Emotion representation, analysis and synthesis in continuous space: A survey, International Conference on Automatic Face and Gesture Recognition, pp. 827-834, (2011); Busso C., Deng Z., Yildirim S., Bulut M., Lee C. M., Et al., Analysis of emotion recognition using facial expressions, speech and multimodal information, Proc. of the 6th Int. Conf. on Multimodal Interfaces (ICMI04), pp. 205-211, (2004); Corneanu C. A., Simon M. O., Cohn J. F., Guerrero S. E., Survey on RGB, 3D, thermal, and multimodal approaches for facial expression recognition: History, trends, and affect-related applications, IEEE Transactions on Pattern Analysis and Machine Intelligence, 38, 8, pp. 1548-1568, (2016); Shoumy N. J., Ang L. M., Seng K. P., Rahaman D. M., Zia T., Multimodal big data affective analytics: A comprehensive survey using text, audio, visual and physiological signals, Journal of Network and Computer Applications, 149, 1, (2020); Chunawale A., Bedekar D., Human emotion recognition using physiological signals: A survey, 2nd Int. Conf. on Communication and Information Processing (ICCIP-2020), (2020); Saganowski S., Dutkowiak A., Dziadek A., Dziezyc M., Komoszynska J., Et al., Emotion recognition using wearables: A systematic literature review-work-in-progress, IEEE Int. Conf. on Pervasive Computing and Communications Workshops (PerCom Workshops), pp. 1-6, (2020); Faust O., Hagiwara Y., Hong T. J., Lih O. S., Acharya U. R., Deep learning for healthcare applications based on physiological signals: A review, Computer Methods and Programs in Biomedicine, 161, 1, pp. 1-13, (2018); Bong S. Z., Murugappan M., Yaacob S., Methods and approaches on inferring human emotional stress changes through physiological signals: A review, Int. Journal of Medical Engineering and Informatics, 5, 2, pp. 152-162, (2013); Jerritta S., Murugappan M., Nagarajan R., Wan K., Physiological signals based human emotion recognition: A review, IEEE 7th Int. Colloquium on Signal Processing and its Applications, pp. 410-415, (2011); Ko B. C., A brief review of facial emotion recognition based on visual information, Sensors, 18, 2, (2018); Revina I. M., Emmanuel S., A survey on human face expression recognition techniques, Journal of King Saud University-Computer and Information Sciences, pp. 1-33, (2018); Filippini C., Perpetuini D., Cardone D., Chiarelli A. M., Merla A., Thermal infrared imaging-based affective computing and its application to facilitate human robot interaction: A review, Applied Sciences, 10, 8, (2020); Yang B., Li X., Hou Y., Meier A., Cheng X., Et al., Non-invasive (non-contact) measurements of human thermal physiology signals and thermal comfort/discomfort poses-a review, Energy and Buildings, 224, 1, (2020); Kumar C. N., Shivakumar G., A survey on human emotion analysis using thermal imaging and physiological variables, Int. Journal of Current Engineering and Scientific Research (IJCESR), 4, 4, pp. 122-126, (2017); Ponsi G., Panasiti M. S., Rizza G., Aglioti S. M., Thermal facial reactivity patterns predict social categorization bias triggered by unconscious and conscious emotional stimuli, Proc. of the Royal Society B: Biological Sciences, 284, 1861, (2017); Ioannou S., Gallese V., Merla A., Thermal infrared imaging in psychophysiology: Potentialities and limits, Psychophysiology, 51, 10, pp. 951-963, (2014); van Gastel M., Stuijk S., de Haan G., Robust respiration detection from remote photoplethysmography, Biomedical Optics Express, 7, 12, pp. 4941-4957, (2016); Cho Y., Automated mental stress recognition through mobile thermal imaging, Seventh Int. Conf. on Affective Computing and Intelligent Interaction (ACII), pp. 596-600, (2017); He X., Goubran R., Knoefel F., IR night vision video-based estimation of heart and respiration rates, IEEE Sensors Applications Symposium (SAS), pp. 1-5, (2017); Merla A., Di Donato L., Rossini P., Romani G., Emotion detection through functional infrared imaging: preliminary results, Biomedizinische Technick, 48, 2, pp. 284-286, (2004); Alkali A. H., Saatchi R., Elphick H., Burke D., Thermal image processing for real-time non-contact respiration rate monitoring, IET Circuits, Devices & Systems, 11, 2, pp. 142-148, (2017); Nakayama Y., Sun G., Abe S., Matsui T., Non-contact measurement of respiratory and heart rates using a CMOS camera-equipped infrared camera for prompt infection screening at airport quarantine stations, IEEE Int. Conf. on Computational Intelligence and Virtual Environments for Measurement Systems and Applications (CIVEMSA), pp. 1-4, (2015); Park S. B., Kim G., Baek H. J., Han J. H., Kim J. H., Remote pulse rate measurement from near-infrared videos, IEEE Signal Processing Letters, 25, 8, pp. 1271-1275, (2018); Yang M., Liu Q., Turner T., Wu Y., Vital sign estimation from passive thermal video, IEEE Conf. on Computer Vision and Pattern Recognition, pp. 1-8, (2008); Fei J., Pavlidis I., Thermistor at a distance: Unobtrusive measurement of breathing, IEEE Transactions on Biomedical Engineering, 57, 4, pp. 988-998, (2009); Murthy J. N., van Jaarsveld J., Fei J., Pavlidis I., Harrykissoon R. I., Et al., Thermal infrared imaging: A novel method to monitor airflow during polysomnography, Sleep, 32, 11, pp. 1521-1527, (2009); Basu A., Routray A., Shit S., Deb A. K., Human emotion recognition from facial thermal image based on fused statistical feature and multi-class SVM, Annual IEEE India Conf. (INDICON), pp. 1-5, (2015); Mohd M. N. H., Kashima M., Sato K., Watanabe M., Mental stress recognition based on non-invasive and non-contact measurement from stereo thermal and visible sensors, Int. Journal of Affective Engineering, 14, 1, pp. 9-17, (2015); Nguyen T., Tran K., Nguyen H., Towards thermal region of interest for human emotion estimation, 10th Int. Conf. on Knowledge and Systems Engineering (KSE), pp. 152-157, (2018); Wang S., Pan B., Chen H., Ji Q., Thermal augmented expression recognition, IEEE Transactions on Cybernetics, 48, 7, pp. 2203-2214, (2018); Yan X., Andrews T. J., Jenkins R., Young A.W., Cross-cultural differences and similarities underlying otherrace effects for facial identity and expression, Quarterly Journal of Experimental Psychology, 69, 7, pp. 1247-1254, (2018); Wang S., Liu Z., Wang Z., Wu G., Shen P., Et al., Analyses of a multimodal spontaneous facial expression database, IEEE Transactions on Affective Computing, 4, 1, pp. 34-46, (2013); Bhowmik M. K., Saha P., Singha A., Bhattacharjee D., Dutta P., Enhancement of robustness of face recognition system through reduced Gaussianity in Log-ICA, Expert Systems with Applications, 116, 1, pp. 96-107, (2019); Zhang X., Yin L., Cohn J. F., Canavan S., Reale M., Et al., Bp4d-spontaneous: A high-resolution spontaneous 3d dynamic facial expression database, Image and Vision Computing, 32, 10, pp. 692-706, (2014); Chu C.-H., Peng S.-M., Implementation of face recognition for screen unlocking on mobile device, Proc. of the 23rd ACM Int. Conf. on Multimedia, pp. 1027-1030, (2015); Jian B.-L., Chen C. L., Huang M. W., Yau H. T., Emotion-specific facial activation maps based on infrared thermal image sequences, IEEE Access, 7, pp. 48046-48052, (2019); Elanthendral V., Rekha R., Rameshkumar M., Thermal imaging for facial expression-fatigue detection, International Journal for Research in Applied Science & Engineering Technology (IJRASET), 2, XII, pp. 14-17, (2014); Nguyen H., Chen F., Kotani K., Le B., Fusion of visible images and thermal image sequences for automated facial emotion estimation, Journal of Mobile Multimedia, 10, 3-4, pp. 294-308, (2014); Kopaczka M., Kolk R., Merhof D., A fully annotated thermal face database and its application for thermal facial expression recognition, IEEE Int. Instrumentation and Measurement Technology Conf. (I2MTC), pp. 1-6, (2018); Merhof D., Acar K., Kopaczka M., Robust facial landmark detection and face tracking in thermal infrared images using active appearance models, Int. Conf. on Computer Vision Theory and Applications, pp. 150-158, (2016); Mostafa E., Farag A., Shalaby A., Ali A., Gault T., Et al., Long term facial parts tracking in thermal imaging for uncooperative emotion recognition, IEEE Sixth Int. Conf. on Biometrics: Theory, Applications and Systems (BTAS), pp. 1-6, (2013); Wang S., He S., Wu Y., He M., Ji Q., Fusion of visible and thermal images for facial expression recognition, Frontiers of Computer Science, 8, 2, pp. 232-242, (2014); Shi X., Wang S., Zhu Y., Expression recognition from visible images with the help of thermal images, Proc. of the 5th ACM on Int. Conf. on Multimedia Retrieval, pp. 563-566, (2015); Siddiqui M. F. H., Javaid A. Y., A multimodal facial emotion recognition framework through the fusion of speech with visible and infrared images, Multimodal Technologies and Interaction, 4, 3, (2020); Wang S., He S., Spontaneous facial expression recognition by fusing thermal infrared and visible images, Intelligent Autonomous Systems 12. Advances in Intelligent Systems and Computing, 194, pp. 263-272, (2013); Nayak S., Panda S. K., Uttarkabat S., A non-contact framework based on thermal and visual imaging for classification of affective states during HCI, 4th Int. Conf. on Trends in Electronics and Informatics (ICOEI)(48184), pp. 653-660, (2020); Kolli A., Fasih A., Al Machot F., Kyamakya K., Non-intrusive car driver's emotion recognition using thermal camera, International Workshop on Nonlinear Dynamics and Synchronization, INDS. Proc. of the Joint INDS'11 & ISTET'11, pp. 1-5, (2011); Latif M., Sidek S., Rusli N., Fatai S., Emotion detection from thermal facial imprint based on GLCM features, ARPN J. Eng. Appl. Sci, 11, 1, pp. 345-350, (2016); Kopaczka M., Kolk R., Merhof D., A fully annotated thermal face database and its application for thermal facial expression recognition, IEEE Int. Instrumentation and Measurement Technology Conf. (I2MTC), pp. 1-6, (2018); Cross C. B., Skipper J. A., Petkie D. T., Thermal imaging to detect physiological indicators of stress in humans, Proc. SPIE Thermosense: Thermal Infrared Applications XXXV, 8705, (2013); Carrapico R., Mourao A., Magalhaes J., Cavaco S., A comparison of thermal image descriptors for face analysis, 23rd European Signal Processing Conf. (EUSIPCO), pp. 829-833, (2015); Perez-Rosas V., Narvaez A., Burzo M., Mihalcea R., Thermal imaging for affect detection, PETRA '13: The 6th Int. Conf. on Pervasive Technologies Related to Assistive Environments, pp. 1-4, (2013); Liu P., Yin L., Spontaneous facial expression analysis based on temperature changes and head motions, 11th IEEE Int. Conf. and Workshops on Automatic Face and Gesture Recognition (FG), pp. 1-6, (2015); Wang S., He M., Gao Z., He S., Ji Q., Emotion recognition from thermal infrared images using deep Boltzmann machine, Frontiers of Computer Science, 8, 4, pp. 609-618, (2014); Boccanfuso L., Wang Q., Leite I., Li B., Torres C., Et al., A thermal emotion classifier for improved human-robot interaction, 25th IEEE Int. Sym. on Robot and Human Interactive Communication (RO-MAN), pp. 718-723, (2016); Latif M., Yusof M. H., Sidek S., Rusli N., Texture descriptors based affective states recognition-frontal face thermal image, IEEE EMBS Conf. on Biomedical Engineering and Sciences (IECBES), pp. 80-85, (2016); Nguyen H., Kotani K., Chen F., Le B., Estimation of human emotions using thermal facial information, Fifth Int. Conf. on Graphic and Image Processing (ICGIP2013) Proc, 9069, (2014); Goulart C., Valadao C., Delisle-Rodriguez D., Caldeira E., Bastos T., Et al., Emotion analysis in children through facial emissivity of infrared thermal imaging, PloS One, 14, 3, (2019); Khan M. M., Ward R. D., Ingleby M., Classifying pretended and evoked facial expressions of positive and negative affective states using infrared measurement of skin temperature, ACM Transactions on Applied Perception (TAP), 6, 1, pp. 1-22, (2009); Haamer R. E., Rusadze E., Lusi I., Ahmed T., Escalera S., Et al., Review on emotion recognition databases, Human-Robot Interaction-Theory and Application, (2017); Wang S., Liu Z., Lv S., Lv Y., Wu G., Et al., A natural visible and infrared facial expression database for expression recognition and emotion inference, IEEE Transactions on Multimedia, 12, 7, pp. 682-691, (2010); Nguyen H., Kotani K., Chen F., Le B., A thermal facial emotion database and its analysis, Pacific-Rim Sym. on Image and Video Technology, pp. 397-408, (2013); Ordun C., Raff E., Purushotham S., The use of AI for thermal emotion recognition: A review of problems and limitations in standard design and data, pp. 1-13, (2020); Zhu X., Ramanan D., Face detection, pose estimation, and landmark localization in the wild, IEEE Conf. on Computer Vision and Pattern Recognition, pp. 2879-2886, (2012); Trujillo L., Olague G., Hammoud R., Hernandez B., Automatic feature localization in thermal images for facial expression recognition, IEEE Computer Society Conf. on Computer Vision and Pattern Recognition (CVPR'05)-Workshops, (2005); Cruz-Albarran I. A., Benitez-Rangel J. P., Osornio-Rios R. A., Morales-Hernandez L. A., Human emotions detection based on a smart-thermal system of thermographic images, Infrared Physics & Technology, 81, 1, pp. 250-261, (2017); Stemberger J., Allison R. S., Schnell T., Thermal imaging as a way to classify cognitive workload, Canadian Conf. on Computer and Robot Vision, pp. 231-238, (2010); Jarlier S., Grandjean D., Delplanque S., N'Diaye K., Cayeux I., Et al., Thermal analysis of facial muscles contractions, IEEE Transactions on Affective Computing, 2, 1, pp. 2-9, (2011); Rivera H., Goulart C., Favarato A., Valadao C., Caldeira E., Et al., Development of an automatic expression recognition system based on facial action coding system, Simpósio Brasileiro de Automação Inteligente (SBAI2017), pp. 615-620, (2017); Ojala T., Pietikainen M., Maenpaa T., Multiresolution gray-scale and rotation invariant texture classification with local binary patterns, IEEE Transactions on Pattern Analysis and Machine Intelligence, 24, 7, pp. 971-987, (2002); Mallat K., Dugelay J. L., A benchmark database of visible and thermal paired face images across multiple variations, Int. Conf. of the Biometrics Special Interest Group (BIOSIG), pp. 1-5, (2018)","A.S.A. Mohamed; School of Computer Sciences, Universiti Sains Malaysia, Penang, 11800, Malaysia; email: sufril@usm.my","","Tech Science Press","","","","","","02676192","","CSSEE","","English","Comput Syst Sci Eng","Article","Final","All Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85103239312"
"Bello R.-W.; Mohamed A.S.A.; Talib A.Z.; Sani S.; Ab Wahab M.N.","Bello, Rotimi-Williams (57209469141); Mohamed, Ahmad Sufril Azlan (57190968285); Talib, Abdullah Zawawi (35570816900); Sani, Salisu (57522765300); Ab Wahab, Mohd Nadhir (57223397087)","57209469141; 57190968285; 35570816900; 57522765300; 57223397087","Behavior Recognition of Group-ranched Cattle from Video Sequences using Deep Learning","2022","Indian Journal of Animal Research","56","4","","505","512","7","0","10.18805/IJAR.B-1369","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131322545&doi=10.18805%2fIJAR.B-1369&partnerID=40&md5=a306a5730fdf64bae72ce6f0511cede4","School of Computer Sciences, Universiti Sains Malaysia, Pulau Pinang, 11800, Malaysia; Department of Mathematics/Computer Sciences, University of Africa, Toru-Orua, Bayelsa State, Nigeria; Department of Information Technology, Federal University, Jigawa State, Dutse, Nigeria","Bello R.-W., School of Computer Sciences, Universiti Sains Malaysia, Pulau Pinang, 11800, Malaysia, Department of Mathematics/Computer Sciences, University of Africa, Toru-Orua, Bayelsa State, Nigeria; Mohamed A.S.A., School of Computer Sciences, Universiti Sains Malaysia, Pulau Pinang, 11800, Malaysia; Talib A.Z., School of Computer Sciences, Universiti Sains Malaysia, Pulau Pinang, 11800, Malaysia; Sani S., School of Computer Sciences, Universiti Sains Malaysia, Pulau Pinang, 11800, Malaysia, Department of Information Technology, Federal University, Jigawa State, Dutse, Nigeria; Ab Wahab M.N., School of Computer Sciences, Universiti Sains Malaysia, Pulau Pinang, 11800, Malaysia","Background: One important indicator for the wellbeing status of livestock is their daily behavior. More often than not, daily behavior recognition involves detecting the heads or body gestures of the livestock using conventional methods or tools. To prevail over such limitations, an effective approach using deep learning is proposed in this study for cattle behavior recognition. Methods: The approach for detecting the behavior of individual cows was designed in terms of their eating, drinking, active, and inactive behaviors captured from video sequences and based on the investigation of the attributes and practicality of the state-of-the-art deep learning methods. Result: Among the four models employed, Mask R-CNN achieved average recognition accuracies of 93.34%, 88.03%, 93.51% and 93.38% for eating, drinking, active and inactive behaviors. This implied that Mask R-CNN achieved higher cow detection accuracy and speed than the remaining models with 20 fps, making the proposed approach competes favorably well with other approaches and suitable for behavior recognition of group-ranched cattle in real-time. © 2022 Agricultural Research Communication Centre. All rights reserved.","Behavior recognition; Deep learning; Group-ranched cattle; Mask R-CNN","accuracy; Article; augmentation index; behavior; behavior recognition; cow; deep learning; differential threshold; drinking; drinking behavior; eating; eating habit; learning; mathematical model; nonhuman; training; velocity; videorecording","","","","","Universiti Sains Malaysia, (304/ PKOMP/6315262)","The authors would like to acknowledge the financial support from Universiti Sains Malaysia (USM) under short term university grant (304/ PKOMP/6315262).","Bello R., Talib A., Mohamed A., Deep learning-based architectures for recognition of cow using cow nose image pattern, Gazi University Journal of Science, 33, pp. 831-844, (2020); Bello R.W., Olubummo D.A., Seiyaboh Z., Enuma O.C., Talib A.Z., Mohamed A.S.A., Cattle Identification: the History of Nose Prints Approach in Brief, IOP Conference Series: Earth and Environmental Science, 594, pp. 1-9, (2020); Bello R.W., Talib A.Z., Mohamed A.S.A., Contour extraction of individual cattle from an image using enhanced Mask R-CNN instance segmentation method, IEEE Access, 9, pp. 56984-57000, (2021); Bello R.W., Talib A.Z.H., Mohamed A.S.A.B., Deep belief network approach for recognition of cow using cow nose image pattern, Walailak Journal of Science and Technology, 18, pp. 1-14, (2021); Bochkovskiy A., Wang C.Y., Liao H.Y.M., YOLOv4: Optimal speed and accuracy of object detection, pp. 1-17, (2020); Chauhan J.H., Hadiya K.K., Dhami A.J., Sarvaiya N.P., Ovarian dynamics, plasma endocrine profile and fertility response following synchronization protocols in crossbred cows with cystic ovaries, Indian Journal of Animal Research, 55, pp. 127-133, (2021); Chouhan D., Aich R., Jain R.K., Chhabra D., Acute phase protein as biomarker for diagnosis of sub-clinical mastitis in cross-bred cows, Indian Journal of Animal Research, 55, pp. 193-198, (2021); Dohare A.K., Bangar Y.C., Sharma V.B., Verma M.R., Modelling the effect of mastitis on milk yield in dairy cows using covariance structures fitted to repeated measures, Indian Journal of Animal Research, 55, pp. 11-14, (2021); Fuentes A., Yoon S., Park J., Park D.S., Deep learning-based hierarchical cattle behavior recognition with spatiotemporal information, Computers and Electronics in Agriculture, 177, pp. 1-11, (2020); He K., Gkioxari G., Dollar P., Girshick R., Mask R-CNN, IEEE Transactions on Pattern Analysis and Machine Intelligence, 42, pp. 386-397, (2020); He K., Gkioxari G., Dollar P., Girshick R., Mask R-CNN, Proceedings of the IEEE International Conference on Computer Vision, pp. 2961-2969, (2017); Jiang B., Wu Q., Yin X., Wu D., Song H., He D., FLYOLOv3 deep learning for key parts of dairy cow body detection, Computers and Electronics in Agriculture, 166, pp. 1-8, (2019); Jiang M., Rao Y., Zhang J., Shen Y., Automatic behavior recognition of group housed goats using deep learning, Computers and Electronics in Agriculture, 177, pp. 1-13, (2020); Jingqiu G., Zhihai W., Ronghua G., Huarui W., Cow behavior recognition based on image analysis and activities, International Journal of Agricultural and Biological Engineering, 10, pp. 165-174, (2017); Kashiha M.A., Bahr C., Ott S., Moons C.P.H., Niewold T.A., Tuyttens F., Berckmans D., Automatic monitoring of pig locomotion using image analysis, Livestock Science, 159, pp. 141-148, (2014); Kim J., Chung Y., Choi Y., Sa J., Kim H., Chung Y., Park D., Kim H., Depth based detection of standing-pigs in moving noise environments, Sensors, 17, pp. 1-19, (2017); Lao F., Brown-Brandl T., Stinn J.P., Liu K., Teng G., Xin H., Automatic recognition of lactating sow behaviors through depth image processing, Computers and Electronics in Agriculture, 125, pp. 56-62, (2016); Nasirahmadi A., Hensel O., Edwards S.A., Sturm B., A new approach for categorizing pig lying behaviour based on a Delaunay triangulation method, Animal, 11, pp. 131-139, (2017); Pedersen L.J., Overview of commercial pig production systems and their main welfare challenges, Advances in Pig Welfare, pp. 1-23, (2018); Redmon J., Farhadi A., YOLOv3: An incremental improvement, pp. 1-6, (2018); Ren S., He K., Girshick R., Sun J., Faster R-CNN: Towards real-time object detection with region proposal networks, IEEE Transactions on Pattern Analysis and Machine Intelligence, 39, pp. 1137-1149, (2017); Russell B.C., Torralba A., Murphy K.P., Freeman W.T., LabelMe: A database and web-based tool for image annotation, International Journal of Computer Vision, 77, pp. 157-173, (2008); Saberioon M.M., Cisar P., Automated multiple fish tracking in three-dimension using a structured light sensor, Computers and Electronics in Agriculture, 121, pp. 215-221, (2016); Shen W., Cheng F., Zhang Y., Wei X., Fu Q., Zhang Y., Automatic recognition of ingestive-related behaviors of dairy cows based on triaxial acceleration, Information Processing in Agriculture, 7, pp. 427-443, (2020); Simonyan K., Zisserman A., Very deep convolutional networks for large-scale image recognition, (2015); Stavrakakis S., Li W., Guy J.H., Morgan G., Ushaw G., Johnson G.R., Edwards S.A., Validity of the Microsoft Kinect sensor for assessment of normal walking patterns in pigs, Computers and Electronics in Agriculture, 117, pp. 1-7, (2015); Thorat A.B., Borikar S.T., Siddiqui M.F.M.F., Rajurkar S.R., Moregaonkar S.D., Ghorpade P.B., Khawale T.S., A study on occurrence and haemato-biochemical alterations in SARA in cattle treated with different therapeutic regimens, Indian Journal of Animal Research, 55, pp. 90-95, (2021); Velarde A., Fabrega E., Blanco-Penedo I., Dalmau A., Animal welfare towards sustainability in pork meat production, Meat Science, 109, pp. 13-17, (2015); Yang A., Huang H., Zheng B., Li S., Gan H., Chen C., Yang X., Xue Y., An automatic recognition framework for sow daily behaviours based on motion and image analyses, Biosystems Engineering, 192, pp. 56-71, (2020); Yang Q., Xiao D., Lin S., Feeding behavior recognition for group-housed pigs with the Faster R-CNN, Computers and Electronics in Agriculture, 155, pp. 453-460, (2018); Zaborski D., Grzesiak W., Utilization of boosted classification trees for the detection of cows with conception difficulties, Indian Journal of Animal Research, 55, pp. 359-363, (2021); Zheng C., Zhu X., Yang X., Wang L., Tu S., Xue Y., Automatic recognition of lactating sow postures from depth images by deep learning detector, Computers and Electronics in Agriculture, 147, pp. 51-63, (2018); Zhu W., Guo Y., Jiao P., Ma C., Chen C., Recognition and drinking behaviour analysis of individual pigs based on machine vision, Livestock Science, 205, pp. 129-136, (2017)","A.S.A. Mohamed; School of Computer Sciences, Universiti Sains Malaysia, Pulau Pinang, 11800, Malaysia; email: sufril@usm.my; M.N. Ab Wahab; School of Computer Sciences, Universiti Sains Malaysia, Pulau Pinang, 11800, Malaysia; email: mohdnadhir@usm.my","","Agricultural Research Communication Centre","","","","","","03676722","","","","English","Ind. J. Ani. Res","Article","Final","All Open Access; Green Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85131322545"
"Ab Wahab M.N.; Mohamed A.S.A.; Chung K.C.","Ab Wahab, Mohd Nadhir (36471236100); Mohamed, Ahmad Sufril Azlan (57190968285); Chung, Kong Chee (57213590210)","36471236100; 57190968285; 57213590210","iPassenger: Smart passenger analytics system","2019","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11870 LNCS","","","465","476","11","1","10.1007/978-3-030-34032-2_41","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077861214&doi=10.1007%2f978-3-030-34032-2_41&partnerID=40&md5=7f75c2b2631152bbc49b569aae3af9fb","School of Computer Sciences, Universiti Sains Malaysia, Gelugor, 11800, Penang, Malaysia","Ab Wahab M.N., School of Computer Sciences, Universiti Sains Malaysia, Gelugor, 11800, Penang, Malaysia; Mohamed A.S.A., School of Computer Sciences, Universiti Sains Malaysia, Gelugor, 11800, Penang, Malaysia; Chung K.C., School of Computer Sciences, Universiti Sains Malaysia, Gelugor, 11800, Penang, Malaysia","Automation with intelligence has gradually become the community’s talking point around the world. Nowadays, even public transportation such as bus is equipped with video surveillance cameras mainly for monitoring and security purposes. However, they can also be used for other purposes, such as analytics system through passengers counting. This is important to help the bus service provider to improve their fleet management operation by keep tracking of the passenger ridership information. Based on this information, the service provider company can be more flexible with bus scheduling by calculation of lines and stations efficiently. Hence, this paper proposed an analytics system through video footages acquired from daily operation. The incoming and outgoing passengers are tracked based on their head using blob detection and trained MobileNet SSD. The reference line is drawn to make sure that the detected passengers’ head crossed the line before it is considered either as incoming or outgoing passengers. Based on this video analysis, the number of passengers is counted concurrent with the location of the bus. This system is tested in several cases and managed to give proper information for the bus service provider. The data is stored in a cloud database for history, graphically visualize the passenger ridership in a day, month or year. Therefore, this system is able to help the bus company to manage their resource efficiently and consequently improving their service quality and lower the cost of transportation. © Springer Nature Switzerland AG 2019.","Analytics system; Blob detection; Fleet management; Passenger ridership; Passengers counting","Bus transportation; Buses; Security systems; Analytics systems; Blob detection; Fleet management; Passenger ridership; Passengers counting; Fleet operations","","","","","School of Computer Sciences; University of Southern Maine, USM; Universiti Sains Malaysia, (PKOMP/6315262, PKOMP/8014001)","Funding text 1: This research is a collaboration between School of Computer Sciences, USM with Rapid Penang Sdn. Bhd. and affiliated with Robotics, Computer Vision & Image Processing (RCVIP) Research Group Lab at School of Computer Sciences, Universiti Sains Malaysia. This project is fully funded by USM Short Term Grant (PKOMP/6315262) and partially funded by USM RU Grant (PKOMP/8014001).; Funding text 2: Acknowledgments. This research is a collaboration between School of Computer Sciences, USM with Rapid Penang Sdn. Bhd. and affiliated with Robotics, Computer Vision & Image Processing (RCVIP) Research Group Lab at School of Computer Sciences, Universiti Sains Malaysia. This project is fully funded by USM Short Term Grant (PKOMP/6315262) and partially funded by USM RU Grant (PKOMP/8014001).","Allam Z., Newman P., Redefining the smart city: Culture, metabolism, and governance, Smart Cities, 1, 1, pp. 4-25, (2018); Albino V., Berardi U., Dangelico R.M., Smart cities: Definitions, dimensions, performance, and initiatives, J. Urban Technol., 22, 1, pp. 1724-1738, (2015); Debnath A.K., Chin H.C., Haque M.M., Yuen B., A methodological framework for benchmarking smart transport cities, Cities, 37, pp. 47-56, (2014); Sharaby N., Shiftan Y., The impact of fare integration on travel behavior and transit ridership, Transp. Policy, 21, pp. 63-70, (2012); Gerland H., McDonald I., BCounted: Automatic people counting and people flow management at airports, 15Th International Conference on Automated People Movers and Automated Transit Systems, pp. 74-81, (2016); Bonyar A., Geczy A., Harsanyi G., Hanak P., Passenger detection and counting inside vehicles for ecall-a review on current possibilities, IEEE 24Th International Symposium for Design and Technology in Electronic Packaging (SIITME), pp. 221-225, (2018); Oberli C., Landau D., Performance evaluation of UHF RFID technologies for real-time passenger tracking in intelligent public transportation systems, IEEE International Symposium on Wireless Communication Systems, pp. 108-112, (2008); Zhou Z., Chen B., Yu H., Understanding rfid counting protocols, IEEE/ACM Trans. Netw., 24, 1, pp. 312-327, (2016); Kulkarni D.R., Kulkarni S.H., Nalawade P.B., Jagtap S.P., Passenger counting in bus transport system: A review, Int. J. Innov. Emerg. Res. Eng., 3, 3, pp. 101-103, (2016); Ferreira M., de Gouveia J., Facchini E., Pokorny M., Dias E., Real-time monitoring of public transit passenger flows through Radio Frequency Identification-RFID technology embedded in fare smart cards, Latest Trends Syst, 2, pp. 599-605, (2012); de Potter P., Belet P., Poppe C., Verstockt S., Lambert P., van De Walle R., Passenger counting in public rail transport-using head-shoulder contour tracking, International Conference on Computer Vision Theory and Applications, pp. 705-708, (2012); Li F., Yang F., Liang H., Yang W., Automatic passenger counting system for bus based on RGB-D video, Proceedings of the 2Nd Annual International Conference on Electronics, Electrical Engineering and Information Science (EEEIS 2016), pp. 209-220, (2016); Sojol J.I., Piya N.F., Sadman S., Motahar T., Smart bus: An automated passenger counting system, Mathematics, 118, 18, pp. 3169-3177, (2018)","M.N. Ab Wahab; School of Computer Sciences, Universiti Sains Malaysia, Gelugor, 11800, Malaysia; email: mohdnadhir@usm.my","Badioze Zaman H.; Mohamad Ali N.; Ahmad M.N.; Smeaton A.F.; Shih T.K.; Velastin S.; Terutoshi T.","Springer","","6th International Conference on Advances in Visual Informatics, IVIC 2019","19 November 2019 through 21 November 2019","Bangi","235609","03029743","978-303034031-5","","","English","Lect. Notes Comput. Sci.","Conference paper","Final","","Scopus","2-s2.0-85077861214"
"Younis H.A.; Mohamed A.S.A.; Ab Wahab M.N.; Jamaludin R.; Salisu S.","Younis, Hussain A. (57210408507); Mohamed, A.S.A. (57190968285); Ab Wahab, M.N. (57223397087); Jamaludin, R. (54401335900); Salisu, Sani (57718330400)","57210408507; 57190968285; 57223397087; 54401335900; 57718330400","A New Speech Recognition Model in a Human-Robot Interaction Scenario Using NAO Robot: Proposal and Preliminary Model","2021","International Conference on Communication and Information Technology, ICICT 2021","","","","215","220","5","4","10.1109/ICICT52195.2021.9568457","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118471605&doi=10.1109%2fICICT52195.2021.9568457&partnerID=40&md5=3be5399e0e51b7d5a2dd6c1393f1d233","Universiti Sains Malaysia, School of Computer Sciences, USM, Penang, 11800, Malaysia; College of Education for Women, University of Basrah, Basrah, Iraq; Universiti Sains Malaysia, School of Computer Sciences, Minden, Penang, 11800, Malaysia; Universiti Sains Malaysia, Centre For Instructional Technology Multimedia, Minden, Penang, 11800, Malaysia; Information Technology, Federal University Dutse, Jigawa, Kano, Nigeria","Younis H.A., Universiti Sains Malaysia, School of Computer Sciences, USM, Penang, 11800, Malaysia, College of Education for Women, University of Basrah, Basrah, Iraq; Mohamed A.S.A., Universiti Sains Malaysia, School of Computer Sciences, Minden, Penang, 11800, Malaysia; Ab Wahab M.N., Universiti Sains Malaysia, School of Computer Sciences, USM, Penang, 11800, Malaysia; Jamaludin R., Universiti Sains Malaysia, Centre For Instructional Technology Multimedia, Minden, Penang, 11800, Malaysia; Salisu S., Universiti Sains Malaysia, School of Computer Sciences, USM, Penang, 11800, Malaysia, Information Technology, Federal University Dutse, Jigawa, Kano, Nigeria","There are several terms for speech recognition. Auto speech recognition (ASR), speech-to-text, and computer speech recognition are all terms used to describe speech recognition. A single user's voice it is necessary to distinguish between speech recognition and voice recognition. The first is to translate speech into text, such as, the audible voice and concept (human speech), and the second is to define only sound, such as, animal sound, car, etc. There is no algorithm that is specifically designed for this field; instead, techniques such as N-grams and neural networks are used to explain and treat this type. Natural Language Processing (NLP), Hidden Markov Model (HMM), and Speaker Diarization (SD). The last type would be addressed in my work. Natural language processing is a computational technique that can be used and applied to various levels of linguistic analysis (dare, deep analysis) to represent natural language in a useful or more representation. It is still possible to improve current recognition and identification systems in order to achieve greater accuracy. A new approach has been proposed that distinguishes speech in four stages: speech recognition, tokenization, extracting features of speech from texts, and part speech: The three patterns of Name Entity Recognition (NER), followed by the possibility of implementing the proposed model It achieved more accurate and applied results in an educational environment by using a NAO-robot.  © 2021 IEEE.","Model; NAO-robot; Natural Language Processing (NLP); Speech Recognition","Character recognition; Hidden Markov models; Human robot interaction; Linguistics; Speech; Speech recognition; Auto speech recognition; Computer speech recognition; Human speech; Humans-robot interactions; N-grams; NAO-robot; Natural language processing; Preliminary model; Recognition models; Single users; Natural language processing systems","","","","","School of Computer Sciences at Universiti Sains Malaysia; Universiti Sains Malaysia","This project is supported by the RUI grant (1001 / PMEDIA / 8016096), Universiti Sains Malaysia. Author acknowledges the contributions from Centre for Instructional Technology and Multimedia, Universiti Sains Malaysia and the School of Computer Sciences at Universiti Sains Malaysia.","Abdulkareem A., Somefun T.E., Chinedum O.K., Agbetuyi F., Design and implementation of speech recognition system integrated with internet of things, International Journal of Electrical and Computer Engineering (IJECE), 11, 2, pp. 1796-1803, (2021); Bartneck C., The end of the beginning: A reflection on the first five years of the hri conference, Scientometrics, 86, 2, pp. 487-504, (2011); Khilari P., A review on speech to text conversion methods, Int. J. Adv. Res. Comput. Eng. Technol., 4, 7, pp. 3067-3072, (2015); A review on automatic speech recognition architecture and approaches, Int. J. Signal Process. Image Process. Pattern Recognit., 9, 4, pp. 393-404, (2016); Mukherjee P., Santra S., Bhowmick S., Paul A., Chatterjee P., Deyasi A., Development of gui for text-To-speech recognition using natural language processing, 2018 2nd Int. Conf. Electron. Mater. Eng. Nano-Technology, IEMENTech 2018, pp. 1-4, (2018); Razak Z., Sumali S.R., Yamani M., Idris I., Ahmedy I., To-Text engine for jawi character, Cssr, pp. 565-568, (2010); Samad S.A., Hussain A., Fah L.K., Pitch detection of speech signals using the cross-correiation technique, Tencon 2000.Proceedings., pp. 283-286, (2000); Syiam M.M., Klash H.M., Mahmoud I., Haggag S.S., Hardware implementation of neural network on FPGA for accidents diagnosis of the multi-purpose research reactor of Egypt, Proceedings of the 15th International Conference on Microelectronics, pp. 326-329, (2003); Blunt P., Haskins B., A model for incorporating an automatic speech recognition system in a noisy educational environment, Proc.-2019 Int. Multidiscip. Inf. Technol. Eng. Conf. Imitec 2019, pp. 1-7, (2019); Padmanabhan J., Premkumar M.J.J., Machine learning in automatic speech recognition: A survey, Iete Tech. Rev. (Institution Electron. Telecommun. Eng. India), 32, 4, pp. 240-251, (2015); Younis H.A., Mohamed A.S.A., Jamaludin R., Ab Wahab M.N., Survey of robotics in education, taxonomy, applications, and platforms during covid-19, Computers, Materials and Continua, 67, pp. 687-707, (2021); Younis H.A., Jamaludin R., Wahab M.N.A., Mohamed A.S.A., The review of nao robotics in educational 2014-2020 inmcovid-19 virus (pandemic era): Technologies, type of application, advantage, disadvantage and motivation, Journal of Physics: Conference Series., (2020); Diyas Y., Brakk D., Aimambetov Y., Sandygulova A., Evaluating peer versus teacher robot within educational scenario of programming learning, ACM/IEEE Int. Conf. Human-Robot Interact., 2016, pp. 425-426, (2016); Mubin O., Alhashmi M., Baroud R., Alnajjar F.S., Humanoid robots as teaching assistants in an arab school, Acm Int. Conf. Proceeding Ser., pp. 462-466, (2019); Yoshino K., Zhang S., Construction of teaching assistant robot in programming class, Proc.-2018 7th Int. Congr. Adv. Appl. Informatics, IIAI-AAI 2018, pp. 215-220, (2018); Vrochidou E., Najoua A., Lytridis C., Salonidis M., Ferelis V., Papakostas G.A., Social robot nao as a self-regulating didactic mediator: A case study of teaching/learning numeracy, 2018 26th International Conference on Software, Telecommunications and Computer Networks, SoftCOM 2018, pp. 93-98; Furui S., Speech and Speaker Recognition Evaluation, (2007); Sakoe H., Chiba S., Dynamic programming algorithm optimization for spoken word recognition, Ieee Transactions on Acoustics, Speech and Signal Processing, 26, 1, pp. 43-49, (1978); Ngo T.V., Le Ha T., Nguyen P.T., Nguyen L.M., Combining advanced methods in Japanese-Vietnamese neural machine translation, Proc. 2018 10th Int. Conf. Knowl. Syst. Eng. Kse 2018, pp. 318-322, (2018); Li J., Deng L., Umbach R.H., Gong Y., Robust Automatic Speech Recognition: A Bridge to Practical Applications, (2015); Sanjana B., RejinaParvin J., Voice assisted text reading system for the visually impaired using tts method, IEEE. 10 Humanit. Technol. Conf. R10-HTC 9, pp. 1-6, (2016); Sun Z., Li Z., Nishimorii T., Development and assessment of robot teaching assistant in facilitating learning, Proc.-6th Int. Conf. Educ. Innov. Through Technol. Eitt 2017, 2018, pp. 165-169, (2018); Younis H.A., Mohamed A.S.A., Wahab M.N.A., Jamaludin R., A Review of the Applicability of Robots in Education with Neuro-linguistic Programming (NLP), (2021); Tanaka F., Takahashi T., Matsuzoe S., Tazawa N., Morita M., Telepresence robot helps children in communicating with teachers who speak a different language, ACM/IEEE Int. Conf. Human-Robot Interact., pp. 399-406, (2014)","","","Institute of Electrical and Electronics Engineers Inc.","","2021 International Conference on Communication and Information Technology, ICICT 2021","5 June 2021 through 6 June 2021","Basrah","173106","","978-166543914-5","","","English","Int. Conf. Commun. Inf. Technol., ICICT","Conference paper","Final","","Scopus","2-s2.0-85118471605"
"Wang C.; Mohamed A.S.A.","Wang, Chuanchuan (58507481300); Mohamed, Ahmad Sufril Azlan (57190968285)","58507481300; 57190968285","Attention Relational Network for Skeleton-Based Group Activity Recognition","2023","IEEE Access","11","","","129230","129239","9","0","10.1109/ACCESS.2023.3332651","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85177053317&doi=10.1109%2fACCESS.2023.3332651&partnerID=40&md5=d32122ccb8b653b1da18fcc97303039a","Universiti Sains Malaysia, George Town, School of Computer Sciences, Penang, 11800, Malaysia; Guangzhou College of Technology and Business, School of Engineering, Guangzhou, 510850, China","Wang C., Universiti Sains Malaysia, George Town, School of Computer Sciences, Penang, 11800, Malaysia, Guangzhou College of Technology and Business, School of Engineering, Guangzhou, 510850, China; Mohamed A.S.A., Universiti Sains Malaysia, George Town, School of Computer Sciences, Penang, 11800, Malaysia","Group activity recognition is a significant and challenging task in computer vision. The solution of group activity prediction can be classified with traditional hand-crafted features, RGB video features, and skeleton data-based deep learning architectures, such as Graph Convolutional Networks (GCNs), Recurrent Neural Networks (RNNs), and Long Short-Term Memory (LSTMs). However, they rarely explore pose information and rarely use relational networks to reason about group activity behavior. In this work, we leverage minimal prior knowledge about the skeleton information to reason about the interactions from group activity. The objective is to obtain discriminative representations and filter out some ambiguous actions to enhance the performance of group activity recognition. Our contribution is a proposed Attention Relation Network (ARN) that fuses the attention mechanisms and joint vector sequences into the relation network. The skeleton joints vector sequences are previously unexplored pose information and assign greater significance attributed to individuals who are more relevant for distinguishing the group activity behavior. First, our model focuses on the specified edge-level information (encompassing both edge and edge motion data) within the skeleton dataset, considering directionality, to analyze the spatiotemporal aspects of the action. Second, recognizing the inherent motion directionality, we establish diverse directions for skeleton edges and extract distinct motion features (including translation and rotation information) aligned with these various orientations, thereby augmenting the utilization of motion attributes related to the action. We also introduce a representation of human motion achieved by combining relational networks and examining their integrated characteristics. Extensive experiments were tested in the Hockey and UT-interaction datasets to evaluate our method, obtaining competitive performance to the state-of-the-art. Results demonstrate the modeling potential of a skeleton-based method for group activity recognition.  © 2013 IEEE.","attention mechanism; Group activity recognition; relational network; skeleton joint director sequences","Knowledge management; Musculoskeletal system; Recurrent neural networks; Three dimensional displays; Activity recognition; Attention mechanisms; Features extraction; Group activities; Group activity recognition; Relational network; Skeleton; Skeleton joint director sequence; Skeleton joints; Three-dimensional display; Sports","","","","","","","Dave I.R., Chen C., Shah M., SPAct: Self-supervised privacy preservation for action recognition, Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), pp. 20132-20141, (2022); Liu W., Kang G., Huang P.-Y., Chang X., Yu L., Qian Y., Liang J., Gui L., Wen J., Chen P., Hauptmann A.G., Argus: Efficient activity detection system for extended video analysis, Proc. IEEE Winter Appl. Comput. Vis. Workshops (WACVW), pp. 126-133, (2020); Li Y., Chen L., He R., Wang Z., Wu G., Wang L., Multi-Sports: A multi-person video dataset of spatio-temporally localized sports actions, Proc. IEEE/CVF Int. Conf. Comput. Vis. (ICCV), pp. 13516-13525, (2021); Siddiqui A.J., Boukerche A., A novel lightweight defense method against adversarial patches-based attacks on automated vehicle make and model recognition systems, J. Netw. Syst. Manage, 29, 4, (2021); Liu J., Tan R., Han G., Sun N., Kwong S., Privacy-preserving in-home fall detection using visual shielding sensing and private informationembedding, IEEE Trans. Multimedia, 23, pp. 3684-3699, (2021); Presotto R., Civitarese G., Bettini C., FedCLAR: Federated clustering for personalized sensor-based human activity recognition, Proc. IEEE Int. Conf. Pervasive Comput. Commun. (PerCom), pp. 227-236, (2022); Sharif M.H., Jiao L., Omlin C.W., CNN-ViT supported weaklysupervised video segment level anomaly detection, Sensors, 23, 18, (2023); Wang X., Ji Q., Hierarchical context modeling for video event recognition, IEEE Trans. Pattern Anal. Mach. Intell, 39, 9, pp. 1770-1782, (2017); Yan S., Xiong Y., Lin D., Spatial temporal graph convolutional networks for skeleton-based action recognition, Proc. AAAI Conf. Artif. Intell, pp. 1-9, (2018); Rajasegaran J., Pavlakos G., Kanazawa A., Feichtenhofer C., Malik J., On the benefits of 3D pose and tracking for human action recognition, Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), pp. 1-9, (2023); Zhou H., Liu Q., Wang Y., Learning discriminative representations for skeleton based action recognition, Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), pp. 10608-10617, (2023); Liu Y., Xue P., Li H., Wang C., A review of action recognition using joints based on deep learning, J. Electron. Inf. Technol, 43, 6, pp. 1789-1802, (2021); Duan H., Zhao Y., Chen K., Lin D., Dai B., Revisiting skeletonbased action recognition, Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), pp. 2959-2968, (2022); Du Y., Wang W., Wang L., Hierarchical recurrent neural network for skeleton based action recognition, Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), Boston, MA, USA, pp. 1110-1118, (2015); Xia K., Huang J., Wang H., LSTM-CNN architecture for human activity recognition, IEEE Access, 8, pp. 56855-56866, (2020); Ahmad T., Jin L., Zhang X., Lai S., Tang G., Lin L., Graph convolutional neural network for human action recognition: A comprehensive survey, IEEE Trans. Artif. Intell, 2, 2, pp. 128-145, (2021); Xu K., Ye F., Zhong Q., Topology-aware convolutional neural network for efficient skeleton-based action recognition, Proc. AAAI Conf. Artif. Intell, pp. 1-9, (2022); Abdullahi S.B., Chamnongthai K., American sign language words recognition of skeletal videos using processed video driven multi-stacked deep LSTM, Sensors, 22, 4, (2022); Abdullahi S.B., Chamnongthai K., American sign language words recognition using spatio-temporal prosodic and angle features: A sequential learning approach, IEEE Access, 10, pp. 15911-15923, (2022); Perez M., Liu J., Kot A.C., Interaction recognition through body parts relation reasoning, Pattern Recognition. Auckland, New Zealand, Cham, Switzerland: Springer, (2020); Askari F., Ramaprasad R., Clark J.J., Levine M.D., Interaction classification with key actor detection in multi-person sports videos, Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. Workshops (CVPRW), pp. 3579-3587, (2022); Ryoo M.S., Aggarwal J.K., Spatio-temporal relationship match: Video structure comparison for recognition of complex human activities, Proc. IEEE 12th Int. Conf. Comput. Vis., pp. 1593-1600, (2009); Lu L., Lu Y., Yu R., Di H., Zhang L., Wang S., GAIM: Graph attention interaction model for collective activity recognition, IEEE Trans. Multimedia, 22, 2, pp. 524-539, (2020); Wang P., Li W., Li C., Hou Y., Action recognition based on joint trajectory maps with convolutional neural networks, Knowl.-Based Syst, 158, pp. 43-53, (2018); Si C., Chen W., Wang W., Wang L., Tan T., An attention enhanced graph convolutional LSTM network for skeleton-based action recognition, Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), pp. 1227-1236, (2019); Song S., Lan C., Xing J., Zeng W., Liu J., An end-to-end spatio-temporal attention model for human action recognition from skeleton data, Proc. AAAI Conf. Artif. Intell, pp. 1-10, (2017); Carreira J., Zisserman A., Quo vadis, action recognition? A new model and the kinetics dataset, Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), pp. 4724-4733, (2017); Ibrahim M.S., Muralidharan S., Deng Z., Vahdat A., Mori G., A hierarchical deep temporal model for group activity recognition, Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), pp. 1971-1980, (2016); Wang X., Xu X., Mu Y., Neural Koopman pooling: Control-inspired temporal dynamics encoding for skeleton-based action recognition, Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), pp. 10597-10607, (2023); Chen Y., Zhang Z., Yuan C., Li B., Deng Y., Hu W., Channel-wise topology refinement graph convolution for skeleton-based action recognition, Proc. IEEE/CVF Int. Conf. Comput. Vis. (ICCV), pp. 13339-13348, (2021); Shi L., Zhang Y., Cheng J., Lu H., Two-stream adaptive graph convolutional networks for skeleton-based action recognition, Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), pp. 12018-12027, (2019); Ye F., Pu S., Zhong Q., Li C., Xie D., Tang H., Dynamic GCN: Context-enriched topology learning for skeleton-based action recognition, Proc. 28th ACM Int. Conf. Multimedia, pp. 1-10, (2020); Graves A., Long short-term memory, Supervised Sequence Labelling With Recurrent Neural Networks, pp. 37-45, (2012); Farah S., A David W., Humaira N., Aneela Z., Steffen E., Short-term multi-hour ahead country-wide wind power prediction for Germany using gated recurrent unit deep learning, Renew. Sustain. Energy Rev, 167, (2022); Song S., Lan C., Xing J., Zeng W., Liu J., Spatio-temporal attention-based LSTM networks for 3D action recognition and detection, IEEE Trans. Image Process, 27, 7, pp. 3459-3471, (2018); Tang J., Shu X., Yan R., Zhang L., Coherence constrained graph LSTM for group activity recognition, IEEE Trans. Pattern Anal. Mach. Intell, 44, 2, pp. 636-647, (2022); Li R., Wang S., Zhu F., Adaptive graph convolutional neural networks, Proc. AAAI Conf. Artif. Intell, pp. 1-9, (2018); Li S., He X., Song W., Hao A., Qin H., Graph diffusion convolutional network for skeleton based semantic recognition of two-person actions, IEEE Trans. Pattern Anal. Mach. Intell, 45, 7, pp. 8477-8493, (2023); Santoro A., Raposo D., Barrett D.G., A simple neural network module for relational reasoning, Proc. Adv. Neural Inf. Process. Syst, 30, pp. 1-10, (2017); Johnson J., Hariharan B., Maaten Der L.Van, Fei-Fei L., Zitnick C.L., Girshick R., CLEVR: A diagnostic dataset for compositional language and elementary visual reasoning, Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), pp. 1988-1997, (2017); Hasan Chowdhury M.I., Nguyen K., Sridharan S., Fookes C., Hierarchical relational attention for video question answering, Proc. 25th IEEE Int. Conf. Image Process. (ICIP), pp. 599-603, (2018); Ibrahim M.S., Mori G., Hierarchical relational networks for group activity recognition and retrieval, Proc. 15th Eur. Conf. Comput. Vis. (ECCV), Munich, Germany, pp. 1-16, (2018); Perez M., Liu J., Kot A.C., Interaction relational network for mutual action recognition, IEEE Trans. Multimedia, 24, pp. 366-376, (2022); Perez M., Liu J., Kot A.C., Skeleton-based relational reasoning for group activity analysis, Pattern Recognit, 122, (2022); Lyons R., Distance covariance in metric spaces, Ann. Probab, 41, 5, pp. 3284-3305, (2013); Sohn Y., Rebello N.S., Supervised and unsupervised spectral angle classifiers, Photogramm. Eng. Remote Sens, 68, 12, pp. 1271-1282, (2002); Rosenfeld A., Weszka J.S., An improved method of angle detection on digital curves, IEEE Trans. Comput, C-24, 9, pp. 940-941, (1975); Luong M.-T., Pham H., Manning C.D., Effective approaches to attention-based neural machine translation, (2015); Askari F., Jiang R., Li Z., Niu J., Shi Y., Clark J.J., Self-supervised video interaction classification using image representation of skeleton data, Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit.Workshops (CVPRW), pp. 5229-5238, (2023); Cao Z., Hidalgo G., Simon T., Wei S.-E., Sheikh Y., OpenPose: Realtime multi-person 2D pose estimation using part affinity fields, IEEE Trans. Pattern Anal. Mach. Intell, 43, 1, pp. 172-186, (2021); Donahue J., Hendricks L.A., Guadarrama S., Rohrbach M., Venugopalan S., Darrell T., Saenko K., Long-term recurrent convolutional networks for visual recognition and description, Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), pp. 2625-2634, (2015); Jiang M., Dong J., Ma D., Sun J., He J., Lang L., Inception spatial temporal graph convolutional networks for skeleton-based action recognition, Proc. Int. Symp. Control Eng. Robot. (ISCER), pp. 208-213, (2022); Kong Y., Fu Y., Close human interaction recognition using patchaware models, IEEE Trans. Image Process, 25, 1, pp. 167-178, (2016); Cao Y., Barrett D., Barbu A., Narayanaswamy S., Yu H., Michaux A., Lin Y., Dickinson S., Siskind J.M., Wang S., Recognize human activities from partially observed videos, Proc. IEEE Conf. Comput. Vis. Pattern Recognit., pp. 2658-2665, (2013); Lan T., Chen T.-C., Savarese S., A hierarchical representation for future action prediction, Computer Vision-ECCV 2014, Zurich, Switzerland, Cham, Switzerland: Springer, (2014); Berlin S.J., John M., Human interaction recognition through deep learning network, Proc. IEEE Int. Carnahan Conf. Secur. Technol. (ICCST), pp. 1-4, (2016); Sener F., Ikizler-Cinbis N., Two-person interaction recognition via spatial multiple instance embedding, J. Vis. Commun. Image Represent, 32, pp. 63-73, (2015); Amer M.R., Todorovic S., Sum product networks for activity recognition, IEEE Trans. Pattern Anal. Mach. Intell, 38, 4, pp. 800-813, (2016); Ke Q., Bennamoun M., An S., Sohel F., Boussaid F., Leveraging structural context models and ranking score fusion for human interaction prediction, IEEE Trans. Multimedia, 20, 7, pp. 1712-1723, (2018)","A.S.A. Mohamed; Universiti Sains Malaysia, George Town, School of Computer Sciences, Penang, 11800, Malaysia; email: sufril@usm.my","","Institute of Electrical and Electronics Engineers Inc.","","","","","","21693536","","","","English","IEEE Access","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85177053317"
"Mohamed A.S.A.; Chingeng P.S.; Mat Isa N.A.; Surip S.S.","Mohamed, A.S.A. (57190968285); Chingeng, P.S. (57197832344); Mat Isa, N.A. (6603297760); Surip, S.S. (57192173209)","57190968285; 57197832344; 6603297760; 57192173209","Body matching algorithm using normalize dynamic time warping (NDTW) skeleton tracking for traditional dance movement","2017","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","10645 LNCS","","","669","680","11","1","10.1007/978-3-319-70010-6_62","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85035135801&doi=10.1007%2f978-3-319-70010-6_62&partnerID=40&md5=8dc415760e01c20cbb444fa60b3576c1","School of Computer Sciences, Universiti Sains Malaysia, Gelugor, Malaysia; School of the Arts, Universiti Sains Malaysia, Gelugor, Malaysia","Mohamed A.S.A., School of Computer Sciences, Universiti Sains Malaysia, Gelugor, Malaysia; Chingeng P.S., School of Computer Sciences, Universiti Sains Malaysia, Gelugor, Malaysia; Mat Isa N.A., School of Computer Sciences, Universiti Sains Malaysia, Gelugor, Malaysia; Surip S.S., School of the Arts, Universiti Sains Malaysia, Gelugor, Malaysia","Traditional dance in Malaysia is generating considerable amount of interest due to its unique elements of heritage which have contributed to its diverse music and dance forms. For example, Zapin, Kuda Kepang, Mak Yong, Joget, Ngajat and much more. Recent developments in technology and ever- growing online community, traditional dance are undergoing a revolution where these dance form can be studied and observed easily especially when there are dance software that can help guide users to learn by performing the dance steps in real-time. However, the use of gesture sensor for accurately mapping the dance movements of traditional dance is not yet explored, since only modern dances are normally available to the masses in the form of computer games. This paper outlines a new approach to implement Normalize Dynamic Time Warping (NDTW) algorithm using skeleton tracking techniques to imitate the intricate movements of traditional dance and to assess the robustness of the algorithm. For this study, the traditional dance of Zapin was chosen because it consists of simple body movements and data were acquired using Microsoft Kinect. The results showed that the proposed algorithm gave the overall matching rate of 99.21% with maximum mean success rate of dancers gave 99.68% and non-dancers gave the percentage of 98.76%. This technique may be considered as a relatively unexplored application area, and the proposed system is an attempt to address the problem with reasonable accuracy and scopes for further research. © Springer International Publishing AG 2017.","Body matching; Kinect; Motion capture; Skeleton tracking; Traditional dance","Computer games; Body matching; Dynamic time warping; Kinect; Motion capture; On-line communities; Reasonable accuracy; Tracking techniques; Traditional dance; Musculoskeletal system","","","","","Universiti Sains Malaysia, USM, (304/PKOMP/6313280)","Acknowledgements. The author wish to thank Universiti Sains Malaysia for the support it has extended in the completion of the present research through Short Term University Grant No. 304/PKOMP/6313280.","Alexiadis D.S., Kelly P., Daras P., O'Connor N.E., Boubekeur T., Moussa M.B., Evaluating a dancer’s performance using kinect-based skeleton tracking, ACM International Conference on Multimedia, pp. 659-662, (2011); Brodd-Reijer C., Dance Quantification with Kinect: Adjusting Music Volume by Using Depth Data from a Kinect Sensor, (2012); Carmona J.M., Climent J., A performance evaluation of HMM and DTW for gesture recognition, CIARP 2012. LNCS, 7441, pp. 236-243, (2012); Correa D.S., Sciotti D.F., Prado M.G., Mobile robots navigation in indoor environments using kinect sensor, Second Brazilian Conference on Critical Embedded Systems (CBSEC), pp. 36-41, (2012); Csaba G., Somlyai L., Vamossy Z., Differences between kinect and structured lighting sensor in robot navigation, 2012 IEEE 10Th International Symposium Applied Machine Intelligence and Informatics (SAMI), pp. 85-90, (2012); Essid S., Alexiadis D., Tournemenne R., Gowing M., Kelly P., Monaghan D., Daras P., Dremeau A., O'Connor N.E., An advanced virtual dance performance evaluator, 2012 IEEE International Conference Acoustics, Speech and Signal Processing (ICASSP), (2012); Gowing M., Kell P., O'Connor N.E., Concolato C., Essid S., Lefeuvre J., Zhang Q., Enhanced visualisation of dance performance from automatically synchronised multimodal recordings, MM International Multimedia Conference, pp. 667-670, (2011); Hani B.-S., Clinton J., Evaluating the effect of 3D world integration within a social software environment, 2015 12Th International Conference Information Technology -New Generations (ITNG), pp. 255-260, (2015); Higinio G., Riveiro B., Esteban V.-F., Martinez-Sanchez J., Pedro A., Metrological evaluation of microsoft kinect and asus xtion sensors, pp. 1800-1806, (2013); Hu M.-C., Chen C.-W., Cheng W.-H., Chang C.-H., Lai J.-H., Wu J.-L., Real-time human movement retrieval and assessment with kinect sensor, IEEE Trans. Cybern, pp. 742-753, (2014); Huang C.-H., Boyer E., Ilic S., Robust human body shape and pose tracking, International Conference on 3D Vision, pp. 287-294, (2013); Jia W., Won-Jae Y., Jafar S., Erdal O., 3D image reconstruction and human body tracking using stereo vision and kinect technology, 2012 IEEE International Conference Electro/Information Technology (EIT, (2012); Jo H., Yu H., Kim K., Sung J.H., Motion tracking system for multi-user with multiple kinects, Int. J. U-And E-Serv. Sci. Technol., pp. 99-108, (2015); Kar R., Konar A., Chakraborty A., Dance composition using microsoft kinect, Transactions on Computational Science XXV. LNCS, 9030, pp. 20-34, (2015); Adistambha K., Ritz C.H., Burnett I.S., Motion classification using dynamic time warping, 2008 IEEE 10Th Workshop on Multimedia Signal Processing, (2008); Kitsikidis A., Dimitropoulos K., Douka S., Grammalidis N., Dance analysis using multiple Kinect sensors, Computer Vision Theory and Applications (VISAPP), pp. 789-795, (2014); Kyan M., Sun G., Li H., Zhong L., Muneesawang P., Dong N., Guan L., An approach to ballet dance training through MS kinect and visualization in a cave virtual reality environment, ACM Trans. Intell. Syst. Technol. (TIST), 623, (2015); Marija M., Mile J., Darko M., Analysis of the problem of Macedonian folk dance recognition, Conference for Informatics and Information Technology, (2013); Martin C.C., Burkert D.C., Choi K.R., A real-time ergonomic monitoring system using the microsoft kinect, Systems and Information Symposium (SIEDS, pp. 50-55, (2012); Mohamed A., Surip S., Real-time interactive cultural dance with gesture gaming elements via kinect-based skeleton tracking, 6Th International Conference on Local Knowledge (ICLK, pp. 385-391, (2016); Moran A., Kamhi G., Popov A., Groscot R., Introducing Intel® RealSense™. Robotics Innovation Program, (2015); Nazeeh A., Khan A., Alnowaimi M., Morfeq A.H., Ehab A.H., Accuracy of Joint Angles Tracking Using Markerless Motion System, (2014); Papadopoulos G.T., Axenopoulos A., Daras P., Real-time skeleton-tracking-based human action recognition using kinect data, MMM 2014. LNCS, 8325, pp. 473-483, (2014); Park H., Lee J., Bae J., Development of a dance rehabilitation system using kinect and a vibration feedback glove, 2015 15Th International Conference Control, Automation and Systems (ICCAS, (2015); Pohl H., Hadjakos A., Dance pattern recognition using dynamic time warping, Sound and Music Computing, (2010); Raptis M., Kirovski D., Hoppe H., Real-time classification of dance gestures from skeleton animation, Symposium on Computer Animation, pp. 147-156, (2011); Schulz S., Woerner A., Automatic motion segmentation for human motion synthesis, AMDO 2010. LNCS, 6169, pp. 182-191, (2010); Sungphill M., Youngbin P., Wook K.D., Hong S.I., Multiple kinect sensor fusion for human, Int. J. Adv. Robot. Syst., (2015); Tang J.K., Chan J.C., Leung H., Interactive dancing game with real-time recognition of continuous dance moves from 3D human motion capture, International Conference on Ubiquitous Information Management and Communication, (2011); Vantigodi S., Radhakrishnan V.B., Action recognition from motion capture data using meta-cognitive RBF network classifier, 2014 IEEE Ninth International Conference Intelligent Sensors, Sensor Networks and Information Processing (ISSNIP, pp. 1-6, (2014); Wang Q., Kurillo G., Ofli F., Bajcsy R., Evaluation of pose tracking accuracy in the first and second generations of microsoft kinect, Healthcare Informatics, pp. 380-389, (2015); Yang Y., Leung H., Deng L., Automatic dance lesson generation, IEEE Trans. Learn. Technol., pp. 191-198, (2011); (2016); (2016); (2016); Makwana H., Singh T., Comparison of different algorithm for face recognition, Global J. Comput. Sci. Technol. Graph. Vis, 13, 9, (2013); Ahonen T., Hadid A., Pietikainen M., Face recognition with local binary patterns, ECCV 2004. LNCS, 3021, pp. 469-481, (2004); Singh A., Comparison of face recognition algorithms on dummy faces, Int. J. Multimed. Appl., 4, 4, pp. 121-135, (2012); Muhammad M.A.N., Ruhaiyem N.I.R., Mohamed A.S.A., Keeping curiosity in local historical knowledge alive by sensor based simulation game using flash actionscript 3, Proceedings of the International Conference Local Knowledge, (2016); Ravi P.L., Ruhaiyem N.I.R., Intelligent gameplay for improved retro games, J. Telecommun. Electron. Comput. Eng. (JTEC), 8, 6, pp. 23-26, (2016)","A.S.A. Mohamed; School of Computer Sciences, Universiti Sains Malaysia, Gelugor, Malaysia; email: sufril@usm.my","Shih T.K.; Velastin S.; Robinson P.; Smeaton A.F.; Terutoshi T.; Badioze Zaman H.; Jaafar A.; Mohamad Ali N.","Springer Verlag","","5th International Visual Informatics Conference, IVIC 2017","28 November 2017 through 30 November 2017","Bangi","205759","03029743","978-331970009-0","","","English","Lect. Notes Comput. Sci.","Conference paper","Final","","Scopus","2-s2.0-85035135801"
"Bello R.-W.; Talib A.Z.H.; Mohamed A.S.A.B.","Bello, Rotimi-Williams (57209469141); Talib, Abdullah Zawawi Hj (35570816900); Mohamed, Ahmad Sufril Azlan Bin (57190968285)","57209469141; 35570816900; 57190968285","Deep belief network approach for recognition of cow using cow nose image pattern","2021","Walailak Journal of Science and Technology","18","5","8984","1","14","13","8","10.48048/wjst.2021.8984","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102515523&doi=10.48048%2fwjst.2021.8984&partnerID=40&md5=d71a97f803cc60b24bf406f16b3553b1","School of Computer Sciences, Universiti Sains Malaysia, Pulau Pinang, Malaysia","Bello R.-W., School of Computer Sciences, Universiti Sains Malaysia, Pulau Pinang, Malaysia; Talib A.Z.H., School of Computer Sciences, Universiti Sains Malaysia, Pulau Pinang, Malaysia; Mohamed A.S.A.B., School of Computer Sciences, Universiti Sains Malaysia, Pulau Pinang, Malaysia","A deep belief network is proposed to learn the discriminatory cow nose image texture features for a robust representation of cows' features and recognition using a cow nose image pattern. Deep belief network is a deep learning model that is graphically based, and it is applied to learn the extracted feature sets of cow nose image pattern for hierarchical representation by using the training details of the training phase of the system proposed. Deep belief network application is useful in animal biometrics to monitor the animals through its recognition and identification techniques. Biometrics application emanated from computer vision and pattern recognition. Its application plays an important role in registering and monitoring animals through its recognition and identification techniques. Because the existing physical-based feature representation methods and manual visual feature extractions cannot handle animal recognition, the deep belief network technique is proposed using the animal's visual attributes. An experiment performed under a controlled condition of identification indicated that the proposed method outshines the existing methods with approximately 98.99 % accuracy. Four thousand cow nose images from an existing database of 400 individual cows contribute to the community of research, especially in the animal biometrics for identification of individual cow. © 2021, Walailak University. All rights reserved.","Animal biometrics; Convolutional neural network; Cow nose image; Deep belief network; Identification","","","","","","","","Vlad M, Parvulet RA, Vlad MS, A survey of livestock identification systems,  WSEAS International Conference on Automation and Information, pp. 165-170, (2012); Roberts CM, Radio frequency identification (RFID), Comput. Secur, 25, pp. 18-26, (2006); Wang Z, Fu Z, Chen W, Hu J, A RFID-based traceability system for cattle breeding in china, Proceedings of the 2010 IEEE International Conference on Computer Application and System Modeling, pp. V2-567, (2010); Krizhevsky A, Sutskever I, Hinton G, Imagenet classification with deep convolutional neural networks, Proceedings of the Advances in Neural Information Processing Systems, pp. 1097-1105, (2012); Farabet C, Couprie C, Najman L, LeCun Y, Learning hierarchical features for scene labeling, IEEE Trans. Pattern Anal. Mac. Intell, 35, pp. 1915-1929, (2013); Sun Y, Wang X, Tang X, Deep convolutional network cascade for facial point detection, Proc. IEEE Conf. Comput. Vis. Pattern Recognit, 2013, pp. 3476-3483, (2013); Kumar S, Singh SK, Visual animal biometrics: Survey, IET Biometrics, 6, pp. 139-156, (2016); Kumar S, Singh SK, Datta T, Gupta HP, A fast cattle recognition system using smart devices, Proceedings of the 2016 ACM conference on Multimedia, pp. 742-743, (2016); Barron UG, Butler F, McDonnell K, Ward S, The end of the identity crisis? Advances in biometric markers for animal identification, Irish Veterinary J, 62, pp. 204-208, (2009); Jain AK, Ross AA, Nandakumar K, Introduction to biometrics, (2011); Giot R, El-Abed M, Rosenberger C, Fast computation of the performance evaluation of biometric systems: Application to multibiometrics, Future Generat. Comput. Syst, 29, pp. 788-799, (2013); Jain AK, Ross A, Prabhakar S, An introduction to biometric recognition, IEEE Trans. Circ. Syst. Video Tech, 14, pp. 4-20, (2004); Petersen WE, The identification of the bovine by means of nose-prints, J. Dairy Sci, 5, pp. 249-258, (1922); Kohl HS, Burkhart T, Animal biometrics: Quantifying and detecting phenotypic appearance, Trends Ecol. Evol, 28, pp. 432-441, (2013); Reiter S, Sattlecker G, Lidauer L, Kickinger F, Ohlschuster M, Auer W, Schweinzer V, Klein-Jobstl D, Drillich M, Iwersen M, Evaluation of an ear-tag-based accelerometer for monitoring rumination in dairy cows, J. Dairy Sci, 101, pp. 3398-3411, (2018); Seijas C, Montilla G, Frassato L, Identification of Rodent Species Using Deep Learning, Computación y Sistemas, 23, (2019); Hansen MF, Smith ML, Smith LN, Salter MG, Baxter EM, Farish M, Grieve B, Towards on-farm pig face recognition using convolutional neural networks, Comput. Ind, 98, pp. 145-152, (2018); Kumar S, Singh SK, Cattle recognition: A new frontier in visual animal biometrics research, Proceedings of the National Academy of Sciences, pp. 1-20, (2019); Norouzzadeh MS, Nguyen A, Kosmala M, Swanson A, Palmer MS, Packer C, Clune J, Automatically identifying, counting, and describing wild animals in camera-trap images with deep learning, Proc. Natl. Acad. Sci Unit States Am, 115, pp. E5716-E5725, (2018); Zin TT, Phyo CN, Tin P, Hama H, Kobayashi I, Image technology based cow identification system using deep learning, Proceedings of the International Multi Conference of Engineers and Computer Scientists, (2018); Kumar S, Pandey A, Satwik KSR, Kumar S, Singh SK, Singh AK, Mohan A, Deep learning framework for recognition of cattle using muzzle point image pattern, Measurement, 116, pp. 1-17, (2018); Iswanto IA, Li B, Visual object tracking based on mean-shift and particle-Kalman filter, Proc. Comput. Sci, 116, pp. 587-595, (2017); Noviyanto A, Arymurthy AM, Automatic cattle identification based on muzzle photo using speed-up robust features approach,  European Conference of Computer Science, (2012); Nasirahmadi A, Richter U, Hensel O, Edwards S, Sturm B, Using machine vision for investigation of changes in pig group lying patterns, Comput. Electron. Agr, 119, pp. 184-190, (2015); Minagawa H, Fujimura T, Ichiyanagi M, Tanaka K, Fangquan M, Identification of beef cattle by analyzing images of their muzzle patterns lifted on paper,  Asian Conference for Information Technology in Asian Agricultural Information Technology and Management, pp. 596-600, (2002); Barry B, Gonzales-Barron U, McDonnell K, Butler F, Ward S, Using muzzle pattern recognition as a biometric approach for cattle identification, Trans. ASABE, 50, pp. 1073-1080, (2007); Dalal N, Triggs B, Histograms of oriented gradients for human detection, Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, pp. 886-893, (2005); Awad AI, Zawbaa HM, Mahmoud HA, Nabi EHHA, Fayed RH, Hassanien AE, A robust cattle identification scheme using muzzle print images, Proceedings of IEEE Federated Conference on Computer Science and Information Systems, pp. 529-534, (2013); Noviyanto A, Arymurthy AM, Beef cattle identification based on muzzle pattern using a matching refinement technique in the sift method, Comp. Electr. Agr, 99, pp. 77-84, (2013); Ehsani K, Bagherinezhad H, Redmon J, Mottaghi R, Farhadi A, Who let the dogs out? Modeling dog behavior from visual data, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 4051-4060, (2018); Kumar S, Tiwari S, Singh SK, Face recognition for cattle,  IEEE International Conference on Image Information Processing, pp. 65-72, (2015); Gaber T, Tharwat A, Hassanien AE, Snasel V, Biometric cattle identification approach based on webers local descriptor and AdaBoost classifier, Comp. Electr. Agr, 122, pp. 55-66, (2016); Risha KP, Chempak KA, Sindhu CS, Difference of gaussian on frame differenced image, Int. J. Innovat. Res. Electr. Electron. Instrum. Contr. Eng, 3, pp. 92-95, (2016); Vincent P, Larochelle H, Lajoie I, Bengio Y, Manzagol PA, Stacked denoising auto-encoders: Learning useful representations in a deep network with a local denoising criterion, J. Mach. Learn. Res, 11, pp. 3371-3408, (2010); Vincent P, Larochelle H, Bengio Y, Manzagol PA, Extracting and composing robust features with denoising autoencoders,  International Conference on Machine Learning, pp. 1096-1103, (2008); Bengio Y, Learning deep architectures for AI, Found. Trends Mach. Learn, 2, pp. 1-127, (2009); Bengio Y, Courville A, Vincent P, Representation learning: A review and new perspectives, IEEE Trans. Pattern Anal. Mach. Intell, 35, pp. 1798-1828, (2013)","R.-W. Bello; School of Computer Sciences, Universiti Sains Malaysia, Pulau Pinang, Malaysia; email: sirbrw@yahoo.com","","Walailak University","","","","","","16863933","","","","English","Walailak J. Sci. Technol.","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85102515523"
"Yunus M.N.H.; Jaafar M.H.; Mohamed A.S.A.; Azraai N.Z.; Hossain M.S.","Yunus, Muhamad Nurul Hisyam (57226543249); Jaafar, Mohd Hafiidz (57210229668); Mohamed, Ahmad Sufril Azlan (57190968285); Azraai, Nur Zaidi (57191244519); Hossain, Md. Sohrab (55147219600)","57226543249; 57210229668; 57190968285; 57191244519; 55147219600","Implementation of kinetic and kinematic variables in ergonomic risk assessment using motion capture simulation: A review","2021","International Journal of Environmental Research and Public Health","18","16","8342","","","","23","10.3390/ijerph18168342","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111943890&doi=10.3390%2fijerph18168342&partnerID=40&md5=9a2c74426b27ae5842d1135e9b4574c1","School of Industrial Technology, Universiti Sains Malaysia, USM, Penang, 11800, Malaysia; National Poison Centre, Universiti Sains Malaysia, USM, Penang, 11800, Malaysia; School of Computer Sciences, Universiti Sains Malaysia, USM, Penang, 11800, Malaysia; School of the Arts, Universiti Sains Malaysia, USM, Penang, 11800, Malaysia","Yunus M.N.H., School of Industrial Technology, Universiti Sains Malaysia, USM, Penang, 11800, Malaysia; Jaafar M.H., School of Industrial Technology, Universiti Sains Malaysia, USM, Penang, 11800, Malaysia, National Poison Centre, Universiti Sains Malaysia, USM, Penang, 11800, Malaysia; Mohamed A.S.A., School of Computer Sciences, Universiti Sains Malaysia, USM, Penang, 11800, Malaysia; Azraai N.Z., School of the Arts, Universiti Sains Malaysia, USM, Penang, 11800, Malaysia; Hossain M.S., School of Industrial Technology, Universiti Sains Malaysia, USM, Penang, 11800, Malaysia","Work‐related musculoskeletal disorders (WMSDs) are among the most common disorders in any work sector and industry. Ergonomic risk assessment can reduce the risk of WMSDs. Motion capture that can provide accurate and real‐time quantitative data has been widely used as a tool for ergonomic risk assessment. However, most ergonomic risk assessments that use motion capture still depend on the traditional ergonomic risk assessment method, focusing on qualitative data. There-fore, this article aims to provide a view on the ergonomic risk assessment and apply current motion capture technology to understand classical mechanics of physics that include velocity, acceleration, force, and momentum in ergonomic risk assessment. This review suggests that using motion capture technologies with kinetic and kinematic variables, such as velocity, acceleration, and force, can help avoid inconsistency and develop more reliable results in ergonomic risk assessment. Most studies related to the physical measurement conducted with motion capture prefer to use non‐op-tical motion capture because it is a low‐cost system and simple experimental setup. However, the present review reveals that optical motion capture can provide more accurate data. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Ergonomic risk assessment; Kinematic variables; Kinetic variables; Motion capture","Biomechanical Phenomena; Ergonomics; Humans; Musculoskeletal Diseases; Occupational Diseases; Risk Assessment; computer simulation; muscle; qualitative analysis; quantitative analysis; risk assessment; biomechanics; ergonomics; human; musculoskeletal disease; occupational disease; risk assessment","","","","","Division of Research & Innovation Universiti Sains Malaysia; Division of Research & Innovation Uni‐ versiti Sains Malaysia","Funding text 1: Authors gratefully acknowledge the Division of Research & Innovation Universiti Sains Malaysia, to provide financial support for completing this study.; Funding text 2: Acknowledgments: Authors gratefully acknowledge the Division of Research & Innovation Uni‐ versiti Sains Malaysia, to provide financial support for completing this study.","Delice E.K., Can G.F., A new approach for ergonomic risk assessment integrating KEMIRA, best worst and MCDM methods, Soft Comput, 24, pp. 15093-15110, (2020); Samaei S.E., Tirgar A., Khanjani N., Mostafaee M., Hosseinabadi M.B., Effect of personal risk factors on the prevalence rate of musculoskeletal disorders among workers of an Iranian rubber factory, Work, 57, pp. 547-553, (2017); Penkala S., El-Debal H., Coxon K., Work‐related musculoskeletal problems related to laboratory training in university medical science students: A cross sectional survey, BMC Public Health, 18, (2018); Alvarez D., Alvarez J.C., Gonzalez R.C., Lopez A.M., Upper limb joint angle measurement in occupational health, Comput. Methods Biomech. Biomed. Eng, 19, pp. 159-170, (2016); Antwi-Afari M.F., Li H., Edwards D.J., Parn E.A., Seo J., Wong AY, Biomechanical analysis of risk factors for work‐related musculoskeletal disorders during repetitive lifting task in construction workers, Autom. Constr, 83, pp. 41-47, (2017); Hossain M.D., Aftab A., Hassan M., Imam A., Mahmud I., Chowdhury I.A., Kabir R.I., Sarker M., Prevalence of work-related musculoskeletal disorders (WMSDs) and ergonomic risk assessment among readymade garment workers of Bangla-desh: A cross sectional study, PLoS ONE, 13, (2018); Enez K., Nalbantoglu S.S., Comparison of ergonomic risk assessment outputs from OWAS and REBA in forestry timber har-vesting, Int. J. Ind. Ergon, 70, pp. 51-57, (2019); Isusi I., Work‐Related Musculoskeletal Disorders—Facts and Figures (Syntesis of 10 National Reports), pp. 1-80, (2020); Pal A., Dhara P., Evaluation of Work‐Related Musculoskeletal Disorders and Postural Stress of Female “Jari” Workers, Indian J. Occup. Environ. Med, 21, (2017); Fletcher S.R., Johnson T.L., Thrower J., A study to trial the use of inertial non‐optical motion capture for ergonomic analysis of manufacturing work, Proc. Inst. Mech. Eng. Part B J. Eng. Manuf, 232, pp. 90-98, (2018); Merino G., da Silva L., Mattos D., Guimaraes B., Merino E., Ergonomic evaluation of the musculoskeletal risks in a banana harvesting activity through qualitative and quantitative measures, with emphasis on motion capture (Xsens) and EMG, Int. J. Ind. Ergon, 69, pp. 80-89, (2019); Petrosyan T., Dunoyan A., Mkrtchyan H., Application of Motion Capture Systems in Ergonomic Analysis, Armen. J. Spec. Educ, 1, pp. 107-117, (2020); Schmitz A., Ye M., Boggess G., Shapiro R., Yang R., Noehren B., The measurement of in vivo joint angles during a squat using a single camera markerless motion capture system as compared to a marker‐based system, Gait Posture, 41, pp. 694-698, (2015); Bortolini M., Faccio M., Gamberi M., Pilati F., Motion Analysis System (MAS) for production and ergonomics assessment in the manufacturing processes, Comput. Ind. Eng, 139, (2020); Akhavian R., Behzadan A.H., Smartphone‐based construction workers’ activity recognition and classification, Autom. Constr, 71, pp. 198-209, (2016); Ide D., Tokcalar O., Gunduz T., The effect of joint forces and torques on speed variation in automobile assembly lines, Work, 61, pp. 211-224, (2018); Abaeian H., Moselhi O., Al-hussein M., System Dynamics Model Application for Ergonomic Assessment of Manual Material Handling Tasks, (2016); Bortolini M., Gamberi M., Pilati F., Regattieri A., Automatic assessment of the ergonomic risk for manual manufacturing and assembly activities through optical motion capture technology, Procedia CIRP, 72, pp. 81-86, (2018); Kaharuddin M.Z., Khairu Razak S.B., Kushairi M.I., Abd Rahman M.S., An W.C., Ngali Z., Siswanto W.A., Salleh S.M., Yusup E.M., Biomechanics Analysis of Combat Sport (Silat) by Using Motion Capture System, IOP Conf. Ser. Mater. Sci. Eng, 166, (2017); Ray P.K., Parida R., Sarkar S., Ergonomic Analysis of Construction Jobs in India: A Biomechanical Modelling Approach, Pro-cedia Manuf, 3, pp. 4606-4612, (2015); Agethen P., Otto M., Mengel S., Rukzio E., Using Marker‐less Motion Capture Systems for Walk Path Analysis in Paced Assembly Flow Lines, Procedia CIRP, 54, pp. 152-157, (2016); Kim J.H., Hwang J., Jung M., Mo S., The Kinematic Evaluation of Shoulder and Elbow Joints for Different Walking Speeds, Int. J. Eng. Technol, 11, pp. 169-172, (2019); Soh A.A.S.A., Jafri M.Z., Azraai N.Z., Power estimation of martial arts movement with different physical, mood, and behavior using motion capture camera, Optics for Arts, Architecture, and Archaeology VI, 10331, (2017); Maurice P., Malaise A., Amiot C., Paris N., Richard G.J., Rochel O., Ivaldi S., Human movement and ergonomics: An indus-try‐oriented dataset for collaborative robotics, Int. J. Robot. Res, 38, pp. 1529-1537, (2019); Eldar R., Fisher-Gewirtzman D., Ergonomic design visualization mapping‐developing an assistive model for design activities, Int. J. Ind. Ergon, 74, (2019); Antwi-Afari M.F., Li H., Luo X.E., Edwards D.J., Owusu-Manu D., Darko A., Overexertion‐related construction workers’ activity recognition and ergonomic risk assessment based on wearable insole pressure system, Proceedings of the West Africa Built Environment Research Conference, pp. 788-796; Scalise L., Paone N., Pressure sensor matrix for indirect measurement of grip and push forces exerted on a handle, Measure-ment: J. Int. Meas. Confed, 73, pp. 419-428, (2015); Nath N.D., Akhavian R., Behzadan A.H., Ergonomic analysis of construction worker’s body postures using wearable mobile sensors, Appl. Ergon, 62, pp. 107-117, (2017); Jahanbanifar S., Akhavian R., Evaluation of wearable sensors to quantify construction workers muscle force: An ergonomic analysis, Proceedings of the Winter Simulation Conference, pp. 3921-3929; Waddell M.L., Amazeen E.L., Lift speed moderates the effects of muscle activity on perceived heaviness, Q. J. Exp. Psychol, 71, pp. 2174-2185, (2016); Fleron M.K., Ubbesen N.C.H., Battistella F., Dejtiar D.L., Oliveira A.S., Accuracy between optical and inertial motion capture systems for assessing trunk speed during preferred gait and transition periods, Sports Biomech, 18, pp. 366-377, (2019); White S.C., Hostler D., The effect of firefighter protective garments, self‐contained breathing apparatus and exertion in the heat on postural sway, Ergonomics, 60, pp. 1137-1145, (2017); Nelson-Wong E., Gallagher K., Johnson E., Antonioli C., Ferguson A., Harris S., Johnson H., Miller J.B., Increasing standing tolerance in office workers with standing‐induced back pain, Ergonomics, 63, pp. 804-817, (2020); Jun D., Johnston V., McPhail S.M., O'Leary S., Are Measures of Postural Behavior Using Motion Sensors in Seated Office Workers Reliable?, Hum. Factors, 61, pp. 1141-1161, (2019); Aurand A.M., Dufour J.S., Marras W.S., Accuracy map of an optical motion capture system with 42 or 21 cameras in a large measurement volume, J. Biomech, 58, pp. 237-240, (2017); Arendra A., Akhmad S., Lumintu I., Working tool redesign to reduce ergonomic risk of salt evaporation field workers based on RULA and REBA assessments using esMOCA Instrument, J. Phys. Conf. Ser, 1477, (2020); Dhyani M., Roll S.C., Gilbertson M.W., Orlowski M., Anvari A., Li Q., Anthony B., Samir A.E., A pilot study to precisely quantify forces applied by sonographers while scanning: A step toward reducing ergonomic injury, Work, 58, pp. 241-247, (2017); Yu K., Barmaki R., Unberath M., Mears A., Brey J., Hwan T., Chung M.D., Navab N., On the Accuracy of Low‐Cost Motion Capture Systems for Range of Motion Measurements, (2018); Patrizi A., Pennestri E., Valentini P.P., Comparison between low‐cost marker‐less and high‐end marker‐based motion capture systems for the computer‐aided assessment of working ergonomics, Ergonomics, 59, pp. 155-162, (2016); Perrott M.A., Pizzari T., Cook J., McClelland J.A., Comparison of lower limb and trunk kinematics between markerless and marker‐based motion capture systems, Gait Posture, 52, pp. 57-61, (2017); Karatsidis A., Bellusci G., Schepers H.M., de Zee M., Andersen M.S., Veltink P.H., Estimation of ground reaction forces and moments during gait using only inertial motion capture, Sensors, 17, (2017); Humadi A., Nazarahari M., Ahmad R., Rouhani H., In‐field instrumented ergonomic risk assessment: Inertial measurement units versus Kinect V2, Int. J. Ind. Ergon, 84, (2021); Brunner O., Mertens A., Nitsch V., Brandl C., Accuracy of a Markerless Motion Capture System for Postural Ergonomic Risk Assessment in Occupational Practice, Int. J. Occup. Saf. Ergon, (2021); Huang C., Kim W., Zhang Y., Xiong S., Development and Validation of a Wearable Inertial Sensors‐Based Automated System for Assessing Work‐Related Musculoskeletal Disorders in the Workspace, Int. J. Environ. Res. Public Health, 17, (2020); Abedi M., Ghanbary A., Habibi E., Palyzban F., Ghasemi H., Hasani A.A., Back Compressive Force (BCF) assessment using UTAH method in manual handling tasks among workers of a chemical manufacturing company, J. Occup. Health Epidemiol, 7, pp. 222-226, (2018); Kamat S.R., Zula N.M., Rayme N.S., Shamsuddin S., Husain K., The ergonomics body posture on repetitive and heavy lifting activities of workers in aerospace manufacturing warehouse, IOP Conf. Ser. Mater. Sci. Eng, 210, (2017); Namnik N., Negahban H., Salehi R., Shafizadeh R., Tabib M.S., Validity and reliability of Persian version of the Specific Nordic questionnaire in Iranian industrial workers, Work, 54, pp. 35-41, (2016); Kahraman T., Genc A., Goz E., The Nordic Musculoskeletal Questionnaire: Cross‐cultural adaptation into Turkish assessing its psychometric properties, Disabil. Rehabil, 38, pp. 2153-2160, (2016); Cremasco M.M., Giustetto A., Caffaro F., Colantoni A., Cavallo E., Grigolato S., Risk assessment for musculoskeletal disorders in forestry: A comparison between RULA and REBA in the manual feeding of a wood‐chipper, Int. J. Environ. Res. Public Health, 16, (2019); Bidiawati J.R., Suryani E., Improving the Work Position of Worker’s Based on Quick Exposure Check Method to Reduce the Risk of Work‐Related Musculoskeletal Disorders, Procedia Manuf, 4, pp. 496-503, (2015); Gomez-Galan M., Callejon-Ferre A.J., Perez-Alonso J., Diaz-Perez M., Carrillo-Castrillo J.A., Musculoskeletal risks: RULA bibliometric review, Int. J. Environ. Res. Public Health, 17, (2020); Hita-Gutierrez M., Gomez-Galan M., Diaz-Perez M., Callejon-Ferre A.J., An overview of reba method applications in the world, Int. J. Environ. Res. Public Health, 17, (2020); Abd Rahman M.K., Shahriman A.B., Desa H., Daud R., Razlan Z.M., Wan K., Cheng E.M., Afendi M., Comparative Study of Rapid Upper Limb Assessment (RULA) and Rapid Entire Body Assessment (REBA) between Conventional and Machine Assisted Napier Grass Harvest Works, Appl. Mech. Mater, 786, pp. 275-280, (2015); Plantard P., Auvinet E., Le Pierres A.S., Multon F., Pose estimation with a kinect for ergonomic studies: Evaluation of the accuracy using a virtual mannequin, Sensors, 15, pp. 1785-1803, (2015); Lowe B.D., Dempsey P.G., Jones E.M., Ergonomics assessment methods used by ergonomics professionals, Appl. Ergon, 81, (2019); Zare M., Biau S., Brunet R., Roquelaure Y., Comparison of three methods for evaluation of work postures in a truck assembly plant, Ergonomics, 60, pp. 1551-1563, (2017); Kim W., Sung J., Saakes D., Huang C., Xiong S., Ergonomic postural assessment using a new open‐source human pose estimation technology (OpenPose), Int. J. Ind. Ergon, 84, (2021); Ansari N.A., Sheikh D.M.J., Evaluation of work Posture by RULA and REBA: A Case Study, IOSR J. Mech. Civ. Eng, 11, pp. 18-23, (2014); Rizkya I., Syahputri K., Sari R.M., Siregar I., Evaluation of work posture and quantification of fatigue by Rapid Entire Body Assessment (REBA), IOP Conf. Ser. Mater. Sci. Eng, 309, (2018); Choi K.H., Kim D.M., Cho M.U., Park C.W., Kim S.Y., Kim M.J., Kong Y.K., Application of aula risk assessment tool by comparison with other ergonomic risk assessment tools, Int. J. Environ. Res. Public Health, 17, (2020); Jones T., Kumar S., Comparison of ergonomic risk assessment output in four sawmill jobs, Int. J. Occup. Saf. Ergon, 16, pp. 105-111, (2010); Kim T., Xiong S., Comparison of seven fall risk assessment tools in community‐dwelling Korean older women, Ergonomics, 60, pp. 421-429, (2017); Valero E., Sivanathan A., Bosche F., Abdel-Wahab M., Musculoskeletal disorders in construction: A review and a novel system for activity tracking with body area network, Appl. Ergon, 54, pp. 120-130, (2016); Lee W., Seto E., Lin K.Y., Migliaccio G.C., An evaluation of wearable sensors and their placements for analyzing construction worker’s trunk posture in laboratory conditions, Appl. Ergon, 65, pp. 424-436, (2017); Plantard P., Shum H.P., Le Pierres A.S., Multon F., Validation of an ergonomic assessment method using Kinect data in real workplace conditions, Appl. Ergon, 65, pp. 562-569, (2017); Brandl C., Mertens A., Schlick C.M., Effect of sampling interval on the reliability of ergonomic analysis using the Ovako working posture analysing system (OWAS), Int. J. Ind. Ergon, 57, pp. 68-73, (2017); Soh A.A., Jafri M.Z., Azraai N.Z., Study of human body: Kinematics and kinetics of a martial arts (Silat) performers using 3D-motion capture, AIP Conf. Proc, 1657, (2015); Swaminathan R., Williams J.M., Jones M.D., Theobald P.S., The prediction of neck extensor force using surface electromyog-raphy, J. Back Musculoskelet. Rehabil, 29, pp. 279-285, (2016); Caputo F., Amato E.D., Spada S., Sessa F., Losardo M., Upper Body Motion Tracking System with Inertial Sensors for Ergonomic Issues in Industrial Environments, Adv. Phys. Ergon. Hum. Factors, 489, pp. 801-812, (2016); Lavender S.A., Sommerich C.M., Bigelow S., Weston E.B., Seagren K., Pay N.A., Sillars D., Ramachandran V., Sun C., Xu Y., Et al., A biomechanical evaluation of potential ergonomic solutions for use by firefighter and EMS providers when lifting heavy patients in their homes, Appl. Ergon, 82, (2020); Dickerson C.R., Alenabi T., Martin B.J., Chaffin D.B., Shoulder muscular activity in individuals with low back pain and spinal cord injury during seated manual load transfer tasks, Ergonomics, 61, pp. 1094-1101, (2018); Greenland K.O., Merryweather A.S., Bloswick D.S., The effect of lifting speed on cumulative and peak biomechanical loading for symmetric lifting tasks, Saf. Health Work, 4, pp. 105-110, (2013); Yoon J., Shiekhzadeh A., Nordin M., The effect of load weight vs. pace on muscle recruitment during lifting, Appl. Ergon, 43, pp. 1044-1050, (2012); Wang T.J., A study on utilizing 3D motion‐capture analysis to assist in Chinese opera teaching, Res. Danc. Educ, (2020); Chen J., Qiu J., Ahn C., Construction worker’s awkward posture recognition through supervised motion tensor decomposition, Autom. Constr, 77, pp. 67-81, (2017); Wardoyo S., Hutajulu P.T., Togibasa O., A Development of Force Plate for Biomechanics Analysis of Standing and Walking, J. Phys. Conf. Ser, 739, (2016)","M.H. Jaafar; School of Industrial Technology, Universiti Sains Malaysia, USM, Penang, 11800, Malaysia; email: mhafiidz@usm.my; M.S. Hossain; School of Industrial Technology, Universiti Sains Malaysia, USM, Penang, 11800, Malaysia; email: sohrab@usm.my","","MDPI AG","","","","","","16617827","","","34444087","English","Int. J. Environ. Res. Public Health","Review","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85111943890"
"Pearson S.J.; Ritchings T.; Mohamed A.S.A.","Pearson, Stephen J. (7201386436); Ritchings, Tim (15045654400); Mohamed, Azlan S.A. (57190968285)","7201386436; 15045654400; 57190968285","Regional strain variations in the human patellar tendon","2014","Medicine and Science in Sports and Exercise","46","7","","1343","1351","8","17","10.1249/MSS.0000000000000247","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84902549535&doi=10.1249%2fMSS.0000000000000247&partnerID=40&md5=c076f3f0f1e4433c4a398e1a429848a7","Centre for Health, Sport and Rehabilitation Sciences Research, University of Salford, Manchester M66PU, United Kingdom; Control and Systems Engineering Research Centre, University of Salford, Greater Manchester, United Kingdom","Pearson S.J., Centre for Health, Sport and Rehabilitation Sciences Research, University of Salford, Manchester M66PU, United Kingdom; Ritchings T., Control and Systems Engineering Research Centre, University of Salford, Greater Manchester, United Kingdom; Mohamed A.S.A., Control and Systems Engineering Research Centre, University of Salford, Greater Manchester, United Kingdom","Purpose: Characteristics of localized tendon strain in vivo are largely unknown. The present study examines local tendon strain between the deep, middle, and surface structures at the proximal and distal aspects of the patellar tendon during ramped isometric contractions. Methods: Male subjects (age 28.0 ± 6.3 yr) were examined for patellar tendon excursion (anterior, midsection, and posterior) during ramped isometric voluntary contractions using real-time B-mode ultrasonography and dynamometry. Regional tendon excursion measurements were compared using an automated pixel tracking method. Strain was determined from the tendon delta length normalized to initial/resting segment length. Results: Strain increased from 10% to 100% of force for all regions. Significantly greater mean strain was seen for the anterior proximal region compared to the posterior and mid layer of the tendon (7.5% ± 1.1% vs 3.7% ± 0.5% vs 5.5% ± 1.0%; P < 0.05). Similarly, the distal posterior region showed greater mean strain compared to the mid and anterior regions (7.9% ± 0.6% vs 5.0% ± 0.6% vs 5.4% ± 0.6%; P < 0.05). Relative changes in strain differences from 50% to 100% of force for the proximal region were greatest for the anterior to midline regions (4.6% ± 0.6% and 5.6% ± 0.6%, respectively) and those for the distal region were also greatest for the anterior to midline regions (4.4% ± 0.2% and 5.3% ± 0.2%, respectively). The largest mean strain for the proximal region was at the anterior layer (7.5% ± 1.1%) and that for the distal tendon region was at the posterior layer (7.9% ± 0.9%). Conclusions: This study shows significant regional differences in strain during ramped isometric contractions for the patellar tendon. Lower proximal strains in the posterior tendon compared to the anterior region may be associated with the suggestion of ""stress shielding"" as an etiological factor in insertional tendinopathy. © 2014 by the American College of Sports Medicine.","In Vivo; regional structural properties; tendon; ultrasound","Adult; Biomechanical Phenomena; Humans; Isometric Contraction; Male; Muscle Strength Dynamometer; Patellar Ligament; Risk Factors; Stress, Mechanical; Young Adult; adult; biomechanics; dynamometer; human; male; mechanical stress; muscle isometric contraction; patella ligament; physiology; risk factor; young adult","","","","","","","Almekinders L.C., Vellema J.H., Weinhold P.S., Strain patterns in the patellar tendon and the implications for patellar tendinopathy, Knee Surg Sports Traumatol Arthrosc., 10, 1, pp. 2-5, (2002); Arndt A., Bengtsson A.S., Peolsson M., Thorstensson A., Movin T., Non-uniform displacement within the Achilles tendon during passive ankle joint motion, Knee Surg Sports Traumatol Arthrosc., pp. 1868-1874, (2011); Arndt A., Bruggemann G.P., Koebke J., Segesser B., Asymmetrical loading of the human triceps surae: I. Mediolateral force differences in the Achilles tendon, Foot Ankle Int., 20, pp. 444-449, (1999); Basso O., Amis A.A., Race A., Johnson D.P., Patellar tendon fiber strains: Their differential responses to quadriceps tension, Clin Orthop Relat Res., 400, pp. 246-253, (2002); Bojsen-Moller J., Hansen P., Aagaard P., Svantsson U., Kjaer M., Magnusson P., Differential displacement of the human soleus and medial gastrocnemius aponeuroses during isometric plantar flexion contractions in vivo, J. Appl Physiol., 97, pp. 1908-1914, (2004); Carolan B., Cafarelli E., Adaptations in coactivation after isometric resistance training, J Appl Physiol., 73, 3, pp. 911-917, (1992); Carroll C.C., Dickinson J.M., Haus J.M., Et al., Influence of aging on the in vivo properties of human patellar tendon, J Appl Physiol., 105, 6, pp. 1907-1915, (2008); Child S., Bryant A.L., Clark R.A., Crossley K.M., Mechanical properties of the Achilles tendon aponeurosis are altered in athletes with Achilles tendinopathy, Am J Sports Med., 9, 38, pp. 1885-1893, (2010); Couppe C., Kongsgaard M., Aagaard P., Et al., Habitual loading results in tendon hypertrophy and increased stiffness of the human patellar tendon, J Appl Physiol., 105, 3, pp. 805-810, (2008); Dilley A., Greening J., Lynn B., Leary R., Morris V., The use of cross-correlation analysis between high-frequency ultrasound images to measure longitudinal median nerve movement, Ultrasound Med Biol., 27, 9, pp. 1211-1218, (2001); Farron J., Varghese T., Thelen D.G., Measurement of tendon strain during muscle twitch contractions using ultrasound elastography, IEEE Trans Ultrason Ferroelectr Freq Control., 56, 1, pp. 27-35, (2009); Hansen P., Bojsen-Moller J., Aagaard P., Kjaer M., Magnusson S.P., Mechanical properties of the human patellar tendon, in vivo, Clin Biomech (Bristol, Avon)., 21, 1, pp. 54-58, (2006); Hansen P., Haraldsson B.T., Aagaard P., Et al., Lower strength of the human posterior patellar tendon seems unrelated to mature collagen cross-linking and fibril morphology, J Appl Physiol., 108, 1, pp. 47-52, (2010); Haraldsson B.T., Aagaard P., Krogsgaard M., Alkjaer T., Kjaer M., Magnusson S.P., Region-specific mechanical properties of the human patella tendon, J Appl Physiol., 98, 3, pp. 1006-1012, (2005); Hermens H.J., Fredriks B., Disselhorst-Klug C., Rau G., Development of recommendations for SEMG sensors and sensor placement procedures, J Electromyogr Kinesiol., 10, 5, pp. 361-374, (2000); Kim Y.S., Kim J.M., Bigliani L.U., Kim H.J., Jung H.W., In vivo strain analysis of the intact supraspinatus tendon by ultrasound speckles tracking imaging, J Orthop Res., 29, 12, pp. 1931-1937, (2011); Kongsgaard M., Reitelseder S., Pedersen T.G., Et al., Region specific patellar tendon hypertrophy in humans following resistance training, Acta Physiol (Oxf)., 191, 2, pp. 111-121, (2007); Korstanje J.W., Selles R.W., Stam H.J., Hovius S.E., Bosch J.G., Development and validation of ultrasound speckle tracking to quantify tendon displacement, J Biomech., 43, 7, pp. 1373-1379, (2010); Kovanen V., Suominen H., Age and training-related changes in the collagen metabolism of rat skeletal muscle, Eur J Appl Physiol Occup Physiol., 58, 7, pp. 765-771, (1989); Krevolin J.L., Pandy M.G., Pearce J.C., Moment arm of the patellar tendon in the human knee, J Biomech., 37, 5, pp. 785-788, (2004); Kubo K., Ikebukuro T., Maki A., Yata H., Tsunoda N., Time course of changes in the human Achilles tendon properties and metabolism during training and detraining in vivo, Eur J Appl Physiol., 112, 7, pp. 2679-2691, (2012); Leadbetter W.B., Cell-matrix response in tendon injury, Clin Sports Med., 11, 3, pp. 533-578, (1992); Lersch C., Grotsch A., Segesser B., Koebke J., Bruggemann G.P., Potthast W., Influence of calcaneus angle and muscle forces on strain distribution in the human Achilles tendon, Clin Biomech (Bristol, Avon), 27, 9, pp. 955-961, (2012); Lippold O.C., The relationship between integrated action potentials in a human muscle and its isometric tension, J Physiol., 177, pp. 492-499, (1952); Loram I.D., Maganaris C.N., Lakie M., Use of ultrasound to make noninvasive in vivo measurement of continuous changes in human muscle contractile length, J Appl Physiol., 100, 4, pp. 1311-1323, (2006); Maganaris C.N., Narici M.V., Almekinders L.C., Maffulli N., Biomechanics and pathophysiology of overuse tendon injuries: Ideas on insertional tendinopathy, Sports Med., 34, 14, pp. 1005-1017, (2004); Magnusson S.P., Kjaer M., Region-specific differences in Achilles tendon cross-sectional area in runners and non-runners, Eur J Appl Physiol., 90, pp. 549-553, (2003); Miller B.F., Olesen J.L., Hansen M., Dlssing S., Crameri R.M., Welling R.J., Coordinated collagen and muscle protein synthesis in human patella tendon and quadriceps muscle after exercise, J Physiol., 15, 567, pp. 1021-1033, (2005); Ofer N., Akselrod S., Nyska M., Werner M., Glaser E., Shabat S., Motion-based tendon diagnosis using sequence processing of ultrasound images, J Orthop Res., 22, 6, pp. 1296-1302, (2004); Onambele G.N., Burgess K., Pearson S.J., Gender-specific in vivo measurement of the structural and mechanical properties of the human patellar tendon, J Orthop Res., 25, 12, pp. 1635-1642, (2007); Pearson S.J., Burgess K., Onambele G.N., Creep and the in vivo assessment of human patellar tendon mechanical properties, Clin Biomech (Bristol, Avon), 22, 6, pp. 712-717, (2007); Pearson S.J., Onambele G.N., Influence of time of day on tendon compliance and estimations of voluntary activation levels, Muscle Nerve, 33, 6, pp. 792-800, (2006); Pearson S.J., Ritchings T., Mohamed A.S., The use of normalized crosscorrelation analysis for automatic tendon excursion measurement in dynamic ultrasound imaging, J Appl Biomech., 29, 2, pp. 165-173, (2013); Revell J., Mirmehdi M., McNally D., Computer vision elastography: Speckle adaptive motion estimation for elastography using ultrasound sequences, IEEE Trans Med Imaging., 24, 6, pp. 755-766, (2005)","S.J. Pearson; Centre for Health, Sport and Rehabilitation Sciences Research, University of Salford, Manchester M66PU, United Kingdom; email: s.pearson@salford.ac.uk","","Lippincott Williams and Wilkins","","","","","","01959131","","MSCSB","24389512","English","Med. Sci. Sports Exerc.","Article","Final","All Open Access; Bronze Open Access; Green Open Access","Scopus","2-s2.0-84902549535"
"Chin D.J.Y.; Mohamed A.S.A.; Shariff K.A.; Ishikawa K.","Chin, Daniel Jie Yuan (57355532800); Mohamed, Ahmad Sufril Azlan (57190968285); Shariff, Khairul Anuar (57190617705); Ishikawa, Kunio (7404554763)","57355532800; 57190968285; 57190617705; 7404554763","GPU-Accelerated Enhanced Marching Cubes 33 for Fast 3D Reconstruction of Large Bone Defect CT Images","2021","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","13051 LNCS","","","374","384","10","0","10.1007/978-3-030-90235-3_33","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120534381&doi=10.1007%2f978-3-030-90235-3_33&partnerID=40&md5=5036d5f0960ce69ad704d1f87c39fec0","School of Computer Sciences, Universiti Sains Malaysia, Gelugor, Penang, 11800, Malaysia; School of Materials and Mineral Resources Engineering, Universiti Sains Malaysia, Nibong Tebal, Penang, 14300, Malaysia; Dental Materials Science and Technology Division, Faculty of Dental Medicine, Airlangga University, JI. Prof. Dr. Moestopo No. 47, Surabaya, East Java, 60132, Indonesia; Department of Biomaterials, Faculty of Dental Science, Kyushu University, 3-1-1 Maidashi, Higashi-ku, Fukuoka, 812-8582, Japan","Chin D.J.Y., School of Computer Sciences, Universiti Sains Malaysia, Gelugor, Penang, 11800, Malaysia; Mohamed A.S.A., School of Computer Sciences, Universiti Sains Malaysia, Gelugor, Penang, 11800, Malaysia; Shariff K.A., School of Materials and Mineral Resources Engineering, Universiti Sains Malaysia, Nibong Tebal, Penang, 14300, Malaysia, Dental Materials Science and Technology Division, Faculty of Dental Medicine, Airlangga University, JI. Prof. Dr. Moestopo No. 47, Surabaya, East Java, 60132, Indonesia; Ishikawa K., Department of Biomaterials, Faculty of Dental Science, Kyushu University, 3-1-1 Maidashi, Higashi-ku, Fukuoka, 812-8582, Japan","With the advancement in three-dimensional technologies, three-dimensional reconstruction of medical images serves as reliable assistance for doctors and surgeons in evaluating and diagnosing bone defects. Amongst the existing reconstruction methods, the Marching Cubes algorithm is highly popular in the surface rendering research study. There are many improvements made over the Marching Cubes algorithm, but due to the relatively small image datasets used during evaluation, it is difficult to judge the effectiveness of the improvements on large image datasets and the reconstructed models may not be viewable in lower-end specs digital devices like tablets and smartphones. Thus, an enhancement over the extended Marching Cubes 33 with graphics processing unit acceleration to improve the reconstruction accuracy, execution time, and model portability for large image datasets is proposed in this study. The obtained results show that the proposed enhancement successfully increased the accuracy by 5.29%, decreased the execution time by 11.16%, and decreased the number of vertices and faces by 73.72%. This shows that it is possible to view bone defect models with a high similarity percentage on lower-end spec digital devices and print them out with a three-dimensional printer. © 2021, Springer Nature Switzerland AG.","Enhanced marching cubes 33; Fast 3D reconstruction; Large CT images","3D printers; Computerized tomography; Diagnosis; Geometry; Graphics processing unit; Image enhancement; Image reconstruction; Large dataset; Medical imaging; Program processors; 3D reconstruction; Bone defect; CT Image; Enhanced marching cube 33; Fast 3d reconstruction; Image datasets; Large CT image; Large images; Marching cube; Marching Cubes algorithm; Digital devices","","","","","Japan International Cooperation Agency, JICA, (304/PBA-HAN/6050449/A119); Universiti Sains Malaysia","Acknowledgement. The authors are grateful to Universiti Sains Malaysia and ASEAN University Network/Southeast Asia Engineering Education Development Network (AUN/SEED-Net) Japan International Cooperation Agency (JICA) project for supporting this documented work through Special Program for Research Against COVID-19 (SPRAC) grant [304/PBA-HAN/6050449/A119].","Alasal S.A., Alsmirat M., Al-Mnayyis A., Baker Q.B., Al-Ayyoub M., Improving radiolo-gists’ and orthopedists’ QoE in diagnosing lumbar disk herniation using 3D modeling, Int. J. Electric. Comput. Eng., 11, 5, pp. 4336-4344, (2021); Chernyaev E.V., Marching cubes 33: Construction of topologically correct isosurfaces, GRAPHICON’95, Pp. 1–8. Technical Report CERN CN95–17, Saint-Petersburg, Russia, (1995); Custodio L., Pesco S., Silva C., An extended triangulation to the marching cubes 33 algorithm, J. Braz. Comput. Soc., 25, 6, pp. 1-18, (2019); Lorensen W., Cline H., Marching cubes: A high resolution 3D surface construction algorithm, SIGGRAPH’87 Proceedings of the 14Th Annual Conference on Computer Graphics and Interactive Techniques 1987, Vol. 21, No. 4, Pp. 163–169. ACM, NY, (1987); Al-Mnayyis A., Alasal S.A., Alsmirat M., Baker Q.B., Alzu'Bi S., Lumbar disk 3D modeling from limited number of MRI axial slices, Int. J. Electric. Comput. Eng., 10, 4, pp. 4101-4108, (2020); Garland M., Heckbert P.S., Surface simplification using quadric error metrics,  Annual Conference on Computer Graphics and Interactive Techniques 1997, pp. 209-216, (1997)","A.S.A. Mohamed; School of Computer Sciences, Universiti Sains Malaysia, Gelugor, Penang, 11800, Malaysia; email: sufril@usm.my","Badioze Zaman H.; Smeaton A.F.; Shih T.K.; Velastin S.; Terutoshi T.; Jørgensen B.N.; Aris H.; Ibrahim N.","Springer Science and Business Media Deutschland GmbH","","7th International Conference on Advances in Visual Informatics, IVIC 2021","23 November 2021 through 25 November 2021","Kajang","268729","03029743","978-303090234-6","","","English","Lect. Notes Comput. Sci.","Conference paper","Final","","Scopus","2-s2.0-85120534381"
"Bello R.W.; Mohamed A.S.A.; Talib A.Z.; Olubummo D.A.; Enuma O.C.","Bello, R.W. (57209469141); Mohamed, A.S.A. (57190968285); Talib, A.Z. (35570816900); Olubummo, D.A. (57216345013); Enuma, O.C. (57221132880)","57209469141; 57190968285; 35570816900; 57216345013; 57221132880","Computer vision-based techniques for cow object recognition","2021","IOP Conference Series: Earth and Environmental Science","858","1","012008","","","","2","10.1088/1755-1315/858/1/012008","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117929218&doi=10.1088%2f1755-1315%2f858%2f1%2f012008&partnerID=40&md5=2501fd5e1f0e5891ca80a4e9d5bdb29d","School of Computer Sciences, Universiti Sains Malaysia, Pulau Pinang, 11800, Malaysia; Department of Mathematics/Computer Science, University of Africa, Bayelsa, Toru-Orua, Nigeria; Department of Computer and Information Systems, Robert Morris University, Moon-Township, PA, United States; Department of Planning, Research and Statistics, Ministry of Health, Bayelsa State, Nigeria","Bello R.W., School of Computer Sciences, Universiti Sains Malaysia, Pulau Pinang, 11800, Malaysia, Department of Mathematics/Computer Science, University of Africa, Bayelsa, Toru-Orua, Nigeria; Mohamed A.S.A., School of Computer Sciences, Universiti Sains Malaysia, Pulau Pinang, 11800, Malaysia; Talib A.Z., School of Computer Sciences, Universiti Sains Malaysia, Pulau Pinang, 11800, Malaysia; Olubummo D.A., Department of Computer and Information Systems, Robert Morris University, Moon-Township, PA, United States; Enuma O.C., Department of Planning, Research and Statistics, Ministry of Health, Bayelsa State, Nigeria","The productivity of livestock farming depends on the welfare of the livestock. This can be achieved by physically and constantly monitoring their behaviors and activities by human experts. However, the degree of having high accuracy and consistency with manual monitoring in a commercial farm is herculean, and in most cases impractical. Hence, there is a need for a method that can overcome the challenges. Proposed in this paper, therefore, is the cow detection and monitoring method using computer vision techniques. The proposed method is capable of tracking and identifying cow objects in video experiments, thereby actualizing precision livestock farming. The method generates reasonable results when compared to other methods. © Published under licence by IOP Publishing Ltd.","","","","","","","","","Sharma B, Koundal D, Cattle health monitoring system using wireless sensor network: A survey from innovation perspective, IET Wireless Sensor Syst, 8, pp. 143-151, (2018); Halachmi I, Guarino M, Bewley J, Pastell M, Smart animal agriculture: Application of real-time sensors to improve animal well-being and production, Annual Review Animal Biosci, 7, pp. 403-425, (2019); Chen C S, Chen W C, Research and development of automatic monitoring system for livestock farms, Appl. Sci, 9, pp. 1-16, (2019); Muller M, Bibi A, Giancola S, Alsubaihi S, Ghanem B, Proceedings of the European Conference on Computer Vision Trackingnet: A large-scale dataset and benchmark for object tracking in the wild, pp. 300-317, (2018); Bello R W, Talib A Z H, Mohamed A S A B, Deep belief network approach for recognition of cow using cow nose image pattern, Walailak J. Sci. Technol, 18, pp. 1-14, (2021); Bello R W, Talib A Z, Mohamed AS A, Deep learning-based architectures for recognition of cow using cow nose image pattern, Gazi Univer. J. Sci, 33, pp. 831-844, (2020); Qiao N, Yu J X, Proceedings of the 33rd Chinese Control Conference On particle filter and mean shift tracking algorithm based on multi-feature fusion 4712-5 IEEE, (2014); Hou Z Q, Liu X, Yu W S, Li W, Huang A Q, 2014 Fifth Intern. Conf Intell. Syst. Design Engin. Appl. Mean-shift tracking algorithm with improved background-weighted histogram, pp. 597-602, (2014); Zhang J, Sun H, Guan W, Wang J, Xie Y, Shang B, 2010 Fifth Intern. Conf. Front. Comp. Sci. Technol. Robust human tracking algorithm applied for occlusion handling, pp. 546-551, (2010); Ju M Y, Ouyang C S, Chang H S, 2010 International Conf. Machine Learning and Cybernetics Mean shift tracking using fuzzy color histogram 2904-08 IEEE, (2010); Bello R W, Mohamed A S A, Talib A Z, Real-time cow detection and identification using enhanced particle filter, IOP Conf. Series: Materials Sci. Engin, 1051, pp. 1-8, (2021); Bello R W, Olubummo D A, Seiyaboh Z, Enuma O C, Talib A Z, Mohamed A S A, Cattle identification: the history of nose prints approach in brief, In IOP Conf. Series: Earth. Environ. Sci, 594, pp. 1-9, (2020); Yang Y X, Gao Y, Zhang X Y, Visual tracking for soccer robot based on adaptive kalman filter, 2, pp. 27-30, (2010); Kim G, Kim H, Park J, Yu Y, International Conf. Inform. Security Assurance Vehicle tracking based on kalman filter in tunnel, pp. 250-256, (2011); Orderud F, Proc. Scandinavian Conf. Simulation and Modelling Comparison of kalman filter estimation approaches for state space models with nonlinear measurements, pp. 1-8, (2005); Sayankar B B, Rangaree P H, Giripunje S D, 2010 3rd International Conf. Emerging Trends Engin. Technol. Filtering of images by kalman filter using vhdl, pp. 741-744, (2010); Iswanto I A, Li B, Visual object tracking based on mean-shift and particle-Kalman filter, Procedia Comp. Sci, 116, pp. 587-595, (2017); Wang L, Li L, Kong D, 2013 International Conf. Comp. Inform. Sci. An improved particle filter tracking algorithm based on motion and appearance features, pp. 110-113, (2013); Maggio E, Cavallaro A, Proceed, IEEE International Conf. Acoustics, Speech, and Signal Processing Hybrid particle filter and mean shift tracker with adaptive transition model ii/221-ii/224, (2005)","R.W. Bello; School of Computer Sciences, Universiti Sains Malaysia, Pulau Pinang, 11800, Malaysia; email: sirbrw@yahoo.com","","IOP Publishing Ltd","","7th International Conference on Agricultural and Biological Sciences, ABS 2021","9 August 2021 through 11 August 2021","Virtual, Online","172481","17551307","","","","English","IOP Conf. Ser. Earth Environ. Sci.","Conference paper","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85117929218"
"Bello R.-W.; Talib A.Z.; Mohamed A.S.A.; Olubummo D.A.; Otobo F.N.","Bello, Rotimi-Williams (57209469141); Talib, Abdullah Zawawi (35570816900); Mohamed, Ahmad Sufril Azlan (57190968285); Olubummo, Daniel A. (57216345013); Otobo, Firstman Noah (57216337593)","57209469141; 35570816900; 57190968285; 57216345013; 57216337593","Image-based individual cow recognition using body patterns","2020","International Journal of Advanced Computer Science and Applications","11","3","","92","98","6","18","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083241430&partnerID=40&md5=98fd3a91ca93346abd229678bad5b879","School of Computer Sciences, Universiti Sains Malaysia, Pulau, Pinang, 11800, Malaysia; Department of Computer and Information Systems, Robert Morris University, Moon-Township, PA, United States; Department of Mathematical Sciences, University of Africa, Toru-Orua, Bayelsa State, Nigeria","Bello R.-W., School of Computer Sciences, Universiti Sains Malaysia, Pulau, Pinang, 11800, Malaysia; Talib A.Z., School of Computer Sciences, Universiti Sains Malaysia, Pulau, Pinang, 11800, Malaysia; Mohamed A.S.A., School of Computer Sciences, Universiti Sains Malaysia, Pulau, Pinang, 11800, Malaysia; Olubummo D.A., Department of Computer and Information Systems, Robert Morris University, Moon-Township, PA, United States; Otobo F.N., Department of Mathematical Sciences, University of Africa, Toru-Orua, Bayelsa State, Nigeria","The existence of illumination variation, non-rigid object, occlusion, non-linear motion, and real-time implementation requirement has made tracking in computer vision a challenging task. In order to recognize individual cow and to mitigate all the challenging tasks, an image processing system is proposed using the body pattern images of the cow. This system accepts an input image, performs processing operation on the image, and output results in form of classification under certain categories. Technically, convolutional neural network is modeled for the training and testing of each pattern image of 1000 acquired images of 10 species of cow which will pass it through a series of convolution layers with filters, pooling, fully connected layers and softmax function for the pattern images classification with probabilistic values between 0 and 1. The performance evaluation of the proposed system for both training and testing data was carried out for each cow's identification and 92.59% and 89.95% accuracies were achieved respectively. © 2020, Science and Information Organization.","Body patterns; Convolutional neural network; Cow; Image; Recognition","Convolution; Convolutional neural networks; Image classification; Image recognition; Multilayer neural networks; Body pattern; Convolutional neural network; Cow; Illumination variation; Image; Image-based; Non-rigid objects; Pattern images; Recognition; Training and testing; Real time control","","","","","Institute of Postgraduate Studies, Universiti Sains Malaysia, IPS, (11800)","This work was supported in part by the Institute of Postgraduate Studies, Universiti Sains Malaysia, 11800 USM, Penang, MALAYSIA.","Miao Z., Gaynor K.M., Wang J., Liu Z., Muellerklein O., Norouzzadeh M.S., Getz W.M., Insights and approaches using deep learning to classify wildlife, Scientific Reports, 9, 1, pp. 1-9, (2019); Kumar S., Singh S.K., Cattle Recognition: A New Frontier in Visual Animal Biometrics Research, Proceedings of the National Academy of Sciences, India Section A: Physical Sciences, pp. 1-20, (2019); Zin T.T., Phyo C.N., Tin P., Hama H., Kobayashi I., Image technology based cow identification system using deep learning, Proceedings of the International MultiConference of Engineers and Computer Scientists, (2018); Bello R.W., Moradeyo O.M., Monitoring Cattle Grazing Behavior and Intrusion Using Global Positioning System and Virtual Fencing, Asian Journal of Mathematical Sciences, 3, 4, pp. 4-14, (2019); Bello R.W., An overview of animal behavioral adaptive frightening system, International Journal of Mathematics and Physical Sciences Research, 6, 1, pp. 126-133, (2018); Barron U.G., Butler F., McDonnell K., Ward S., The end of the identity crisis? Advances in biometric markers for animal identification, Irish Veterinary J, 62, 3, pp. 204-208, (2009); Shen M., Liu L., Yan L., Lu M., Yao W., Yang X., Review of monitoring technology for animal individual in animal husbandry, Nongye Jixie Xuebao = Transactions of the Chinese Society For Agricultural Machinery, 45, 10, pp. 245-251, (2014); Bello R.W., Abubakar S., Development of a Software Package for Cattle Identification in Nigeria, Journal of Applied Sciences and Environmental Management, 23, 10, pp. 1825-1828, (2019); Bello R.W., Talib A.Z.H., Mohamed A.S.A.B., A Framework for Real-time Cattle Monitoring using Multimedia Networks, International Journal of Recent Technology and Engineering, 8, 5, (2020); Grooms D., Radio Frequency Identification (RFID) Technology For Cattle"", Extension Bulletin E-2970, (2007); Lu Y., He X., Wen Y., Wang P.S., A new cow identification system based on iris analysis and recognition, International Journal of Biometrics, 6, 1, pp. 18-32, (2014); Kumar S., Tiwari S., Singh S.K., Face recognition of cattle: Can it be done?, Proceedings of the National Academy of Sciences, India Section A: Physical Sciences, 86, 2, pp. 137-148, (2016); Kumar S., Tiwari S., Singh S.K., Face recognition for cattle, Third IEEE International Conference On Image Information Processing (ICIIP), pp. 65-72, (2015); Marchant J., Secure animal identification and source verification, JM Communications, UK, pp. 1-28, (2002); Allen A., Golden B., Taylor M., Patterson D., Henriksen D., Skuce R., Evaluation of retinal imaging technology for the biometric identification of bovine animals in northern Ireland, Livest Sci, 116, 1, pp. 42-52, (2008); Baranov A., Graml R., Pirchner F., Schmid D., Breed differences and intra-breed genetic variability of dermatoglyphic pattern of cattle, J Anim Breed Genet, 110, 1-6, pp. 385-392, (1993); Johnston A., Edwards D., Welfare implications of identification of cattle by ear tags, The Veterinary Record, 138, 25, pp. 612-614, (1996); Wardrope D.D., Problems with the use of ear tags in cattle, The Veterinary Record, 137, 26, pp. 675-675, (1995); Wang Z., Fu Z., Chen W., Hu J., A RFID-based traceability system for cattle breeding in china, Proceedings of 2010 International Conference On Computer Application and System Modeling (ICCASM 2010), 2, pp. V2-567, (2010); Noviyanto A., Arymurthy A.M., Beef cattle identification based on muzzle pattern using a matching refinement technique in the sift method, Comput Electron Agric, 99, pp. 77-84, (2013); Petersen W.E., The identification of the bovine by means of nose-prints, Journal of Dairy Science, 5, 3, pp. 249-258, (1922); Minagawa H., Fujimura T., Ichiyanagi M., Tanaka K., Fangquan M., Identification of beef cattle by analyzing images of their muzzle patterns lifted on paper"", Proceedings of the 3rd Asian Conference for Information Technology in Asian agricultural information technology & management, Publications of the Japanese Society of Agricultural Informatics, 8, pp. 596-600, (2002); Tharwat A., Gaber T., Hassanien A.E., Cattle identification based on muzzle images using gabor features and svm classifier, Proceedings of Advanced Machine Learning Technologies and Applications, pp. 236-247, (2014); Mishra S., Tomer O.S., Kalm E., Muzzle dermatoglyphics: A new method to identify bovines, Asian Livestock, pp. 91-96, (1995); Barry B., Gonzales-Barron U., McDonnell K., Butler F., Ward S., Using muzzle pattern recognition as a biometric approach for cattle identification, Trans ASABE, 50, 3, pp. 1073-1080, (2007); Kim H.T., Ikeda Y., Choi H.L., The identification of Japanese black cattle by their faces, Asian Australasian Journal of Animal Sciences, 18, 6, pp. 868-872, (2005); Cai C., Li J., Cattle face recognition using local binary pattern descriptor, Proceedings of 2013 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA), pp. 1-4, (2013); Noviyanto A., Arymurthy A.M., Automatic cattle identification based on muzzle photo using speed-up robust features approach, Proceedings of the 3rd European Conference of Computer Science, ECCS, 110, (2012); Awad A.I., Zawbaa H.M., Mahmoud H.A., Nabi E.H.H.A., Fayed R.H., Hassanien A.E., A robust cattle identification scheme using muzzle print images, Proceedings of 2013 Federated Conference On Computer Science and Information Systems (FedCSIS), pp. 529-534, (2013); Eitel A., Springenberg J.T., Spinello L., Riedmiller M., Burgard W., Multimodal deep learning for robust RGB-D object recognition, 2015 IEEE/RSJ International Conference On Intelligent Robots and Systems (IROS), pp. 681-687, (2015); Russakovsky O., Deng J., Su H., Krause J., Satheesh S., Ma S., Berg A.C., Imagenet large scale visual recognition challenge, International Journal of Computer Vision, 115, 3, pp. 211-252, (2015); Krizhevsky A., Sutskever I., Hinton G.E., Imagenet classification with deep convolutional neural networks, Advances In Neural Information Processing Systems, pp. 1097-1105, (2012); Schwarz M., Schulz H., Behnke S., RGB-D object recognition and pose estimation based on pre-trained convolutional neural network features, 2015 IEEE International Conference On Robotics and Automation (ICRA), pp. 1329-1335, (2015); Jingqiu G., Zhihai W., Ronghua G., Huarui W., Cow behavior recognition based on image analysis and activities, International Journal of Agricultural and Biological Engineering, 10, 3, pp. 165-174, (2017); Andrew W., Greatwood C., Burghardt T., Visual localisation and individual identification of Holstein friesian cattle via deep learning, Proceedings of the IEEE International Conference On Computer Vision, pp. 2850-2859, (2017); Kumar S., Pandey A., Satwik K.S.R., Kumar S., Singh S.K., Singh A.K., Mohan A., Deep learning framework for recognition of cattle using muzzle point image pattern, Measurement, 116, pp. 1-17, (2018); Risha K.P., Chempak K.A., Sindhu C.S., Difference of Gaussian on Frame Differenced Image, International Journal of Innovative Research In Electrical, Electronics, Instrumentation and Control Engineering, 3, 1, pp. 92-95, (2016); Norouzzadeh M.S., Nguyen A., Kosmala M., Swanson A., Palmer M.S., Packer C., Clune J., Automatically identifying, counting, and describing wild animals in camera-trap images with deep learning, Proceedings of the National Academy of Sciences, 115, 25, (2018); Vincent P., Larochelle H., Lajoie I., Bengio Y., Manzagol P.A., Stacked denoising auto-encoders: Learning useful representations in a deep network with a local denoising criterion, Journal of Machine Learning Research, 11, pp. 3371-3408, (2010); Kamencay P., Trnovszky T., Benco M., Hudec R., Sykora P., Satnik A., Accurate wild animal recognition using PCA, LDA and LBPH, 2016 ELEKTRO, pp. 62-67, (2016)","","","Science and Information Organization","","","","","","2158107X","","","","English","Intl. J. Adv.  Comput. Sci. Appl.","Article","Final","","Scopus","2-s2.0-85083241430"
"Pearson S.J.; Mohammed A.S.A.; Hussain S.R.","Pearson, Stephen J. (7201386436); Mohammed, Azlan S.A. (57190968285); Hussain, Syed R. (55860068900)","7201386436; 57190968285; 55860068900","Patellar tendon in vivo regional strain with varying knee angle","2017","Journal of Biomechanics","61","","","45","50","5","15","10.1016/j.jbiomech.2017.06.038","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025440777&doi=10.1016%2fj.jbiomech.2017.06.038&partnerID=40&md5=71454dea2c414daae602708766ce8dbb","Centre for Health, Sport and Rehabilitation Sciences Research, University of Salford, Greater Manchester, United Kingdom; School of Computer Sciences, Universiti Sains Malaysia (USM), 11800, Penang, Malaysia","Pearson S.J., Centre for Health, Sport and Rehabilitation Sciences Research, University of Salford, Greater Manchester, United Kingdom; Mohammed A.S.A., School of Computer Sciences, Universiti Sains Malaysia (USM), 11800, Penang, Malaysia; Hussain S.R., Centre for Health, Sport and Rehabilitation Sciences Research, University of Salford, Greater Manchester, United Kingdom","Purpose Descriptive data on the aspects of site specific in vivo tendon strain with varying knee joint angle are non-existent. The present study determines and compares surface and deep layer strain of the patellar tendon during isometric contractions across a range of knee joint angles. Methods Male participants (age 22.0 ± 3.4) performed ramped isometric knee extensions at knee joint angles of 90°, 70°, 50° and 30° of flexion. Strain patterns of the anterior and posterior regions of the patellar tendon were determined using real-time B-mode ultrasonography at each knee joint angle. Regional strain measures were compared using an automated pixel tracking method. Results Strain was seen to be greatest for both the anterior and posterior regions with the knee at 90° (7.76 ± 0.89% and 5.06 ± 0.76%). Anterior strain was seen to be significantly greater (p < 0.05) than posterior strain for all knee angles apart from 30°, 90° = (7.76 vs. 5.06%), 70° = (4.77 vs. 3.75%), and 50° = (3.74 vs. 2.90%). The relative strain (ratio of anterior to posterior), was greatest with the knee joint angle at 90°, and decreased as the knee joint angle reduced. Conclusions The results from this study indicate that not only are there greater absolute tendon strains with the knee in greater flexion, but that the knee joint angle affects the regional strain differentially, resulting in greater shear between the tendon layers with force application when the knee is in greater degrees of flexion. These results have important implications for rehabilitation and training. © 2017","Isometric; Knee extension; Localised strain; Patella; Tendon","Biomechanical Phenomena; Humans; Isometric Contraction; Knee; Male; Patella; Range of Motion, Articular; Stress, Mechanical; Tendons; Young Adult; Joints (anatomy); Physiological models; Strain; Force application; Isometric; Isometric contractions; Knee extension; Knee-joint angle; Localised; Patella; Patellar Tendon; accident prevention; adult; algorithm; Article; cross-sectional study; echography; human; human experiment; in vivo study; knee function; knee joint angle; male; muscle isometric contraction; muscle strain; musculoskeletal system parameters; normal human; patellar ligament; priority journal; reliability; statistics; young adult; anatomy and histology; biomechanics; joint characteristics and functions; knee; mechanical stress; patella; physiology; tendon; Tendons","","","","","","","Aalbersberg S., Kingma I., Ronsky J.L., Frayne R., van Dieen H.J., Orientation of tendons in vivo with active and passive knee muscles, J. Biomech., 38, pp. 1780-1788, (2005); Almekinders L.C., Tendinitis and other chronic tendinopathies, J. Am. Acad. Orthop. Surg., 6, pp. 157-164, (1998); Almekinders L.C., Vellema J.H., Weinhold P.S., Strain patterns in the patellar tendon and the implications for patellar tendinopathy, Knee Surg Sports Traumatol Arthrosc., 10, 1, (2002); Arndt A., Bengtsson A.S., Peolsson M., Thorstensson A., Movin T., Non-uniform displacement within the Achilles tendon during passive ankle joint motion, Knee Surg Sports Traumatol Arthrosc., 20, 9, pp. 1868-1874, (2012); Astrom M., Rausing A., Chronic Achilles tendinopathy: a survey of surgical and histopathological findings, Clin. Orthop., 316, pp. 151-164, (1995); Basso O., Amis A.A., Race A., Johnson D.P., Patellar tendon fiber strains: their differential responses to quadriceps tension, Clin. Orthop. Relat. Res., 400, pp. 246-253, (2002); Carolan B., Cafarelli E., Adaptations in coactivation after isometric resistance training, (1992); DeFrate L.E., Wook Nha K., Papannagari R., Moses J.M., Gill T.J., Li G., The Biomechanical function of the patellar tendon during in-vivo weight-bearing flexion, J. Biomech., 40, 8, pp. 1716-1722, (2007); Dillon E.M., Erasmus P.J., Muller J.H., Scheffer C., de Villiers R.V., Differential forces within the proximal patellar tendon as an explanation for the characteristic lesion of patellar tendinopathy: an in vivo descriptive experimental study, Am. J. Sports Med., 36, 11, pp. 2119-2127, (2008); Hansen P., Bojsen-Moller J., Aagaard P., Kjaer M., Magnusson S.P., Mechanical properties of the human patellar tendon, in vivo, Clin Biomech (Bristol, Avon), 21, 1, pp. 54-58, (2006); Hansen P., Haraldsson B.T., Aagaard P., Kovanen V., Avery N.C., Qvortrup K., Larsen J.O., Krogsgaard M., Kjaer M., Magnusson S.P., Lower strength of the human posterior patellar tendon seems unrelated to mature collagen cross-linking and fibril morphology, J. Appl. Physiol., 108, 1, pp. 47-52, (2010); Haraldsson B.T., Aagaard P., Krogsgaard M., Alkjaer T., Kjaer M., Magnusson S.P., Region-specific mechanical properties of the human patella tendon, J. Appl. Physiol., 98, 3, pp. 1006-1012, (2010); Hermens H.J., Freriks B., Merletti R., Hagg G., Stegeman D., Blok J.; Herring S.A., Nilson K.L., Introduction to overuse injuries, Clin. Sports Med., 6, pp. 225-239, (1987); Ilfeld F.W., Can stroke modification relieve tennis elbow?, Clin. Orthop., 276, pp. 182-186, (1992); James S.L., Running injuries to the knee, J. Am. Acad. Orthop. Surg., 3, pp. 309-318, (1995); Kader D., Saxena A., Movin T., Maffulli N., Achilles tendinopathy: some aspects of basic science and clinical management, Br. J. Sports Med., 36, pp. 239-249, (2002); Kibler W.B., Chandler T.J., Pace B.K., Principles of rehabilitation after chronic tendon injuries, Clin. Sports Med., 11, pp. 661-671, (1992); Korkia P.K., Tunstall-Pedoe D.S., Mafulli N., An epidemiologic investigation of training and injury patterns in British triathletes, Br. J. Sports Med., 28, pp. 191-196, (1994); Krevolin J.L., Pandy M.G., Pearce J.C., Moment arm of the patellar tendon in the human knee, J. Biomech., 37, 5, pp. 785-788, (2004); Leadbetter W.B., Cell-matrix response in tendon injury, Clin. Sports Med., 11, pp. 533-578, (1992); Lersch C., Grotsch A., Segesser B., Koebke J., Bruggemann G.P., Potthast W., Influence of calcaneus angle and muscle forces on strain distribution in the human Achilles tendon, Clin. Biomech. (Bristol, Avon), 27, 9, pp. 955-961, (2012); Lippold O.C., The relationship between integrated action potentials in a human muscle and its isometric tension, J. Physiol., 177, pp. 492-499, (1952); Maffulli N., Kader D., Tendinopathy of tendo Achillis, J. Bone Joint Surg. Br., 84, pp. 1-8, (2002); Malliaras P., Kamal B., Nowell A., Farley T., Dhamu H., Simpson V., Morrissey D., Langberg H., Maffulli N., Reeves N.D., Patellar tendon adaptation in relation to load-intensity and contraction type, J. Biomech., 46, pp. 1893-1899, (2013); Orchard J.W., Cook J.L., Halpin N., Stress-shielding as a cause of insertional tendinopathy: the operative technique of limited adductor tenotomy supports this theory, J. Sci. Med. Sport., 7, 4, pp. 424-428, (2004); Pearson S.J., Onambele G.N., Influence of time of day on tendon compliance and estimations of voluntary activation levels, Muscle Nerve, 33, 6, pp. 792-800, (2006); Pearson S.J., Ritchings T., Mohamed A.S., Regional strain variations in the patellar tendon, Med. Sci. Sports Exerc., 46, 7, pp. 1343-1351, (2014); Regan W., Wold L.E., Coonrad R., Et al., Mircroscopic histopathology of chronic refractory lateral epicondylitis, Am. J. Sports Med., 20, pp. 746-749, (1992); Richards D.P., Ajemian S.V., Wiley J.P., Zernicke R.F., Knee joint dynamics predict patellar tendinitis in elite volleyball players, Am. J. Sports Med., 24, pp. 676-683, (1996); Riley G., Tendinopathy: from basic science to treatment, Nat. Clin. Pract. Rheumatol., 4, pp. 82-89, (2008); Rufai A., Ralphs J.R., Benjamin M., Structure and histopathology of the insertional region of the human Achilles tendon, J. Orthop. Res., 13, pp. 585-593, (1995); Tsaopoulos D.E., Baltzopoulos V., Maganaris C.N., Human patellar tendon moment arm length: measurement considerations and clinical implications for joint loading assessment, Clin. Biomech. (Bristol, Avon), 21, 7, pp. 657-667, (2006); van der Worp H., Zwerver J., Kuijer P.P., Et al., The impact of physically demanding work of basketball and volleyball players on the risk for patellar tendinopathy and on work limitations, Back Musculoskelet Rehabil., 24, 1, pp. 49-55, (2011); Visnes H., Tegnander A., Bahr R., Ultrasound characteristics of the patellar and quadriceps tendons among young elite athletes, Scand. J. Med. Sci. Sports, 24, (2014); Vogel K.G., Ordog A., Pogany G., Et al., Proteoglycans in the compressed region of the human tibialis posterior tendon and in ligaments, J. Orthop. Res., 11, pp. 68-77, (1993); Ward T.R., Pandit H., Hollinghurst D., Zavatsky A.B., Gill H.S., Thomas N.P., Murray D.W., A low-riding patella in posterior stabilised total knee replacements alters quadriceps' mechanical advantage, resulting in reduced knee flexion moments, Knee, 19, 4, pp. 299-305, (2012); Wearing S.C., Hooper S.L., Purdam C., Cook J., Grigg N., Locke S., Smeathers J.E., The acute transverse strain response of the patellar tendon to quadriceps exercise, Med. Sci. Sports Exerc., 45, 4, pp. 772-777, (2013); Zwerver J., Bredeweg S.W., van den Akker-Scheek I., Prevalence of Jumper's knee among nonelite athletes from different sports: a cross-sectional survey, Am. J. Sports Med., 39, 9, pp. 1984-1988, (2011)","S.J. Pearson; Centre for Health, Sport and Rehabilitation Sciences Research, University of Salford, Greater Manchester, United Kingdom; email: s.pearson@salford.ac.uk","","Elsevier Ltd","","","","","","00219290","","JBMCB","28736078","English","J. Biomech.","Article","Final","","Scopus","2-s2.0-85025440777"
"Mohamed A.S.A.; Ab Wahab M.N.; Suhaily S.S.; Arasu D.B.L.","Mohamed, A.S.A. (57190968285); Ab Wahab, M.N. (36471236100); Suhaily, S.S. (54384197200); Arasu, D.B.L. (57207817920)","57190968285; 36471236100; 54384197200; 57207817920","Smart mirror design powered by raspberry Pi","2018","ACM International Conference Proceeding Series","","","","166","173","7","8","10.1145/3299819.3299840","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062995332&doi=10.1145%2f3299819.3299840&partnerID=40&md5=a71f316475360fd59cb3bce7588ca710","School of Computer Sciences, Universiti Sains Malaysia, Penang, Malaysia; School of Arts, Universiti Sains Malaysia, Penang, Malaysia","Mohamed A.S.A., School of Computer Sciences, Universiti Sains Malaysia, Penang, Malaysia; Ab Wahab M.N., School of Computer Sciences, Universiti Sains Malaysia, Penang, Malaysia; Suhaily S.S., School of Arts, Universiti Sains Malaysia, Penang, Malaysia; Arasu D.B.L., School of Computer Sciences, Universiti Sains Malaysia, Penang, Malaysia","The smart mirror projects consisting of observable mirror, microcontroller, camera and PC monitor. Existing smart projects are limited with features available and only displaying information based on command receiving directly from the user. To make this mirror to be smarter, artificial intelligence are added in this project. Facial expression detection will be implemented so that smart mirror is able to interact with user and recognize changes of the facial muscle. By accordingly to their expression, smart mirror will make decision to display related information. Only recognized user can utilize this smart mirror via face recognition. At end of the project, a working smart mirror is expected and have ability to become one of these connected devices in our households. © 2018 Association for Computing Machinery.","Artificial Intelligence; Face Recognition; Facial Expressions; Smart Mirror","Artificial intelligence; Cloud computing; Mirrors; Facial expression detections; Facial Expressions; Facial muscles; Mirror design; PC monitors; Smart projects; Face recognition","","","","","","","Ross Beveridge B.J., Bolme D., Draper B.A., Teixeira M., The CSU face identification evaluation system, Machine Vision and Applications, 16, 2, pp. 128-138, (2005); Anwar Hossain M., Smart mirror for ambient home environment, IET Conference Proceedings, pp. 589-596, (2007); Jin K., Deng X., Huang Z., Chen S., Design of the smart mirror based on raspberry Pi, 2018 2nd IEEE Advanced Information Management, Communicates, Electronic and Automation Control Conference (IMCEC), pp. 1919-1923, (2018); Jose J., Chakravarthy R., Jacob J., Ali M.M., Dsouza S.M., Home automated smart mirror as an internet of things (IoT) implementation - Survey paper, International Journal of Advanced Research in Computer and Communication Engineering, 6, 2, pp. 126-128, (2017); Khanna V., Vardhan Y., Nair D., Pannu P., Design and development of a smart mirror using raspberry Pi, International Journal of Electrical, Electronics and Data Communication, 5, pp. 63-65, (2017); Lu J., Plataniotis K.N., Venetsanopoulos A.N., Regularized discriminant analysis for the small sample size problem in face recognition, Pattern Recogn. Lett., 24, 16, pp. 3079-3087, (2003); Ok F., Can M., Smart mirror applications with raspberry Pi, 2017 International Conference on Computer Science and Engineering (UBMK), pp. 94-98, (2017); Olivier S.L., Porterfield A.K., Wheeler K.B., Prins J.F., Scheduling task parallelism on multi-socket multicore systems, Proceedings of the 1st International Workshop on Runtime and Operating Systems for Supercomputers (ROSS'11), pp. 49-56, (2011); Sun Y., Geng L., Dan K., Design of smart mirror based on raspberry Pi, 2018 International Conference on Intelligent Transportation, Big Data Smart City (ICITBS), pp. 77-80, (2018); Yusri M.M., Kasim S., Hassan R., Abdullah Z., Ruslai H., Jahidin K., Arshad M.S., Smart mirror for smart life, 2017 6th ICT International Student Project Conference (ICT-ISPC), pp. 1-5, (2017); Zhao W., Chellappa R., Phillips P.J., Rosenfeld A., Face recognition: A literature survey, ACM Comput. Surv., 35, 4, pp. 399-458, (2003)","","","Association for Computing Machinery","","2018 International Conference on Artificial Intelligence and Cloud Computing, AICCC 2018","21 December 2018 through 23 December 2018","Tokyo","145904","","978-145036623-6","","","English","ACM Int. Conf. Proc. Ser.","Conference paper","Final","","Scopus","2-s2.0-85062995332"
"Azlan Mohamed A.S.; Soter E.B.; Singh A.; Raihana Ruhaiyem N.I.","Azlan Mohamed, Ahmad Sufril (57190968285); Soter, Effha Binti (57201704699); Singh, Anand (57201699924); Raihana Ruhaiyem, Nur Intan (57190964192)","57190968285; 57201704699; 57201699924; 57190964192","Gesture based help identification for hospital& elderlycare using dynamic time warping: A systematic study","2017","ACM International Conference Proceeding Series","","","","94","98","4","1","10.1145/3177404.3177426","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045831292&doi=10.1145%2f3177404.3177426&partnerID=40&md5=6809df7237e357161f2a64c9f73f4084","School of Computer Sciences, Universiti Sains Malaysia, Penang, Malaysia","Azlan Mohamed A.S., School of Computer Sciences, Universiti Sains Malaysia, Penang, Malaysia; Soter E.B., School of Computer Sciences, Universiti Sains Malaysia, Penang, Malaysia; Singh A., School of Computer Sciences, Universiti Sains Malaysia, Penang, Malaysia; Raihana Ruhaiyem N.I., School of Computer Sciences, Universiti Sains Malaysia, Penang, Malaysia","Recent advancement of depth imaging sensor technology can provide opportunities, especially in healthcare sector to improve the quality of life of the hospital patients and elderly care institutes by facilitating certain tasks without any assistant and enabling supportive ecosphere. This paper presents a novel hand gesture system via motion capture for appliance control in the hospital environment. A selection of hand gestures were prepared for the subject to perform with a set of questionnaires to learn the user acceptance towards this implementation with the adoption of dynamic time warping (DTW) algorithm for identifying the gestures. The results shown that gestures are easily performed by the patient to control specific tasks in a controlled environment. Most participants agreed that this system is easy to use and learn. The aim of this study is to evaluate the user experience on the proposed gesture setup and study the acceptance of the proposed hand gestures by the subjects. . © 2017 Association for Computing Machinery.","Dynamic Time Warping; Hand Gesture; Kinect; Recognition","Hospitals; Image processing; Palmprint recognition; Surveys; Video signal processing; Appliance controls; Controlled environment; Dynamic time warping; Dynamic time warping algorithms; Hand gesture; Hospital environment; Kinect; Recognition; Gesture recognition","","","","","Universiti Sains Malaysia, (304/PKOMP/6313259)","The authors wish to thank Universiti Sains Malaysia for the support it has extended in the completion of the present research through Short Term University Grant No: 304/PKOMP/6313259.","Ahuja M.K., Singh A., Static vision based hand gesture recognition using principal component analysis, IEEE 3rd International Conference on MOOCs. Innovation and Technology in Education (MITE), (2015); Ann O.C., Theng L.B., Human activity recognition: A review, IEEE International Conference on Control System, Computing and Engineering, pp. 389-393, (2014); Argoty J.A., Figueroa P., Design and development of a prototype of an interactive hospital room with kinect, Proceedings of The 15th International Conference on Human Interaction, (2014); Athavale S., Deshmukh M., Dynamic hand gesture recognition for human computer interaction: A comparative study, International J. of Engineering Research and General Science, 2, 2, pp. 38-55, (2014); Bharambe A., Chanekar D., Naik D., Vitekar A.B., Automatic hand gesture based remote control for home appliances, International J. of Advanced Research in Computer Science and Software Engineering, 5, 2, pp. 567-571, (2015); Bonechere B., Jansen B., Salvia P., Bouzahouene H., Omelina L., Cornelis J., Rooze M., Jan S.V.S., Can the Kinect sensors be used for motion analysis?, Trans. On Electrical and Electronic Circuits and Systems., 4, 1, pp. 1-6, (2014); Chen Z.H., Kim J.T., Liang J., Zhang J., Yuan Y.B., Real-time hand gesture recognition using finger segmentation, The Scientific World J, (2014); Dan R.B., Mohod P.S., Survey on hand gesture recognition approaches, International J. of Computer Science and Information Technologies, 5, 2, pp. 2050-2052, (2012); Ganzeboom M., How hand gestures are recognized using data glove, Human Media Interaction (HMI), (2010); Hernandez J., Mcduff D.J., Picard R.W., Bioinsights Extracting personal data from “Still” wearable motion sensors, Proceeding IEEE 12th International Conference Wearable Implantable Body Sensor Network, (2015); Jacob M.G., Li Y.T., Akingba G.A., Wachs, Collaboration with a robotic scrub nurse, Commun. ACM, 56, 5, pp. 68-75, (2013); Jyothilakshmi P., Rekha K.R., Natraj K.R., Implementation of a smart ward system in a hi-tech hospital by using a kinect sensor camera, International J. on Recent and Innovation Trends in Computing and Comun, 4, 5, pp. 531-536, (2016); Jyothilakshmi P., Rekha K.R., Natraj K.R., Patient assistance system in a super specialty hospital using a kinect sensor camera 2016, International Conference on Electrical, Electronics, and Optimization Techniques, (2016); Kelly P., Marshall S.J., Badland H., Kerr J., Oliver M., Doherthy A.R., Foster C., An ethical framework for automated, wearable cameras in health behavior research, American J. of Preventive Medicine., 44, pp. 314-319, (2013); Kumar P., Verma J., Prasad S., Hand data glove: A wearable real-time device for human computer interaction, International J. of Advanced Science and Technology., 43, pp. 15-26, (2012); Murthy G.R.S., Jadon R.S., A review of vision based hand gestures recognition, International J. of Information Technology and Knowledge Management, 12, 2, pp. 405-410, (2009); Ni B., Da N.C., Moulin P., RGBD-camera based get-up event detection for hospital fall prevention, Proceedings of International Conference on Acoustics, Speech, and Signal Process, pp. 1405-1408, (2000); Ren Z., Yuan J., Zhang Z., Robust hand gesture recognition based on finger-earth mover’s distance with a commodity depth camera, Proceedings of The 19thInternational Conference on Multimedia, pp. 1093-1096, (2011); Riemen R., The sensors behind the apple watch, Electrical Engineering Community, (2015); Rosli L., What happens when Malaysia enters ageing nation status in 2030?, New Straits Times Press (M) Berhad, (2016); Samad S.A., Mansor N., Population ageing and social protection in Malaysia, Malaysia J. of Economic Studies., pp. 139-156, (2013); Solanki U.V., Desai N., Hand gesture based remote control for home appliances: Handmote, World Congress on Information and Commun. Technologies., pp. 419-423, (2011); Sonkusare J.S., Chopade N.B., Sor R., Tade S.L., A review on hand gesture recognition system, International Conference on Computing Communication Control and Automation, (2015); Wachs J.P., Kolsch M., Stern H., Edan Y., Vision- Based hand gesture applications, ACM Commun, 54, 2, pp. 60-71, (2011); Weng C., Li Y., Zhang M., Guo K., Tang X., Pan Z., Robust hand posture recognition integrating multi-cue hand tracking, International Conference on Technologies for E-Learning and Digital Entertainment, 6249, pp. 497-508, (2010)","","","Association for Computing Machinery","Nanyang Technological University","2017 International Conference on Video and Image Processing, ICVIP 2017","27 December 2017 through 29 December 2017","Singapore","135440","","978-145035383-0","","","English","ACM Int. Conf. Proc. Ser.","Conference paper","Final","","Scopus","2-s2.0-85045831292"
"Arasu D.B.L.; Mohamed A.S.A.; Ruhaiyem N.I.R.; Annamalai N.; Lutfi S.L.; Al Qudah M.M.","Arasu, Darshan Babu L. (57207817920); Mohamed, Ahmad Sufril Azlan (57190968285); Ruhaiyem, Nur Intan Raihana (57190964192); Annamalai, Nagaletchimee (57127041900); Lutfi, Syaheerah Lebai (27567802400); Al Qudah, Mustafa M. (57222567501)","57207817920; 57190968285; 57190964192; 57127041900; 27567802400; 57222567501","Human Stress Recognition from Facial Thermal-Based Signature: A Literature Survey","2021","CMES - Computer Modeling in Engineering and Sciences","129","3","","1","20","19","3","10.32604/CMES.2021.016985","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121910570&doi=10.32604%2fCMES.2021.016985&partnerID=40&md5=045006c5bd2e601be7b64c5c25f4cd72","School of Computer Sciences, Universiti Sains Malaysia, Penang, 11800, Malaysia; School of Distance Education, Universiti Sains Malaysia, Penang, 11800, Malaysia","Arasu D.B.L., School of Computer Sciences, Universiti Sains Malaysia, Penang, 11800, Malaysia; Mohamed A.S.A., School of Computer Sciences, Universiti Sains Malaysia, Penang, 11800, Malaysia; Ruhaiyem N.I.R., School of Computer Sciences, Universiti Sains Malaysia, Penang, 11800, Malaysia; Annamalai N., School of Distance Education, Universiti Sains Malaysia, Penang, 11800, Malaysia; Lutfi S.L., School of Computer Sciences, Universiti Sains Malaysia, Penang, 11800, Malaysia; Al Qudah M.M., School of Computer Sciences, Universiti Sains Malaysia, Penang, 11800, Malaysia","Stress is a normal reaction of the human organism which triggered in situations that require a certain level of activation. This reaction has both positive and negative effects on everyone’s life. Therefore, stress management is of vital importance in maintaining the psychological balance of a person. Thermal-based imaging technique is becoming popular among researchers due to its non-contact conductive nature. Moreover, thermal-based imaging has shown promising results in detecting stress in a non-contact and non-invasive manner. Compared to other non-contact stress detection methods such as pupil dilation, keystroke behavior, social media interaction and voice modulation, thermal-based imaging provides better features with clear boundaries and requires no heavy methodology. This paper presented a brief review of previous work on thermal imaging related stress detection in humans. This paper also presented the stages of stress detection based on thermal face signatures such as dataset type, thermal image face detection, feature descriptors and classification performance comparisons are presented. This paper can help future researchers to understand stress detection based on thermal imaging by presenting the popular methods previous researchers use for stress detection based on thermal images. © 2021 Tech Science Press. All rights reserved.","Skin temperature; Stress recognition; Stress state; Thermal imaging; Thermal signature","Classification (of information); Face recognition; Feature extraction; Infrared imaging; Human stress; Non-contact; Skin temperatures; Stress detection; Stress recognition; Stress state; Thermal; Thermal images; Thermal signatures; Thermal-imaging; Stresses","","","","","Universiti Sains Malaysia, (1001/PKOMP/8014001)","Funding Statement: This research was pursued under the Research University Grant by Universiti Sains Malaysia [1001/PKOMP/8014001].","Selye H., Confusion and controversy in the stress field, Journal of Human Stress, 1, 2, pp. 37-44, (1975); Lederbogen F., Baranyai R., Gilles M., Menart-Houtermans B., Tschoepe D., Et al., Effect of mental and physical stress on platelet activation markers in depressed patients and healthy subjects: A pilot study, Psychiatry Research, 127, 1–2, pp. 55-64, (2004); Otto M., Physical stress and bacterial colonization, FEMS Microbiology Reviews, 38, 6, pp. 1250-1270, (2014); Tripathi R. K., Salve B. A., Petare A. U., Raut A. A., Rege N. N., Effect of withania somnifera on physical and cardiovascular performance induced by physical stress in healthy human volunteers, International Journal of Basic & Clinical Pharmacology, 5, pp. 2510-2516, (2016); Pardeshi A. M., Kirtikar S. N., Comparison of anthropometric parameters and blood pressure changes in response to physical stress test in normotensive subjects with or without family history of hypertension, Indian Journal of Physiology and Pharmacology, 60, 2, pp. 208-212, (2016); Oktedalen O., The infuence of prolonged physical stress on gastric juice components in healthy man, Scand J. Gastroenterol, 23, 9, pp. 1132-1136, (1988); Wallen N. H., Held C., Rehnqvist N., Hjemdahl P., Effects of mental and physical stress on platelet function in patients with stable angina pectoris and healthy controls, European Heart Journal, 18, 5, pp. 807-815, (1997); Trapp M., Trapp E. M., Egger J. W., Domej W., Schillaci G., Et al., Impact of mental and physical stress on blood pressure and pulse pressure under normobaric versus hypoxic conditions, PLoS One, 9, 5, (2014); Irfan M., Raja G. K., Murtaza S., Mansoor R., Qayyum M., Et al., Physical stress may result in growth suppression and pubertal delay in working boys, Journal of Medical Hypotheses and Ideas, 6, 1, pp. 35-39, (2012); Kim H. S., A study on the skin stress recognition and beauty care status due to wearing masks, Journal of the Korean Applied Science and Technology, 38, 2, pp. 465-475, (2021); Cohen S., Kamarck T., Mermelstein R., A global measure of perceived stress, Journal of Health and Social Behavior, 24, pp. 385-396, (1983); Dupere V., Dion E., Harkness K., McCabe J., Thouin E., Et al., Adaptation and validation of the life events and difficulties schedule for use with high school dropouts, Journal of Research on Adolescence, 27, 3, pp. 683-689, (2017); Gillan W., Naquin M., Zannis M., Bowers A., Brewer J., Et al., Correlations among stress, physical activity and nutrition: School employee health behavior, ICHPER-SD Journal of Research, 8, 1, pp. 55-60, (2013); Mizuno M., Siddique K., Baum M., Smith S. A., Prenatal programming of hypertension induces sympathetic overactivity in response to physical stress, Hypertension, 61, 1, pp. 180-186, (2013); Taylor A. H., Dorn L., Stress, fatigue, health, and risk of road traffic accidents among professional drivers: The contribution of physical inactivity, Annual Review of Public Health, 27, pp. 371-391, (2006); Jones B. F., A reappraisal of the use of infrared thermal image analysis in medicine, IEEE Transactions on Medical Imaging, 17, 6, pp. 1019-1027, (1998); Jones B. F., Plassmann P., Digital infrared thermal imaging of human skin, IEEE Engineering in Medicine and Biology Magazine, 21, 6, pp. 41-48, (2002); Khan M. M., Cluster-analytic classification of facial expressions using infrared measurements of facial thermal features, (2008); Fujimasa I., Chinzei T., Saito I., Converting far infrared image information to other physiological data, IEEE Engineering in Medicine and Biology Magazine, 19, 3, pp. 71-76, (2000); Bale M., High-resolution infrared technology for soft-tissue injury detection, IEEE Engineering in Medicine and Biology Magazine, 17, 4, pp. 56-59, (1998); Hessler C., Abouelenien M., Burzo M., A survey on extracting physiological measurements from thermal images, Proceedings of the 11th PErvasive Technologies Related to Assistive Environments Conference, pp. 229-236, (2018); Zhao W., Zhao Z., Li C., Discriminative-CCA promoted by EEG signals for physiological-based emotion recognition, 2018 First Asian Conference on Affective Computing and Intelligent Interaction (ACII Asia), pp. 1-6, (2018); Barclay C. J., Launikonis B. S., Components of activation heat in skeletal muscle, Journal of Muscle Research and Cell Motility, 42, 1, pp. 1-16, (2021); Youssef A., Verachtert A., de Bruyne G., Aerts J. M., Reverse engineering of thermoregulatory cold-induced vasoconstriction/Vasodilation during localized cooling, Applied Sciences, 9, 16, (2019); Khan M. M., Ward R. D., Ingleby M., Classifying pretended and evoked facial expressions of positive and negative affective states using infrared measurement of skin temperature, ACM Transactions on Applied Perception, 6, 1, pp. 1-22, (2009); Pavidis I., Eberhardt N. L., Levine J. A., Human behavior: Seeing through the face of deception [Brief communication], Nature, 425, (2002); Pavlidis I., Continuous physiological monitoring, Proceedings of the 25th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (IEEE Cat. No. 03CH37439), 2, pp. 1084-1087, (2003); Pavlidis I., Levine J., Baukol P., Thermal image analysis for anxiety detection, Proceedings 2001 International Conference on Image Processing (Cat. No. 01CH37205), 2, pp. 315-318, (2001); Pavlidis I., Dowdall J., Sun N., Puri C., Fei J., Et al., Interacting with human physiology, Computer Vision and Image Understanding, 108, 1–2, pp. 150-170, (2007); Pavlidis I., Tsiamyrtzis P., Shastri D., Wesley A., Zhou Y., Et al., Fast by nature-how stress patterns define human experience and performance in dexterous tasks, Scientific Reports, 2, 1, pp. 1-9, (2012); Ebisch S. J., Aureli T., Bafunno D., Cardone D., Romani G. L., Et al., Mother and child in synchrony: Thermal facial imprints of autonomic contagion, Biological Psychology, 89, 1, pp. 123-129, (2012); Ioannou S., Ebisch S., Aureli T., Bafunno D., Ioannides H. A., Et al., The autonomic signature of guilt in children: A thermal infrared imaging study, PLoS One, 8, 11, (2013); Hirt C., Eckard M., Kunz A., Stress generation and non-intrusive measurement in virtual environments using eye tracking, Journal of Ambient Intelligence and Humanized Computing, 11, 12, pp. 5977-5989, (2020); Yamanaka K., Kawakami M., Convenient evaluation of mental stress with pupil diameter, International Journal of Occupational Safety and Ergonomics, 15, 4, pp. 447-450, (2009); Gunawardhane S. D., de Silva P. M., Kulathunga D. S., Arunatileka S. M., Non invasive human stress detection using key stroke dynamics and pattern variations, 2013 International Conference on Advances in ICT for Emerging Regions (ICTer), pp. 240-247, (2013); Lin H., Jia J., Guo Q., Xue Y., Li Q., Et al., User-level psychological stress detection from social media using deep neural network, Proceedings of the 22nd ACM International Conference on Multimedia, pp. 507-516, (2014); Hansen J. H., Patil S., Speech under stress: Analysis, modeling and recognition, Speaker classification I, pp. 108-137, (2007); Han H., Byun K., Kang H. G., A deep learning-based stress detection algorithm with speech signal, Proceedings of the 2018 Workshop on Audio-Visual Scene Understanding for Immersive Multimedia, pp. 11-15, (2018); Hansen J. H., Womack B. D., Feature analysis and neural network-based classification of speech under stress, IEEE Transactions on Speech and Audio Processing, 4, 4, pp. 307-313, (1996); Giannakakis G., Pediaditis M., Manousos D., Kazantzaki E., Chiarugie F., Et al., Stress and anxiety detection using facial cues from videos, Biomedical Signal Processing and Control, 31, pp. 89-101, (2017); Zhang J., Mei X., Liu H., Yuan S., Qian T., Detecting negative emotional stress based on facial expression in real time, 2019 IEEE 4th International Conference on Signal and Image Processing (ICSIP), pp. 430-434, (2019); Gao H., Yuce A., Thiran J. P., Detecting emotional stress from facial expressions for driving safety, 2014 IEEE International Conference on Image Processing (ICIP), pp. 5961-5965, (2014); Zhai J., Barreto A., Stress detection in computer users based on digital signal processing of noninvasive physiological variables, 2006 International Conference of the IEEE Engineering in Medicine and Biology Society, pp. 1355-1358, (2006); Shi Y., Nguyen M. H., Blitz P., French B., Fisk S., Et al., Personalized stress detection from physiological measurements, International Symposium on Quality of Life Technology, pp. 28-29, (2010); Hong K., Non-contact physical stress measurement using thermal imaging and blind source separation, Optical Review, 27, 1, pp. 116-125, (2020); Adachi H., Oiwa K., Nozawa A., Drowsiness level modeling based on facial skin temperature distribution using a convolutional neural network, IEEJ Transactions on Electrical and Electronic Engineering, 14, 6, pp. 870-876, (2019); Oiwa K., Nozawa A., Feature extraction of blood pressure from facial skin temperature distribution using deep learning, IEEJ Transactions on Electronics, Information and Systems, 139, 7, pp. 759-765, (2019); Al Qudah M. M., Mohamed A. S., Lutfi S. L., Affective state recognition using thermal-based imaging: A survey, Computer Systems Science & Engineering, 37, 1, pp. 47-62, (2021); Cho Y., Bianchi-Berthouze N., Physiological and affective computing through thermal imaging: A survey, Physiological and Affective Computing through Thermal Imaging: A Survey, (2019); Cho Y., Automated mental stress recognition through mobile thermal imaging, 2017 Seventh International Conference on Affective Computing and Intelligent Interaction (ACII), pp. 596-600, (2017); Elanthendral V. S., Rekha R. K., Rameshkumar M., Thermal imaging for facial expression–Fatigue detection, International Journal for Research in Applied Science & Engineering Technology, 2, (2014); Chu C. H., Peng S. M., Implementation of face recognition for screen unlockingon mobile device, Proceedings of the 23rd ACM International Conference on Multimedia, pp. 1027-1030, (2015); Zhu Z., Tsiamyrtzis P., Pavlidis I., Forehead thermal signature extraction in lie detection, 2007 29th Annual International Conference of the IEEE Engineering in Medicine and Biology Society, IEEE, pp. 243-246, (2007); Hong K., Yuen P., Chen T., Tsitiridis A., Kam F., Et al., Detection and classification of stress using thermal imaging technique, Optics and photonics for counterterrorism and crime fighting V, 7486, (2009); Cross C. B., Skipper J. A., Petkie D. T., Thermal imaging to detect physiological indicators of stress in humans, Thermosense: Thermal infrared applications XXXV, 8705, (2013); Rajoub B. A., Zwiggelaar R., Thermal facial analysis for deception detection, IEEE Transactions on Information Forensics and Security, 9, 6, pp. 1015-1023, (2014); Jenkins S. D., Brown R. D. H., A correlational analysis of human cognitive activity using infrared thermography of the supraorbital region, frontal EEG and self-report of core affective state, Comunicación Presentada en la 12a Conferencia Internacional de Termografía de Infrarrojo Cuantitativa, (2014); Sorostinean M., Ferland F., Tapus A., Reliable stress measurement using face temperature variation with a thermal camera in human-robot interaction, IEEE-RAS 15th International Conference on Humanoid Robots (Humanoids), pp. 14-19, (2015); Mohd M. N. H., Kashima M., Sato K., Watanabe M., Mental stress recognition based on noninvasive and non-contact measurement from stereo thermal and visible sensors, International Journal of Affective Engineering, 14, 1, pp. 9-17, (2015); Abouelenien M., Burzo M., Mihalcea R., Human acute stress detection via integration of physiological signals and thermal imaging, Proceedings of the 9th ACM International Conference on Pervasive Technologies Related to Assistive Environments, pp. 1-8, (2016); Baltaci S., Gokcay D., Stress detection in human–computer interaction: Fusion of pupil dilation and facial temperature features, International Journal of Human–Computer Interaction, 32, 12, pp. 956-966, (2016); Hong K., Liu G., Facial thermal image analysis for stress detection, International Journal of Engineering Research and Technology, 6, 10, pp. 94-98, (2017); Abdelrahman Y., Velloso E., Dingler T., Schmidt A., Vetere F., Cognitive heat: Exploring the usage of thermal imaging to unobtrusively estimate cognitive load, Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies, 1, 3, pp. 1-20, (2017); Powar N. U., Schneider T. R., Skipper J. A., Petkie D. T., Asari V. K., Et al., Thermal facial signatures for state assessment during deception, Electronic Imaging, 2017, 13, pp. 95-104, (2017); Vasavi S., Neeharica P., Poojitha M., Harika T., Framework for stress detection using thermal signature, International Journal of Virtual and Augmented Reality, 2, 2, pp. 1-25, (2018); Vasavi S., Neeharica P., Wadhwa B., Regression modelling for stress detection in humans by assessing most prominent thermal signature, IEEE 9th Annual Information Technology, Electronics and Mobile Communication Conference, pp. 755-762, (2018); Kopaczka M., Jantos T., Merhof D., Towards analysis of mental stress using thermal infrared tomography, Bildverarbeitung für die Medizin, pp. 157-162, (2018); Stoynova A., Infrared thermography monitoring of the face skin temperature as indicator of the cognitive state of a person, 14th Quantitative InfraRed Thermography Conference, pp. 30-35, (2018); He C., Mahfouf M., Torres-Salomao L. A., Facial temperature markers for mental stress assessment in human-machine interface (HMI) control system, ICINCO, 2, 2, pp. 31-38, (2018); Derakhshan A., Mikaeili M., Gedeon T., Nasrabadi A. M., Identifying the optimal features in multimodal deception detection, Multimodal Technologies and Interaction, 4, 2, (2020); Panasiuk J., Prusaczyk P., Grudzien  A., Kowalski M., Study on facial thermal reactions for psychophysical stimuli, Metrology and Measurement Systems, 27, 3, pp. 399-415, (2020); Reshma R., Emotional and physical stress detection and classification using thermal imaging technique, Annals of the Romanian Society for Cell Biology, 25, pp. 8364-8374, (2021); Kumar S., Iftekhar A. S. M., Goebel M., Bullock T., MacLean M. H., Et al., Stressnet: Detecting stress in thermal videos, Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pp. 999-1009, (2021); Engert V., Merla A., Grant J. A., Cardone D., Tusche A., Et al., Exploring the use of thermal infrared imaging in human stress research, PLoS One, 9, 3, (2014); Zheng Y., Face detection and eyeglasses detection for thermal face recognition, Image processing: Machine vision applications V, 8300, (2012); Cutler R. G., Face recognition using infrared images and eigenfaces, (1996); Chen X., Flynn P. J., Bowyer K. W., PCA-Based face recognition in infrared imagery: Baseline and comparative studies, Proceedings of the IEEE International SOI Conference(Cat. No. 03CH37443), pp. 127-134, (2003); Srivastava A., Liu X., Statistical hypothesis pruning for identifying faces from infrared images, Image and Vision Computing, 21, 7, pp. 651-661, (2003); Buddharaju P., Pavlidis I. T., Tsiamyrtzis P., Pose-invariant physiological face recognition in the thermal infrared spectrum, Conference on Computer Vision and Pattern Recognition Workshop, pp. 53-53, (2006); Heo J., Kong S. G., Abidi B. R., Abidi M. A., Fusion of visual and thermal signatures with eyeglass removal for robust face recognition, Conference on Computer Vision and Pattern Recognition Workshop, pp. 122-122, (2004); Gyaourova A., Bebis G., Pavlidis I., Fusion of infrared and visible images for face recognition, European Conference on Computer Vision, pp. 456-468, (2004); Socolinsky D. A., Selinger A., Thermal face recognition in an operational scenario, Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2, (2004); Wang J. G., Sung E., Venkateswarlu R., Registration of infrared and visible-spectrum imagery for face recognition, Proceedings of the Sixth IEEE International Conference on Automatic Face and Gesture Recognition, Proceedings, pp. 638-644, (2004); Chen X., Flynn P. J., Bowyer K. W., IR and visible light face recognition, Computer Vision and Image Understanding, 99, 3, pp. 332-358, (2005); Kong S. G., Heo J., Abidi B. R., Paik J., Abidi M. A., Recent advances in visual and infrared face recognition—A review, Computer Vision and Image Understanding, 97, 1, pp. 103-135, (2005); Zheng Y., A novel thermal face recognition approach using face pattern words, Biometric technology for human identification VII, 7667, (2010); Basu A., Routray A., Shit S., Deb A. K., Human emotion recognition from facial thermal image based on fused statistical feature and multi-class SVM, 2015 Annual IEEE India Conference, pp. 1-5, (2015); Hu M. K., Visual pattern recognition by moment invariants, IRE Transactions on Information Theory, 8, 2, pp. 179-187, (1962); Mostafa E., Hammoud R., Ali A., Farag A., Face recognition in low resolution thermal images, Computer Vision and Image Understanding, 117, 12, pp. 1689-1694, (2013); Reese K., Zheng Y., Elmaghraby A., A comparison of face detection algorithms in visible and thermal spectrums, International Conference on Advances in Computer Science and Application, (2012); Viola P., Jones M. J., Robust real-time face detection, International Journal of Computer Vision, 57, 2, pp. 137-154, (2004); Basbrain A. M., Gan J. Q., Clark A., Accuracy enhancement of the viola-jones algorithm for thermal face detection, International Conference on Intelligent Computing, pp. 71-82, (2017); Tran H., Dong C., Naghedolfeizi M., Zeng X., Using cross-examples in viola-jones algorithm for thermal face detection, Proceedings of the 2021 ACM Southeast Conference, pp. 219-223, (2021); Kowalski M. L., Grudzien  A., Ciurapinski  W., Detection of human faces in thermal infrared images, Metrology and Measurement Systems, 28, 2, pp. 307-321, (2021); Buddharaju P., Pavlidis I. T., Tsiamyrtzis P., Bazakos M., Physiology-based face recognition in the thermal infrared spectrum, IEEE Transactions on Pattern Analysis and Machine Intelligence, 29, 4, pp. 613-626, (2007); Prokoski F. J., Riedel R. B., Infrared identification of faces and body parts, Biometrics, pp. 191-212, (1996); Cho S. Y., Wang L., Ong W. J., Thermal imprint feature analysis for face recognition, 2009 IEEE International Symposium on Industrial Electronics, pp. 1875-1880, (2009); Kopaczka M., Nestler J., Merhof D., Face detection in thermal infrared images: A comparison of algorithm-and machine-learning-based approaches, International Conference on Advanced Concepts for Intelligent Vision Systems, pp. 518-529, (2017); Friedrich G., Yeshurun Y., Seeing people in the dark: Face recognition in infrared images, International Workshop on Biologically Motivated Computer Vision, pp. 348-359, (2002); Kopaczka M., Schock J., Nestler J., Kielholz K., Merhof D., A combined modular system for face detection, head pose estimation, face tracking and emotion recognition in thermal infrared images, IEEE International Conference on Imaging Systems and Techniques, pp. 1-6, (2018); Kopaczka M., Acar K., Merhof D., Robust facial landmark detection and face tracking in thermal infrared images using active appearance models, VISIGRAPP (4: VISAPP), pp. 150-158, (2016); Antonakos E., Alabort-i-Medina J., Tzimiropoulos G., Zafeiriou S. P., Feature-based lucas–kanade and active appearance models, IEEE Transactions on Image Processing, 24, 9, pp. 2617-2632, (2015); Chu W. T., Liu Y. H., Thermal facial landmark detection by deep multi-task learning, IEEE 21st International Workshop on Multimedia Signal Processing, pp. 1-6, (2019); Sonkusare S., Ahmedt-Aristizabal D., Aburn M. J., Nguyen V. T., Pang T., Et al., Detecting changes in facial temperature induced by a sudden auditory stimulus based on deep learning-assisted face tracking, Scientific Reports, 9, 1, pp. 1-11, (2019); Cao Z., Hidalgo G., Simon T., Wei S. E., Sheikh Y., Openpose: Realtime multi-person 2D pose estimation using part affinity fields, IEEE Transactions on Pattern Analysis and Machine Intelligence, 43, 1, pp. 172-186, (2019); Kumar S., Singh S. K., Occluded thermal face recognition using bag of CNN ($ Bo $ CNN), IEEE Signal Processing Letters, 27, pp. 975-979, (2020); Wu Z., Peng M., Chen T., Thermal face recognition using convolutional neural network, International Conference on Optoelectronics and Image Processing, pp. 6-9, (2016); Simonyan K., Zisserman A., Very deep convolutional networks for large-scale image recognition, (2014); He K., Zhang X., Ren S., Sun J., Deep residual learning for image recognition, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 770-778, (2016); Muller M., Baier G., Rummel C., Schindler K., Stephani U., Et al., A multivariate approach to correlation analysis based on random matrix theory, Seizure prediction in epilepsy: From basic mechanisms to clinical applications, pp. 209-226, (2008); Berlovskaya E. E., Isaychev S. A., Chernorizov A. M., Ozheredov I. A., Adamovich T. V., Et al., Diagnosing human psychoemotional states by combining psychological and psychophysiological methods with measurements of infrared and THz radiation from face areas, Psychology in Russia: State of the Art, 13, 2, pp. 64-83, (2020); Kandus J. T., Using functional infrared thermal imaging to measure stress responses, (2018); Jacobs G. D., The physiology of mind–body interactions: The stress response and the relaxation response, The Journal of Alternative & Complementary Medicine, 7, 1, pp. 83-92, (2001); Garbey M., Sun N., Merla A., Pavlidis I., Contact-free measurement of cardiac pulse based on the analysis of thermal imagery, IEEE Transactions on Biomedical Engineering, 54, 8, pp. 1418-1426, (2007); Bara C. P., Papakostas M., Mihalcea R., A deep learning approach towards multimodal stress detection, AffCon@AAAI, CEUR Workshop Proceedings, pp. 67-81, (2020); Gupta S., Stress recognition from image features using deep learning, (2019); Irani R., Nasrollahi K., Dhall A., Moeslund T. B., Gedeon T., Thermal super-pixels for bimodal stress recognition, Sixth International Conference on Image Processing Theory, Tools and Applications, pp. 1-6, (2016)","A.S.A. Mohamed; School of Computer Sciences, Universiti Sains Malaysia, Penang, 11800, Malaysia; email: sufril@usm.my","","Tech Science Press","","","","","","15261492","","","","English","CMES Comput. Model. Eng. Sci.","Review","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85121910570"
"Pauzi A.S.B.; Mohd Nazri F.B.; Sani S.; Bataineh A.M.; Hisyam M.N.; Jaafar M.H.; Ab Wahab M.N.; Mohamed A.S.A.","Pauzi, Ainun Syarafana Binti (57361509800); Mohd Nazri, Firdaus Bin (57361509900); Sani, Salisu (57522765300); Bataineh, Ahmad Mwfaq (57361510000); Hisyam, Muhamad Nurul (57361639500); Jaafar, Mohd Hafiidz (57210229668); Ab Wahab, Mohd Nadhir (57223397087); Mohamed, Ahmad Sufril Azlan (57190968285)","57361509800; 57361509900; 57522765300; 57361510000; 57361639500; 57210229668; 57223397087; 57190968285","Movement Estimation Using Mediapipe BlazePose","2021","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","13051 LNCS","","","562","571","9","12","10.1007/978-3-030-90235-3_49","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120523531&doi=10.1007%2f978-3-030-90235-3_49&partnerID=40&md5=8c9646756ae9c308c1f6978e2787fb26","School of Computer Sciences, Universiti Sains Malaysia, Minden, Penang, 11800, Malaysia","Pauzi A.S.B., School of Computer Sciences, Universiti Sains Malaysia, Minden, Penang, 11800, Malaysia; Mohd Nazri F.B., School of Computer Sciences, Universiti Sains Malaysia, Minden, Penang, 11800, Malaysia; Sani S., School of Computer Sciences, Universiti Sains Malaysia, Minden, Penang, 11800, Malaysia; Bataineh A.M., School of Computer Sciences, Universiti Sains Malaysia, Minden, Penang, 11800, Malaysia; Hisyam M.N., School of Computer Sciences, Universiti Sains Malaysia, Minden, Penang, 11800, Malaysia; Jaafar M.H., School of Computer Sciences, Universiti Sains Malaysia, Minden, Penang, 11800, Malaysia; Ab Wahab M.N., School of Computer Sciences, Universiti Sains Malaysia, Minden, Penang, 11800, Malaysia; Mohamed A.S.A., School of Computer Sciences, Universiti Sains Malaysia, Minden, Penang, 11800, Malaysia","The paper describes a system to track the body movement of a person from a video source while augmenting the labelled skeleton joints onto the body of the person. This work has endless applications in the real world especially in the physical-demanding working environment as well as in the sports industry by implementing deep learning, the techniques can recognize the joints on a person’s body. An algorithm namely Mediapipe Blazepose has been applied using PoseNet dataset to detect and estimate curated movements specifically designed for body injury during heavy workload. The propose method has been compared to IMU based motion capture and the difference accuracy is within 10% since IMU capture real data of the sensors while the deep learning method using 2D image analysis. The expected outcome from this project is a working system that is able to correctly identify and label the skeleton joints on a person’s body as well as perform various calculation such as movement velocity and the angle of joints which could be crucial for determining whether certain body movements could result in injuries either in the short- or long-term period. © 2021, Springer Nature Switzerland AG.","Deep learning; Marker-based; Marker-less; Motion capture","Deep learning; Musculoskeletal system; Body movements; Deep learning; Marker-based; Marker-less; Motion capture; Real-world; Skeleton joints; Sports industries; Video sources; Working environment; Motion estimation","","","","","Ministry of Higher Education, Malaysia, MOHE, (FRGS/1/2020/STG07/USM/02/12)","Acknowledgement. The authors are grateful to the Ministry of Higher Education Malaysia for Fundamental Research Grant Scheme with Project Code: FRGS/1/2020/STG07/USM/02/12 for supporting this documented work.","Eldar R., Fisher-Gewirtzman D., Ergonomic design visualization mapping-developing an assistive model for design activities, Int. J. Ind. Ergon., 74, (2019); Maurice P., Et al., Human movement and ergonomics: An industry-oriented dataset for collaborative robotics, Int. J. Robot. Res., 38, 14, pp. 1529-1537, (2019); Yunus M.N.H., Jaafar M.H., Mohamed A.S.A., Azraai N.Z., Hossain M., Implementation of kinetic and kinematic variables in ergonomic risk assessment using motion capture simulation: A review, Int. J. Environ. Res. Public Health, 18, (2021); Bortolini M., Gamberi M., Pilati F., Regattieri A., Automatic assessment of the ergonomic risk for manual manufacturing and assembly activities through optical motion capture technology, Procedia CIRP, 72, pp. 81-86, (2018); Zhang Z., Fang Q., Gu X., Objective assessment of upper-limb mobility for poststroke rehabilitation, IEEE Trans. Biomed. Eng., 63, pp. 859-868, (2016); Ong Z.C., Seet Y.C., Khoo S.Y., Noroozi S., Development of an economic wireless human motion analysis device for quantitative assessment of human body joint, Measurement, 115, pp. 306-315, (2018); Fletcher S.R., Johnson T.L., Thrower J., A study to trial the use of inertial non-optical motion capture for ergonomic analysis of manufacturing work, Proc. Inst. Mech. Eng. Part B: J. Eng. Manuf., 232, 1, pp. 90-98, (2018); Plantard P., Auvinet E., Le Pierres A.S., Multon F., Pose estimation with a kinect for ergonomic studies: Evaluation of the accuracy using a virtual mannequin, Sensors (Switzerland), 15, 1, pp. 1785-1803, (2015); Wang X., Hu Y.H., Lu M.L., Radwin R.G., The accuracy of a 2D video-based lifting monitor, Ergonomics, 62, 8, pp. 1043-1054, (2019); Alessandro F., Norbert S., Markus M., Gabriele B., Emanuele R., Didier S., Survey of motion tracking methods based on inertial sensors: A focus on upper limb human motion, Sensors, 17, (2017); Mohamed A.S.A., Chingeng P.S., Mat Isa N.A., Surip S.S., Body matching algorithm using normalize dynamic time warping (NDTW) skeleton tracking for traditional dance movement, Advances in Visual Informatics. IVIC 2017. Lecture Notes in Computer Science, Vol. 10645, pp. 669-680, (2017); Warren T., A Closer Look at Microsoft’s New Kinect Sensor, (2020); VALD Performance (N.D.). News and Research, (2020)","A.S.A. Mohamed; School of Computer Sciences, Universiti Sains Malaysia, Penang, Minden, 11800, Malaysia; email: sufril@usm.my","Badioze Zaman H.; Smeaton A.F.; Shih T.K.; Velastin S.; Terutoshi T.; Jørgensen B.N.; Aris H.; Ibrahim N.","Springer Science and Business Media Deutschland GmbH","","7th International Conference on Advances in Visual Informatics, IVIC 2021","23 November 2021 through 25 November 2021","Kajang","268729","03029743","978-303090234-6","","","English","Lect. Notes Comput. Sci.","Conference paper","Final","","Scopus","2-s2.0-85120523531"
"Shanmugasundaram K.; Mohamed A.S.A.; Ruhaiyem N.I.R.","Shanmugasundaram, Karthikeyan (57193491154); Mohamed, Ahmad Sufril Azlan (57190968285); Ruhaiyem, Nur Intan Raihana (57190964192)","57193491154; 57190968285; 57190964192","An overview of hand-based multimodal biometrie system using multi-classifier score fusion with score normalization","2017","Proceedings of IEEE International Conference on Signal Processing and Communication, ICSPC 2017","2018-January","","","53","57","4","6","10.1109/CSPC.2017.8305806","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046944218&doi=10.1109%2fCSPC.2017.8305806&partnerID=40&md5=0a40f31e5e64bca1e652f85cbf6d003d","School of Computer Sciences, Universiti Sains Malaysia, USM, Penang, 11800, Malaysia","Shanmugasundaram K., School of Computer Sciences, Universiti Sains Malaysia, USM, Penang, 11800, Malaysia; Mohamed A.S.A., School of Computer Sciences, Universiti Sains Malaysia, USM, Penang, 11800, Malaysia; Ruhaiyem N.I.R., School of Computer Sciences, Universiti Sains Malaysia, USM, Penang, 11800, Malaysia","In the emerging trends of biometrie authentication, multimodal biometries getting more attention from researchers due to its universality, uniqueness, no intra-class variations, no inter-class similarities, and anti-spoofing attacks than unimodal biometrics. Hand-based multibiometric system is the most successful and used in many real time systems especially law enforcement and forensics. Moreover, it is very user friendly and ease of use among all other biometric traits. Multi classifier fusion is the use of more classifiers for each modality involved rather than single at the score fusion of multibiometric system. Furthermore, hand-based multimodal biometrics can use either or all traits of fingerprint, palm print, finger vein, palm vein, dorsal vein, hand geometry, finger knuckle print and many more. In this paper, we reviewed various score normalization techniques and multi-classifiers used in the hand-based multimodal biometrics for each modality involved at the matching score fusion for enhancing the system performance further. © 2017 IEEE.","Multi-classifier; Multimodal biometrics; Score fusion","Biometrics; Interactive computer systems; Biometrie; Biometrie systems; Emerging trends; Intra-class variation; Multi-classifier; Multi-modal; Multi-modal biometrics; Multibiometric systems; Score fusion; Score normalization; Real time systems","","","","","","","Wozniak M., Et al., A survey of multi-classifier as hybrid systems, Information Fusion, (2014); Aravinth J., Valarmathy S., Multi classifier-based score level fusion of multi-modal biometric recognition and its application to remote biometrics authentication, The Imaging Science Journal, 64, 1, pp. 1-14, (2016); Saigaa M., Et al., Hand-based Biometric for Personal Identification Using Correlation Filter Classifier, (2014); Ramalho M., Et al., Secure multispectral hand recognition system, European Signal Processing Conference, Spain, (2011); Zhu L.Q., Et al., Multimodal Biometric Identification System Based on Finger Geometry, Knuckle Print, and Palm Print, (2010); Peng J., Et al., Multimodal biometric authentication based on score level fusion of finger biometrics, Optik, (2014); Karthik, Et al., Score Level Fusion using Hybrid BF-pfPSO for Face and Fingerprint Multimodal Biometric System, IEEE Conference Proceedings, ICECS, (2016); Bharathi S., Et al., Hand Vein-based Multimodal Biometric Recognition, Acta Polytechnica Hungarica, 12, 6, (2015); Sumathi S., Et al., Multimodal Biometrics for Person Authentication using Hand Image, Ijca, 70, 24, (2013); Nandakumar R.K., Jain A.K., Handbook of Multibiometrics, (2006); Esther, Et al., A multimodal biometric system based on palm print and finger knuckle print recognition methods, An International Arab Journal of Information Technology, 12, 2, (2015); Kumar A., Et al., Personal authentication using finger knuckle surface, IEEE Trans. Info. Forensics and Security, 4, 1, pp. 98-110, (2009); Arulalan V., Et al., Multimodal biometric system using iris and inner knuckle print, Ijca, 106, 6, (2014); Maltoni D., Et al., Handbook of Fingerprint Recognition, (2009); Jain A., Et al., Score normalization in multimodal biometric systems, Pattern Recognition 38-2270-2285, (2005); Cappelli R., Et al., Combining fingerprint classifiers, Proceedings of First International Workshop on Multiple Classifier Systems, pp. 351-361, (2000); Hampel F.R., Et al., Robust Statistics: The Approach Based on Influence Functions, (1986); Meroumia A., Et al., Multimodal Biometric Person Recognition System Based on Fingerprint& Finger-Knuckle-Print Using Correlation Filter Classifier; Hanmandlu M., Et al., Score level fusion of multimodal biometrics using triangular norms, Pattern Recognition Letters, (2001)","A.S.A. Mohamed; School of Computer Sciences, Universiti Sains Malaysia, USM, Penang, 11800, Malaysia; email: sufril@usm.my","","Institute of Electrical and Electronics Engineers Inc.","","2017 IEEE International Conference on Signal Processing and Communication, ICSPC 2017","28 July 2017 through 29 July 2017","Coimbatore","135075","","978-150906730-5","","","English","Proc. IEEE Int. Conf. Signal Process. Commun., ICSPC","Conference paper","Final","","Scopus","2-s2.0-85046944218"
"Salisu S.; Mohamed A.S.A.; Jaafar M.H.; Pauzi A.S.B.; Younis H.A.","Salisu, Sani (57718330400); Mohamed, A.S.A. (57190968285); Jaafar, M.H. (57210229668); Pauzi, Ainun S.B. (57361509800); Younis, Hussain A. (57210408507)","57718330400; 57190968285; 57210229668; 57361509800; 57210408507","A Survey on Deep Learning-Based 2D Human Pose Estimation Models","2023","Computers, Materials and Continua","76","2","","2385","2400","15","0","10.32604/CMC.2023.035904","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173547607&doi=10.32604%2fCMC.2023.035904&partnerID=40&md5=64fc6af919d9e769be71cb4245d4afd4","School of Computer Science, Universiti Sains Malaysia, Penang, 11800, Malaysia; Department of Information Technology, Faculty of Computing, Federal University Dutse, Jigawa, 720211, Nigeria; School of Industrial Technology, Universiti Sains Malaysia, Penang, 11800, Malaysia; College of Education for Women, University of Basrah, Basrah, 61004, Iraq","Salisu S., School of Computer Science, Universiti Sains Malaysia, Penang, 11800, Malaysia, Department of Information Technology, Faculty of Computing, Federal University Dutse, Jigawa, 720211, Nigeria; Mohamed A.S.A., School of Computer Science, Universiti Sains Malaysia, Penang, 11800, Malaysia; Jaafar M.H., School of Industrial Technology, Universiti Sains Malaysia, Penang, 11800, Malaysia; Pauzi A.S.B., School of Computer Science, Universiti Sains Malaysia, Penang, 11800, Malaysia; Younis H.A., School of Computer Science, Universiti Sains Malaysia, Penang, 11800, Malaysia, College of Education for Women, University of Basrah, Basrah, 61004, Iraq","In this article, a comprehensive survey of deep learning-based (DL-based) human pose estimation (HPE) that can help researchers in the domain of computer vision is presented. HPE is among the fastest-growing research domains of computer vision and is used in solving several problems for human endeavours. After the detailed introduction, three different human body modes followed by the main stages of HPE and two pipelines of two-dimensional (2D) HPE are presented. The details of the four components of HPE are also presented. The keypoints output format of two popular 2D HPE datasets and the most cited DL-based HPE articles from the year of breakthrough are both shown in tabular form. This study intends to highlight the limitations of published reviews and surveys respecting presenting a systematic review of the current DL-based solution to the 2D HPE model. Furthermore, a detailed and meaningful survey that will guide new and existing researchers on DL-based 2D HPE models is achieved. Finally, some future research directions in the field of HPE, such as limited data on disabled persons and multi-training DL-based models, are revealed to encourage researchers and promote the growth of HPE research. © 2023 Tech Science Press. All rights reserved.","2D; body parts; dataset; deep learning; Human pose estimation; models","Deep learning; Disabled persons; 2d; Body parts; Dataset; Deep learning; Estimation models; Human pose estimations; Human-body mode; Keypoints; Research domains; Two-dimensional; Computer vision","","","","","Universiti Sains Malaysia, USM, (304PTEKIND.6316497, FRGS/1/2020/STG07/USM/02/12)","Funding Statement: This work was supported by the [Universiti Sains Malaysia] under FRGS Grant Number [FRGS/1/2020/STG07/USM/02/12(203.PKOMP.6711930)] and FRGS Grant Number [304PTEKIND.6316497.USM.].","Xu C., Yu X., Wang Z., Ou L., Multi-view human pose estimation in human-robot interaction, Proc. IECON, pp. 4769-4775, (2020); Zhang P., Lan C., Xing J., Zeng W., Xue J., Et al., View adaptive neural networks for high performance skeleton-based human action recognition, IEEE Transactions on Pattern Analysis and Machine Intelligence, 41, 8, pp. 1963-1978, (2019); Cha J., Saqlain M., Lee C., Lee S., Kim D., Et al., Towards single 2D image-level self-supervision for 3D human pose and shape estimation, Applied Science, 11, 20, pp. 1-19, (2021); Li W., Du R., Chen S., Semantic–structural graph convolutional networks for whole-body human pose estimation, Information, 13, 3, pp. 1-14, (2022); Shotton J., Girshick R., Fitzgibbon A., Sharp T., Cook M., Et al., Efficient human pose estimation from single depth images, IEEE Transaction on Pattern Analysis and Machine Intelligence, 35, 12, pp. 2821-2840, (2013); Yang Y., Ramanan D., Articulated human detection with f lexible mixtures of parts, IEEE Transaction on Pattern Analysis and Machine Intelligence, 35, 12, pp. 2878-2890, (2013); Haq M. A., Planetscope nanosatellites image classification using machine learning, Computer Systems Science & Engineering, 42, 3, pp. 1031-1046, (2022); Hayder I. M., AL-Ali G., Younis H. A., Predicting reaction based on customer’s transaction using machine learning approaches, International Journal of Electrical and Computer Engineering, 13, 1, pp. 1086-1096, (2023); Haq M. A., CNN based automated weed detection system using UAV imagery, Computer Systems Science & Engineering, 42, 2, pp. 837-849, (2021); Younis H. A., Mohamed A. S. A., Ab Wahab M. N., Jamaludin R., Salisu S., Et al., A new speech recognition model in a human-robot interaction scenario using NAO robot: Proposal and preliminary model, Int. Conf. on Communication & Information Technology (ICICT), pp. 215-220, (2021); Haq M. A., Smotednn: A novel model for air pollution forecasting and aqi classification, Computers, Materials & Continua, 71, 1, pp. 1403-1425, (2022); Haq M. A., Jilania A. K., Prabu P., Deep learning-based modeling of groundwater storage change, Computers, Materials & Continua, 70, 3, pp. 4599-4617, (2022); Haq M. A., Rahaman G., Baral P., Ghosh A., Deep learning based supervised image classification using UAV images for forest areas classification, Journal of the Indian Society of Remote Sensing, 49, 3, pp. 601-606, (2021); Yadav C. S., Singh J., Yadav A., Pattanayak H. S., Kumar R., Et al., Malware analysis in IoT & android systems with defensive mechanism, Electroncs, 11, 15, pp. 1-20, (2022); Wang S., Celebi M. E., Zhang Y., Yu X., Lu S., Et al., Advances in data preprocessing for bio-medical data fusion: An overview of the methods, challenges, and prospects, Information Fusion, 76, pp. 376-421, (2021); Zhang Y., Dong Z., Wang S., Yu X., Yao X., Et al., Advances in multimodal data fusion in neuroimaging: Overview, challenges, and novel orientation, Information Fusion, 64, pp. 149-187, (2020); Toshev A., Szegedy C., DeepPose: Human pose estimation via deep neural networks, Proc. ICCVPR, pp. 1653-1660, (2014); Zhang F., Zhu X., Wang C., Single person pose estimation: A survey, (2021); Ullah H., Islam I. U., Ullah M., Afaq M., Khan S. D., Et al., Multi-feature-based crowd video modeling for visual event detection, Multimedia System, 27, 4, pp. 589-597, (2021); Tian Y., Zhang H., Liu Y., Wang L., Recovering 3D human mesh from monocular images: A survey, IEEE Transactions on Pattern Analysis and Machine Intelligence, pp. 1-20, (2023); Huang X., Wang X., Lv W., Bai X., Long X., Et al., PP-YOLOv2: A practical object detector, (2021); Xu X., Chen H., Moreno-Noguer F., Jeni L. A., de la Torre F., Et al., 3D human pose, shape and texture from low-resolution images and videos, IEEE Transaction on Pattern Analysis and Machine Intelligence, 44, 9, pp. 4490-4504, (2022); Nguyen C. H., Nguyen T. C., Tang T. N., Phan N. L. H., Improving object detection by label assignment distillation, Proc. IEEE/CVF Winter, pp. 1322-1331, (2022); Tompson J., Goroshin R., Jain A., LeCun Y., Bregler C., Et al., Efficient object localization using convolutional networks, Proc. IEEE/CVPR, pp. 648-656, (2015); Bisogni C., Castiglione A., Head pose estimation: An extensive survey on recent techniques and applications, Pattern Recognition, 127, pp. 1-14, (2022); Chen Y., Tian Y., He M., Monocular human pose estimation: A survey of deep learning-based methods, Computer Vision and Image Understanding, 192, pp. 1-23, (2020); Wang J., Tan S., Zhen X., Xu S., Zheng F., Et al., Deep 3D human pose estimation: A review, Computer Vision and Image Understanding, 210, pp. 1-21, (2021); Kamboj A., Rani R., Nigam A., A comprehensive survey and deep learning-based approach for human recognition using ear biometric, Visual Computer, 38, pp. 2383-2416, (2021); Chen K., Lin D., Dai B., Revisiting skeleton-based action recognition, 2022 IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR), 1, pp. 2969-2978, (2022); Liu H., Member S., Liu T., Zhang Z., ARHPE: Asymmetric relation-aware representation learning for head pose estimation in industrial human-computer interaction, IEEE Transactions on Industrial Informatics, 18, 10, pp. 7197-7117, (2022); Yu C., Xiao C., Gao B., Yuan L., Zhang L., Et al., Lite-HRNet: A lightweight high-resolution network, Proc. IEEE/CSC/CVPR, pp. 10435-10445, (2021); Wu Q., Zhu A., Cui R., Wang T., Hu F., Et al., Pose-guided inflated 3D convnet for action recognition in videos, Signal Processing: Image Communication, 91, pp. 1-9, (2021); Groos D., Ramampiaro H., Af Ihlen E., EfficientPose: Scalable single-person pose estimation, Applied Intelligence, 51, pp. 2518-2533, (2021); Li Y., Zhang S., Wang Z., Yang S., Yang W., Et al., TokenPose: Learning keypoint tokens for human pose estimation, Proc. IEEE/ICCV, pp. 11293-11302, (2021); Cheng B., Xiao B., Wang J., Shi H., Huang T. S., Et al., HigherhrNet: Scale-aware representation learning for bottom-up human pose estimation, Proc. IEEE CVF/CVPR, pp. 5385-5394, (2020); Zhang F., Zhu X., Dai H., Ye M., Zhu C., Distribution-aware coordinate representation for human pose estimation, Proc. IEEE/CVF/CVPR, pp. 7091-7100, (2020); Wang J., Sun K., Cheng T., Jiang B., Dang C., Et al., Deep high-resolution representation learning for visual recognition, IEEE Transactions on Pattern Analysis and Machine Intelligence, 43, 10, pp. 3349-3364, (2021); Sun K., Xiao B., Liu D., Wang J., Deep high-resolution representation learning for human pose estimation, Proc. IEEE/CVF/CVPR, pp. 5686-5696, (2019); Guler R. A., Neverova N., Kokkinos I., DensePose: Dense human pose estimation in the wild, Proc. of the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), pp. 7297-7306, (2018); Chen Y., Wang Z., Peng Y., Zhang Z., Yu G., Et al., Cascaded pyramid network for multi-person pose estimation, Proc. IEEE/CVF/CVPR, pp. 7103-7112, (2018); He K., Gkioxari G., Dollar P., Girshick R., Mask R-CNN, IEEE Transactio on Pattern Analysis and Machinr Intelligence, 42, 2, pp. 386-397, (2017); Cao Z., Simon T., Wei S. E., Sheikh Y., Realtime multi-person 2D pose estimation using part affinity fields, Proc. IEEE/CVF, CVPR, pp. 1302-1310, (2017); Newell A., Yang K., Deng J., Stacked hourglass networks for human pose estimation, Proc. ECCV, pp. 483-499, (2016); Wei S. E., Ramakrishna V., Kanada T., Sheikh Y., Convolutional pose machines, Proc. IEEE/CVF, CVPR, pp. 4724-4732, (2016); Pfister T., Charles J., Zisserman A., Flowing convnets for human pose estimation in videos, Proc. IEE Xplore, ICCV Santago, Chile, pp. 648-656, (2015); Andriluka M., Pishchulin L., Gehler P., Schiele B., 2D human pose estimation: New benchmark and state of the art analysis, Proc. IEEE/CVF, CVPR, pp. 3686-3693, (2014); Gong W., Zhang S., Gonzalez J., Sobral A., Bouwmans T., Et al., Human pose estimation from monocular images: A comprehensive survey, Sensors, 16, 12, pp. 1-39, (2016); Johnson S., Everingham M., Clustered pose and nonlinear appearance models for human pose estimation, Proc. BMVC, pp. 1-11, (2010); Mehta D., Rhoden H., Casas D., Fua P., Sotnychenco O., Et al., Monocular 3D human pose estimation in the wild using improved CNN supervision, Proc. IEEE/IC3DV, pp. 506-516, (2018); Jiang H., Finding human poses in videos using concurrent matching and segmentation, Proc., ACCV, pp. 228-243, (2010); Freifeld O., Weiss A., Zuffi S., Black M. J., Contour people: A parameterized model of 2D articulated human shape, Proc. IEEE/CSC, CVPR, pp. 639-646, (2010); Joo H., Simon T., Sheikh Y., Total capture: A 3D deformation model for tracking faces, hands, and bodies, Proc. IEEE/CVF, CVPR, pp. 8320-8329, (2018); Xu H., Bazavan E. G., Zanfir A., Freeman W. T., Sukthankar R., Et al., GHUM GHUML: Generative 3D human shape and articulated pose models, Proc. IEEE/CVF, CVPR, pp. 6183-6192, (2020); Pishchulin L., Andriluka M., Gehler P., Schiele B., Poselet conditioned pictorial structures, Proc. IEEE CVPR, pp. 588-595, (2013); Nie B. X., Xiong C., Zhu S. C., Joint action recognition and pose estimation from video, Proc. IEEE/CVPR, pp. 1293-1301, (2015); Andriluka M., Roth S., Schiele B., Pictorial structures revisited: People detection and articulated pose estimation, Proc. IEEE/CVPR, pp. 1014-1021, (2009); Dantone M., Gall J., Leistner C., van Gool L., Human pose estimation using body parts dependent joint regressors, Proc. IEEE/CVPR, pp. 3041-3048, (2013); Gkioxari G., Hariharan B., Girshick R., Malik J., Using k-poselets for detecting people and localizing their keypoints, Proc. IEEE/CVPR, pp. 3582-3589, (2014); Mao W., Ge Y., Shen C., Tian Z., Wang X., Et al., Poseur: Direct human pose regression with transformers, pp. 1-15, (2022); Mao W., Ge Y., Shen C., Tian Z., Wang X., Et al., TFPose: Direct human pose estimation with transformers, pp. 1-15, (2021); Panteleris P., Argyros A., PE-Former: Pose estimation transformer, Proc. ICPRAL, pp. 1-14, (2022); Carreira J., Agrawal P., Fragkiadaki K., Malik J., Human pose estimation with iterative error feedback, Proc. IEEE/CVF, CVPR, pp. 4733-4742, (2016); Nibali A., He Z., Morgan S., Prendergast L., Numerical coordinate regression with convolutional neural networks, pp. 1-10, (2018); Ke L., Qi H., Chang M. C., Lyu S., Multi-scale supervised network for human pose estimation, Proc. IEEE/ICIP, pp. 564-568, (2018); Li S., Xiang X., Lightweight human pose estimation using heatmap-weighting loss, pp. 1-7, (2022); Balakrishnan K., Upadhyay D., BTranspose: Bottleneck transformers for human pose estimation with self-supervised pre-training, pp. 1-24, (2022); Srinivas A., Lin T. Y., Parmar N., Shlens J., Abbeel P., Et al., Bottleneck transformers for visual recognition, Proc. IEEE/CVF, CVPR, pp. 16514-16524, (2021); Lifshitz I., Fetaya E., Ullman S., Human pose estimation using deep consensus voting, pp. 246-260, (2016); Zhang K., Wu R., Yao P., Deng K., Li D., Et al., Learning heatmap-style jigsaw puzzles provides good pretraining for 2d human pose estimation, pp. 1-13, (2020); Xiong Z., Wang C., Li Y., Luo Y., Cao Y., Swin-pose: Swin transformer based human pose estimation, pp. 1-6, (2022); Moon G., Chang J. Y., Lee K. M., Posefix: Model-agnostic general human pose refinement network, Proc. IEEE/CVF, CVPR, pp. 7765-7773, (2019); Pishchulin L., Insafutdinov E., Tang S., Andres B., Andriluka M., Et al., DeepCut: Joint subset partition and labeling for multi person pose estimation, Proc. IEEE/CVF, VCPR, pp. 4929-4937, (2016); Insafutdinov E., Pishchulin L., Andres B., Andriluka M., Schiele B., Deepercut: A deeper, stronger, and faster multi-person pose estimation model, Proc. ECCV, pp. 34-50, (2016); Simonyan K., Zisserman A., Very deep convolutional networks for large-scale image recognition, Proc. ICLI, pp. 1-14, (2015); Zhao Y., Han R., Rao Y., A new feature pyramid network for object detection, Proc., IEEE/ICVRIS, pp. 428-431, (2019); Su Z., Ye M., Zhang G., Dai L., Sheng J., Improvement multi-stage model for human pose estimation, (2019); Brownlee J., Loss and loss functions for training deep learning neural networks, Machine Learning Mastery, (2019); Munea T. L., Jembre Y. Z., Weldegebriel H. T., Chen L., Huang C., Et al., The progress of human pose estimation: A survey and taxonomy of models applied in 2D human pose estimation, IEEE Access, 8, pp. 133330-133348, (2020); Lin T. Y., Maire M., Belongie S., Hays J., Peron P., Et al., Microsoft COCO: Common objects in context, Proc. ECCV, pp. 740-755, (2014); Sun K., Lan C., Xing J., Zeng W., Liu D., Et al., Human pose estimation using global and local normalization, Proc. IEEE/ICCV, pp. 5600-5608, (2017); Eichner M., Marin-Jimenez M., Zisserman A., Ferrari V., 2D articulated human pose estimation and retrieval in (almost) unconstrained still images, International Journal of Computer Vision, 99, 2, pp. 190-214, (2012)","A.S.A. Mohamed; School of Computer Science, Universiti Sains Malaysia, Penang, 11800, Malaysia; email: sufril@usm.my","","Tech Science Press","","","","","","15462218","","","","English","Comput. Mater. Continua","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85173547607"
"Bello R.-W.; Mohamed A.S.A.; Talib A.Z.","Bello, Rotimi-Williams (57209469141); Mohamed, Ahmad Sufril Azlan (57190968285); Talib, Abdullah Zawawi (35570816900)","57209469141; 57190968285; 35570816900","Cow Image Segmentation Using Mask R-CNN Integrated with Grabcut","2022","Lecture Notes in Networks and Systems","322","","","23","32","9","2","10.1007/978-3-030-85990-9_3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121771785&doi=10.1007%2f978-3-030-85990-9_3&partnerID=40&md5=f9d84d1dfedff75e6674744bf2b364a6","School of Computer Sciences, Universiti Sains Malaysia, Pulau Pinang, 11800, Malaysia; Department of Mathematics/Computer Sciences, University of Africa, Toru-Orua, Bayelsa State, Sagbama, Nigeria","Bello R.-W., School of Computer Sciences, Universiti Sains Malaysia, Pulau Pinang, 11800, Malaysia, Department of Mathematics/Computer Sciences, University of Africa, Toru-Orua, Bayelsa State, Sagbama, Nigeria; Mohamed A.S.A., School of Computer Sciences, Universiti Sains Malaysia, Pulau Pinang, 11800, Malaysia; Talib A.Z., School of Computer Sciences, Universiti Sains Malaysia, Pulau Pinang, 11800, Malaysia","Different state-of-the-art object detection methods have been applied in agriculture for precision livestock farming. However, the quality and accuracy of livestock such as cows being detected in an image during computer vision tasks depend on the segmentation and extraction techniques used. Mask R-CNN, an instance segmentation method popular for its class and mask regression has been widely applied for cow image segmentation tasks. However, its algorithm relies on simultaneous localization and mapping algorithms, thereby discrediting its ability to completely segment an image foreground from the image background. In this paper, a Mask R-CNN segmentation method integrated with Grabcut is proposed. The method is for the detection and complete extraction of an image foreground from the image background. We performed an experiment using edge detection to compare the segmentation that is region-based with the boundary estimation. After comparing the segmentation, we have concluded that the Grabcut integrated with the Mask R-CNN is based on the computation of the edge detection approach employed by our proposed method. © 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG.","Cow; Grabcut; Mask R-CNN","","","","","","","","Molecular Genetic Characterization of Animal Genetic Resources. No. 9 FAO Animal Production and Health Guidelines, (2011); Phenotypic Characterization of Animal Genetic Resources. No. 11 FAO Animal Production and Health Guidelines, (2012); Ghosh P., Mustafi S., Mandal S.N., Image-based goat breed identification and localization using deep learning, Int J Comput Vis Image Process, 10, 4, pp. 74-96, (2020); He K., Gkioxari G., Dollar P., Girshick R., Mask r-cnn, Proceedings of the IEEE International Conference on Computer Vision, pp. 2961-2969, (2017); Rother C., Kolmogorov V., Blake A., GrabCut, Interactive Foreground Extraction Using Iterated Graph Cuts. ACM Trans Graph (TOG), 23, 3, pp. 309-314, (2004); Ter-Sarkisov A., Ross R., Kelleher J., Earley B., Keane M (2018) Beef Cattle Instance Segmentation Using Fully Convolutional Neural Network. Arxiv Preprint, Arxiv, 1972, pp. 1-11, (1807); Qiao Y., Truman M., Sukkarieh S., Cattle segmentation and contour extraction based on Mask R-CNN for precision livestock farming, Comput Electron Agric, 165, pp. 1-9, (2019); Xu B., Wang W., Falzon G., Kwan P., Guo L., Chen G., Schneider D., Automated cattle counting using Mask R-CNN in quadcopter vision system, Comput Electron Agric, 171, pp. 1-12, (2020); Xudong Z., Xi K., Ningning F., Gang L., Automatic recognition of dairy cow mastitis from thermal images by a deep learning detector, Comput Electron Agric, 178, pp. 1-11, (2020); Bello R., Talib A., Mohamed A., Deep learning-based architectures for recognition of cow using cow nose image pattern, Gazi Univ J Sci, 33, 3, pp. 831-844, (2020); Angelova A., Zhu S., Efficient object detection and segmentation for fine-grained recognition, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 811-818, (2013); Gao S., Tsang I.W., Ma Y., Learning category-specific dictionary and shared dictionary for fine-grained image categorization, IEEE Trans Image Process, 23, 2, pp. 623-634, (2014); Akata Z., Reed S., Walter D., Lee H., Schiele B., Evaluation of output embeddings for fine grained image classification, The IEEE Conference on Computer Vision and Pattern Recognition, pp. 2927-2936, (2015); Xiao T., Xu Y., Yang K., Zhang J., Peng Y., Zhang Z., The application of two-level attention models in deep convolutional neural network for fine-grained image classification, The IEEE Conference on Computer Vision and Pattern Recognition, pp. 842-850, (2015); Xie S., Yang T., Wang X., Lin Y., Hyper-class augmented and regularized deep learning for fine grained image classification, The IEEE Conference on Computer Vision and Pattern Recognition, pp. 2645-2654, (2015); Selvaraju R.R., Cogswell M., Das A., Vedantam R., Parikh D., Batra D., Grad-CAM: Visual explanations from deep networks via gradient-based localization, The IEEE International Conference on Computer Vision, pp. 618-626, (2017); Zhang N., Donahue J., Girshick R., Darrell T., Part-based R-CNNs for fine-grained category detection, European Conference on Computer Vision, pp. 834-849, (2014); Liu X., Xia T., Wang J., Lin Y (2016) Fully Convolutional Attention Networks for Fine-Grained Recognition. Arxiv Preprint Arxiv, 6765, pp. 1-10, (1603); Wang Y., Wang Z., A survey of recent work on fine-grained image classification techniques, J Vis Commun Image Represent, 59, pp. 210-214, (2019); Wei X.S., Wu J., Cui Q (2019) Deep Learning for Fine-Grained Image Analysis: A Survey. Arxiv Preprint Arxiv, 3069, pp. 1-7, (1907); Bello R.W., Talib A.Z., Mohamed A.S.A., Olubummo D.A., Otobo F.N., Image-based individual cow recognition using body patterns, Int J Adv Comput Sci Appl, 11, 3, pp. 92-98, (2020); Wang D., Tang J., Zhu W., Li H., Xin J., He D., Dairy goat detection based on faster r-cnn from surveillance video, Comput Electron Agric, 154, pp. 443-449, (2018); Norouzzadeh M.S., Nguyen A., Kosmala M., Swanson A., Palmer M.S., Packer C., Clune J., Automatically identifying, counting, and describing wild animals in camera-trap images with deep learning, Proc Natl Acad Sci, 115, pp. E5716-E5725, (2018); Pinheiro P.O., Collobert R., Dollar P., Learning to Segment Object Candidates. Arxiv Preprint Arxiv:1506, 6204, pp. 1990-1998, (2015); Shelhamer E., Long J., Darrell T., Fully convolutional networks for semantic segmentation, IEEE Trans Pattern Anal Mach Intell, 39, 4, pp. 640-651, (2017); Ren S., He K., Girshick R., Sun J., Faster R-CNN: Towards real-time object detection with region proposal networks, IEEE Trans Pattern Anal Mach Intell, 39, 6, pp. 1137-1149, (2016); Girshick R., Donahue J., Darrell T., Malik J., Region-based convolutional networks for accurate object detection and segmentation, IEEE Trans Pattern Anal Mach Intell, 38, 1, pp. 142-158, (2015); Girshick R., Fast R-CNN, Proceedings of the IEEE International Conference on Computer Vision, pp. 1440-1448, (2015); Li K., Hariharan B., Malik J., Iterative instance segmentation, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 3659-3667, (2016); Zhao K., Kang J., Jung J., Sohn G., Building extraction from satellite images using mask r-cnn with building boundary regularization, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, pp. 247-251, (2018); Chen L.-C., Hermans A., Papandreou G., Schroff F., Wang P., Adam H., Masklab: Instance segmentation by refining object detection with semantic and direction features, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Pp 4013–, (2018); Bai M., Urtasun R., Deep watershed transform for instance segmentation, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Pp 5221–, (2017); Zhang H., Tian Y., Wang K., Zhang W., Wang F.Y., Mask SSD: An effective single-stage approach to object instance segmentation, IEEE Trans Image Process, 29, pp. 2078-2093, (2019); Russell B.C., Torralba A., Murphy K.P., Freeman W.T., LabelMe: A database and web-based tool for image annotation, Int J Comput Vision, 77, 1-3, pp. 157-173, (2008); Abadi M., Barham P., Chen J., Chen Z., Davis A., Dean J., Kudlur M., Tensorflow: A system for large-scale machine learning, Proceedings of the 12Th {USENIX} Symposium on Operating Systems Design and Implementation (OSDI ’16), pp. 265-283, (2016)","R.-W. Bello; School of Computer Sciences, Universiti Sains Malaysia, Pulau Pinang, 11800, Malaysia; email: sirbrw@gmail.com","Al-Emran M.; Al-Sharafi M.A.; Al-Kabi M.N.; Shaalan K.","Springer Science and Business Media Deutschland GmbH","","International Conference on Emerging Technologies and Intelligent Systems, ICETIS 2021","25 June 2021 through 26 June 2021","Al Buraimi","263669","23673370","978-303085989-3","","","English","Lect. Notes Networks Syst.","Conference paper","Final","","Scopus","2-s2.0-85121771785"
"Bello R.-W.; Mohamed A.S.A.; Talib A.Z.; Olubummo D.A.; Enuma O.C.","Bello, Rotimi-Williams (57209469141); Mohamed, Ahmad Sufril Azlan (57190968285); Talib, Abdullah Zawawi (35570816900); Olubummo, Daniel A. (57216345013); Enuma, O. Charles (57221132880)","57209469141; 57190968285; 35570816900; 57216345013; 57221132880","Enhanced Deep Learning Framework for Cow Image Segmentation","2021","IAENG International Journal of Computer Science","48","4","IJCS_48_4_38","","","","4","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122454556&partnerID=40&md5=23be8863e48b73c8e1d8c21d5afa4861","School of Computer Sciences, Universiti Sains Malaysia, Pulau Pinang, 11800, Malaysia; Computer and Information Systems, Robert Morris University, Moon-Township, PA, United States; Research and Statistics, Ministry of Health, Bayelsa, Nigeria","Bello R.-W., School of Computer Sciences, Universiti Sains Malaysia, Pulau Pinang, 11800, Malaysia; Mohamed A.S.A., School of Computer Sciences, Universiti Sains Malaysia, Pulau Pinang, 11800, Malaysia; Talib A.Z., School of Computer Sciences, Universiti Sains Malaysia, Pulau Pinang, 11800, Malaysia; Olubummo D.A., Computer and Information Systems, Robert Morris University, Moon-Township, PA, United States; Enuma O.C., Research and Statistics, Ministry of Health, Bayelsa, Nigeria","The applications of deep learning to livestock farming have in recent years gained wide acceptance from the computer vision community due to the continuous achievement of its applications to agricultural tasks. Moreover, the essentiality of deep learning is its practicality in detecting, segmenting, and classifying video and image objects without which precision livestock farming would have been impossible. However, the applications of most of the state-of-the-art models of deep learning to multiple cow objects image segmentation are not accurate and cannot generate colorimetric information due to poor pre-processing mechanism inherent in the associated methods and unequal training of their backbone layers. To overcome the above-mentioned limitations, an enhanced deep learning framework of Mask Region-based Convolutional Neural Network (Mask R-CNN) based on Generalized Color Fourier Descriptors (GCFD) is proposed. The enhanced model produced 0.93 mean Average Precision (mAP). The result shows the performance capability of the proposed framework over the state-of-the-art models for cow image segmentation © 2021. IAENG International Journal of Computer Science. All Rights Reserved.","Deep learning; Gcfd; Image segmentation; Mask r-cnn","Agriculture; Convolutional neural networks; Deep learning; Image enhancement; ART model; Deep learning; Gcfd; Images segmentations; ITS applications; Learning frameworks; Livestock farming; Mask r-cnn; State of the art; Vision communities; Image segmentation","","","","","","","Bello R., Talib A. Z., Mohamed A. S. A., Deep Learning-Based Architectures for Recognition of Cow Using Cow Nose Image Pattern, Gazi University Journal of Science, 33, 3, (2020); Hermans G., Ipema A., Stefanowska J., Metz J., The Effect of Two Traffic Situations on the Behaviour and Performance of Cows In An Automatic Milking System, Journal of Dairy Science, 86, 6, pp. 1997-2004, (2003); Miguel-Pacheco G., Kaler J., Remnant J., Cheyne L., Abbort C., French A., Pridmore T., Huxley J., Behavioural Changes in Dairy Cows With Lameness In An Automatic Milking System, Appl Animal Behav Sci, 150, pp. 1-8, (2014); Rutten C., Velthuis A., Steeneveld W., Hogeveen H., Invited Review: Sensors to Support Health Management on Dairy Farms, Journal of Dairy Science, 96, 4, pp. 1928-1952, (2013); Porto S. M. C., Arcidiacono C., Anguzza U., Cascone G., The Automatic Detection of Dairy Cow Feeding and Standing Behaviours in Free-Stall Barns By A Computer Vision-Based System, Biosystems Engineering, 133, pp. 46-55, (2015); Grandin T., Improving Animal Welfare: A Practical Approach (2nd ed), (2015); Bello R. W., Olubummo D. A., Seiyaboh Z., Enuma O. C., Talib A. Z., Mohamed A. S. A., Cattle identification: the history of nose prints approach in brief, IOP Conference Series: Earth and Environmental Science, 594, pp. 1-9, (2020); Andriluka M., Pishchulin L., Gehler P., Schiele B., 2d Human Pose Estimation: New Benchmark and State of the Art Analysis, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 3686-3693, (2014); Lin T. Y., Maire M., Belongie S., Hays J., Perona P., Ramanan D., Zitnick C. L., Microsoft COCO: Common Objects In Context, European Conference on Computer Vision, pp. 740-755, (2014); Toshev A., Szegedy C., Deep Pose: Human Pose Estimation Via Deep Neural Networks, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 1653-1660, (2014); Newell A., Yang K., Deng J., Stacked Hourglass Networks for Human Pose Estimation, European Conference on Computer Vision, pp. 483-499, (2016); Insafutdinov E., Andriluka M., Pishchulin L., Tang S., Levinkov E., Andres B., Schiele B., Arttrack: Articulated Multi-Person Tracking in the Wild, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 6457-6465, (2017); Cao Z., Hidalgo G., Simon T., Wei S. E., Sheikh Y., OpenPose: Realtime Multi-Person 2D Pose Estimation Using Part Affinity Fields, pp. 1-14, (2018); Cao Z., Simon T., Wei S. E., Sheikh Y., Realtime Multi-Person 2d Pose Estimation Using Part Affinity Fields, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 7291-7299, (2017); Pishchulin L., Insafutdinov E., Tang S., Andres B., Andriluka M., Gehler P. V., Schiele B., Deepcut: Joint Subset Partition and Labeling for Multi-Person Pose Estimation, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 4929-4937, (2016); Ionescu D., Ionescu B., Gadea C., Islam S., An Intelligent Gesture Interface for Controlling TV Sets and Set-Top Boxes, 6th IEEE International Symposium on Applied Computational Intelligence and Informatics (SACI), pp. 159-164, (2011); Sermanet P., Eigen D., Zhang X., Mathieu M., Fergus R., LeCun Y., Overfeat: Integrated Recognition, Localization and Detection Using Convolutional Networks, pp. 1-16, (2013); Girshick R., Fast R-CNN, Proceedings of the IEEE International Conference on Computer Vision, pp. 1440-1448, (2015); Ren S., He K., Girshick R., Sun J., Faster R-CNN: Towards Real-Time Object Detection With Region Proposal Networks, Advances in Neural Information Processing Systems, pp. 91-99, (2015); Redmon J., Divvala S., Girshick R., Farhadi A., You Only Look Once: Unified, Real-Time Object Detection, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 779-788, (2016); Liu W., Anguelov D., Erhan D., Szegedy C., Reed S., Fu C. Y., Berg A. C., SSD: Single Shot Multibox Detector, European Conference on Computer Vision, pp. 21-37, (2016); He K., Gkioxari G., Dollar P., Girshick R., Mask R-CNN, Proceedings of the IEEE International Conference on Computer Vision, pp. 2961-2969, (2017); Zeiler M. D., Fergus R., Visualizing and Understanding Convolutional Networks, European Conference on Computer Vision, pp. 818-833, (2014); Rosyadi A. W., Suciati N., Image segmentation using transition region and k-means clustering, IAENG International Journal of Computer Science, 47, 1, pp. 47-55, (2020); He K., Zhang X., Ren S., Sun J., Deep Residual Learning For Image Recognition, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 770-778, (2016); Szegedy C., Liu W., Jia Y., Sermanet P., Reed S., Anguelov D., Rabinovich A., Going Deeper With Convolutions, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 1-9, (2015); Bello R. -W., Talib A. Z. H., Mohamed A. S. A. B., Deep belief network approach for recognition of cow using cow nose image pattern, Walailak Journal of Science and Technology, 18, 5, pp. 1-14, (2021); Bello R. W., Talib A. Z., Mohamed A. S. A., Olubummo D. A., Otobo F. N., Image-based Individual Cow Recognition using Body Patterns, International Journal of Advanced Computer Science and Applications, 11, 3, pp. 92-98, (2020); Long J., Shelhamer E., Darrell T., Fully Convolutional Networks For Semantic Segmentation, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 3431-3440, (2015); Bolya D., Zhou C., Xiao F., Lee Y. J., Yolact: Real-Time Instance Segmentation, Proceedings of the IEEE International Conference on Computer Vision, pp. 9157-9166, (2019); Cai Z., Vasconcelos N., Cascade R-CNN: High-Quality Object Detection and Instance Segmentation, IEEE Transactions on Pattern Analysis and Machine Intelligence, pp. 1-14, (2019); Bello R. -W., Mohamed A. S. A., Talib A. Z., Contour Extraction of Individual Cattle From an Image Using Enhanced Mask R-CNN Instance Segmentation Method, IEEE Access, 9, pp. 56984-57000, (2021); Xu B., Wang W., Falzon G., Kwan P., Guo L., Chen G., Schneider D., Automated Cattle Counting Using Mask R-CNN in Quadcopter Vision System, Computers and Electronics in Agriculture, 171, pp. 1-12, (2020); Qiao Y., Truman M., Sukkarieh S., Cattle Segmentation and Contour Extraction Based on Mask R-CNN For Precision Livestock Farming, Computers and Electronics in Agriculture, 165, (2019); Russell B. C., Torralba A., Murphy K. P., Freeman W. T., LabelMe: A Database and Web-Based Tool For Image Annotation, International Journal of Computer Vision, 77, 1-3, (2008); Deng J., Dong W., Socher R., Li L. J., Li K., Fei-Fei L., Imagenet: A Large-Scale Hierarchical Image Database, IEEE Conference on Computer Vision and Pattern Recognition, pp. 248-255, (2009); Everingham M., Van Gool L., Williams C. K., Winn J., Zisserman A., The Pascal Visual Object Classes (VOC) Challenge, International Journal of Computer Vision, 88, 2, pp. 303-338, (2010); Mennesson J., Saint-Jean C., Mascarilla L., Color Object Recognition Based on a Clifford Fourier Transform, Guide to Geometric Algebra in Practice, pp. 175-191, (2011); Abadi M., Barham P., Chen J., Chen Z., Davis A., Dean J., Kudlur M., Tensorflow: A System for Large-Scale Machine Learning, 12th {USENIX} Symposium on Operating Systems Design and Implementation, pp. 265-283, (2016); LeCun Y., Bottou L., Orr G.. B., Muller K. R., Efficient Backprop, Neural Networks: Tricks of the Trade, Lecture Notes in Computer Sciences, 1524, pp. 5-50, (1998); Ter-Sarkisov A., Ross R., Kelleher J., Earley B., Keane M., Beef Cattle Instance Segmentation Using Fully Convolutional Neural Network, pp. 1-11, (2018); Kristan M., Matas J., Leonardis A., Vojir T., Pflugfelder R., Fernandez G., Cehovin L., A Novel Performance Evaluation Methodology for Single-Target Trackers, IEEE Transactions on Pattern Analysis and Machine Intelligence, 38, 11, (2016); Li Y., Qi H., Dai J., Ji X., Wei Y., Fully convolutional instance-aware semantic segmentation, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2359-2367, (2017); Zhang H., Tian Y., Wang K., Zhang W., Wang F. Y., Mask SSD: An effective single-stage approach to object instance segmentation, IEEE Transactions on Image Processing, 29, 1, pp. 2078-2093, (2019); Pinheiro P. O., Collobert R., Dollar P., Learning to segment object candidates, Advances in Neural Information Processing Systems, pp. 1990-1998, (2015); Pinheiro P. O., Lin T. Y., Collobert R., Dollar P., Learning to refine object segments, European Conference on Computer Vision, pp. 75-91, (2016); Dai J., He K., Sun J., Instance-aware semantic segmentation via multi-task network cascades, IEEE Conference on Computer Vision and Pattern Recognition, pp. 3150-3158, (2016)","R.-W. Bello; School of Computer Sciences, Universiti Sains Malaysia, Pulau Pinang, 11800, Malaysia; email: sirbrw@yahoo.com","","International Association of Engineers","","","","","","1819656X","","","","English","IAENG Int. J. Comput. Sci.","Article","Final","","Scopus","2-s2.0-85122454556"
"Bello R.-W.; Mohamed A.S.A.; Talib A.Z.","Bello, Rotimi-Williams (57209469141); Mohamed, Ahmad Sufril Azlan (57190968285); Talib, Abdullah Zawawi (35570816900)","57209469141; 57190968285; 35570816900","Contour Extraction of Individual Cattle from an Image Using Enhanced Mask R-CNN Instance Segmentation Method","2021","IEEE Access","9","","9400819","56984","57000","16","21","10.1109/ACCESS.2021.3072636","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104249692&doi=10.1109%2fACCESS.2021.3072636&partnerID=40&md5=e8ed42fe4d10155350dda431983c7360","School of Computer Sciences, Universiti Sains Malaysia, Pulau Pinang, 11800, Malaysia","Bello R.-W., School of Computer Sciences, Universiti Sains Malaysia, Pulau Pinang, 11800, Malaysia; Mohamed A.S.A., School of Computer Sciences, Universiti Sains Malaysia, Pulau Pinang, 11800, Malaysia; Talib A.Z., School of Computer Sciences, Universiti Sains Malaysia, Pulau Pinang, 11800, Malaysia","In animal husbandry, the traceability of individual cattle, their health information, and performance records greatly depend on computer vision and image processing-based approaches. However, some of these approaches perform below expectations in obtaining real-time information about individual cattle. No doubt, inaccurate segmentation and incomplete extraction of each cattle object from an image are notable contributory factors. As accurate segmentation is a prerequisite for obtaining real-time information about individual cattle, and since the algorithm of Mask R-CNN relies on the algorithm of simultaneous localization and mapping (SLAM), for the construction of the semantic map, which sometimes exchanges image background for the foreground, there is a need to enhance the available approaches towards achieving precision animal husbandry. To achieve this, an enhanced Mask R-CNN instance segmentation method is proposed to support indistinct boundaries and irregular shapes of cattle bodies. The methods employed in the research are in multiple folds: (1) Pre-enhancement of the image using generalized color Fourier descriptors (GCFD); (2) Provision of optimal filter size that was smaller than ResNet101 (the backbone of Mask R-CNN) for the extraction of smaller and composite features; (3) Utilization of multiscale semantic features using region proposals; (4) A fully connected layer of existing Mask R-CNN integrated with a sub-network for enhanced segmentation and (5) Post-enhancement of the image using Grabcut. Experiments on the datasets of cattle images produced better results when compared to other state-of-the-art methods with 0.93 mAP. © 2013 IEEE.","Animal husbandry; cattle; Grabcut; instance segmentation; Mask R-CNN","Agriculture; Animals; Convolutional neural networks; Extraction; Image segmentation; Semantics; Contour Extraction; Contributory factors; Fourier descriptors; Health informations; Real-time information; Segmentation methods; Simultaneous localization and mapping; State-of-the-art methods; Image enhancement","","","","","Division of Research and Innovation; RCMO; Research University, (1001 / PKOMP / 8014001); School of Computer Sciences; School of Computer Sciences, Universiti Sains Malaysia; Universiti Sains Malaysia","Funding text 1: This work was supported by the Division of Research and Innovation (RCMO), Research University under Grant 1001 / PKOMP / 8014001 and School of Computer Sciences, Universiti Sains Malaysia.; Funding text 2: The authors received funding from the Division of Research and Innovation (RCMO), Research University Grant (1001 / PKOMP / 8014001) and School of Computer Sciences, Uni-versiti Sains Malaysia for this publication.","Van Hertem T., Rooijakkers L., Berckmans D., Fernandez A.P., Norton T., Berckmans D., Vranken E., Appropriate data visualisation is key to precision livestock farming acceptance, Comput. Electron. Agricult., 138, pp. 1-10, (2017); Kumar S., Pandey A., Satwik K.S.R., Kumar S., Singh S.K., Singh A.K., Mohan A., Deep learning framework for recognition of cattle using muzzle point image pattern, Measurement, 116, pp. 1-17, (2018); Bello R., Talib A.Z., Mohamed A.S.A., Deep learning-based architectures for recognition of cow using cow nose image pattern, Gazi Univ. J. Sci., 33, 3, pp. 831-844, (2020); Norouzzadeh M.S., Nguyen A., Kosmala M., Swanson A., Palmer M.S., Packer C., Clune J., Automatically identifying, counting, and describing wild animals in camera-trap images with deep learning, Proc. Nat. Acad. Sci. USA, 115, 25, pp. E5716-E5725, (2018); Petersen W.E., The identification of the bovine by means of nose-prints, J. Dairy Sci., 5, 3, pp. 249-258, (1922); Bos J.M., Bovenkerk B., Feindt P.H., Van Dam Y.K., The quantified animal: Precision livestock farming and the ethical implications of objectification, Food Ethics, 2, 1, pp. 77-92, (2018); Zin T.T., Kobayashi I., Tin P., Hama H., A general video surveillance framework for animal behavior analysis, Proc. 3rd Int. Conf. Comput. Meas. Control Sensor Netw. (CMCSN), pp. 130-133, (2016); Lynn N.C., Zin T.T., Kobayashi I., Automatic assessing body condition score from digital images by active shape model and multiple regression technique, Proc. Int. Conf. Artif. Life Robot., Miyazaki, pp. 311-314, (2017); Nasirahmadi A., Edwards S.A., Sturm B., Implementation of machine vision for detecting behaviour of cattle and pigs, Livestock Sci., 202, pp. 25-38, (2017); Hansen M.F., Smith M.L., Smith L.N., Jabbar K.A., Forbes D., Automated monitoring of dairy cow body condition, mobility and weight using a single 3D video capture device, Comput. Ind., 98, pp. 14-22, (2018); Tebug S.F., Missohou A., Sabi S.S., Juga J., Poole E.J., Tapio M., Marshall K., Using body measurements to estimate live weight of dairy cattle in low-input systems in Senegal, J. Appl. Animal Res., 46, 1, pp. 87-93, (2018); Zhang A.L., Wu B.P., Wuyun C.T., Jiang D.X., Xuan E.C., Ma F.Y., Algorithm of sheep body dimension measurement and its applications based on image analysis, Comput. Electron. Agricult., 153, pp. 33-45, (2018); Tang P., Wang C., Wang X., Liu W., Zeng W., Wang J., Object detection in videos by high quality object linking, Ieee Trans. Pattern Anal. Mach. Intell., 42, 5, pp. 1272-1278, (2020); Bello R.W., Talib A.Z., Mohamed A.S.A., Olubummo D.A., Otobo F.N., Image-based individual cow recognition using body patterns, Int. J. Adv. Comput. Sci. Appl., 11, 3, pp. 92-98, (2020); Ren S., He K., Girshick R., Sun J., Faster R-CNN: Towards real-time object detection with region proposal networks, Proc. Adv. Neural Inf. Process. Syst., pp. 91-99, (2015); He K., Gkioxari G., Dollar P., Girshick R., Mask R-CNN, Proc. Ieee Int. Conf. Comput. Vis., pp. 2961-2969, (2017); Neumann L., Zisserman A., Vedaldi A., Relaxed softmax: Efficient confidence auto-calibration for safe pedestrian detection, Proc. Nips Workshop Mach. Learn. Intell. Transp. Syst., pp. 1-8, (2018); Chen L.-C., Hermans A., Papandreou G., Schroff F., Wang P., Adam H., MaskLab: Instance segmentation by refining object detection with semantic and direction features, Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., pp. 4013-4022, (2018); Fathi A., Wojna Z., Rathod V., Wang P., Song H.O., Guadarrama S., Murphy K.P., Semantic Instance Segmentation Via Deep Metric Learning, (2017); Gardenier J., Underwood J., Clark C., Object detection for cattle gait tracking, Proc. Ieee Int. Conf. Robot. Autom. (ICRA), pp. 2206-2213, (2018); Song X., Bokkers E.A.M., Tol Der Van P.P.J., Koerkamp P.W.G.G., Van Mourik S., Automated body weight prediction of dairy cows using 3-dimensional vision, J. Dairy Sci., 101, 5, pp. 4448-4459, (2018); Arnab A., Torr P.H.S., Pixelwise instance segmentation with a dynamically instantiated network, Proc. Ieee Conf. Comput. Vis. Pattern Recognit. (CVPR), pp. 441-450, (2017); Bell S., Zitnick C.L., Bala K., Girshick R., Inside-outside net: Detecting objects in context with skip pooling and recurrent neural networks, Proc. Ieee Conf. Comput. Vis. Pattern Recognit. (CVPR), pp. 2874-2883, (2016); Cao Z., Simon T., Wei S.-E., Sheikh Y., Realtime multi-person 2D pose estimation using part affinity fields, Proc. Ieee Conf. Comput. Vis. Pattern Recognit. (CVPR), pp. 7291-7299, (2017); Girshick R., Iandola F., Darrell T., Malik J., Deformable part models are convolutional neural networks, Proc. Ieee Conf. Comput. Vis. Pattern Recognit. (CVPR), pp. 437-446, (2015); Hayder Z., He X., Salzmann M., Shape-aware Instance Segmentation, (2016); Kirillov A., Levinkov E., Andres B., Savchynskyy B., Rother C., InstanceCut: From edges to instances with MultiCut, Proc. Ieee Conf. Comput. Vis. Pattern Recognit. (CVPR), pp. 5008-5017, (2017); Chen L.-C., Papandreou G., Kokkinos I., Murphy K., Yuille A.L., DeepLab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected CRFs, Ieee Trans. Pattern Anal. Mach. Intell., 40, 4, pp. 834-848, (2018); Li K., Hariharan B., Malik J., Iterative instance segmentation, Proc. Ieee Conf. Comput. Vis. Pattern Recognit. (CVPR), pp. 3659-3667, (2016); Zhao K., Kang J., Jung J., Sohn G., Building extraction from satellite images using mask R-CNN with building boundary regularization, Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. Workshops (CVPRW), pp. 247-251, (2018); Bai M., Urtasun R., Deep watershed transform for instance segmentation, Proc. Ieee Conf. Comput. Vis. Pattern Recognit. (CVPR), pp. 5221-5229, (2017); Ter-Sarkisov A., Ross R., Kelleher J., Earley B., Keane M., Beef Cattle Instance Segmentation Using Fully Convolutional Neural Network, (2018); Pinheiro P.O., Collobert R., Dollar P., Learning to segment object candidates, Proc. Adv. Neural Inf. Process. Syst., pp. 1990-1998, (2015); Pinheiro P.O., Lin T.Y., Collobert R., Dollar P., Learning to refine object segments, Proc. Eur. Conf. Comput. Vis., pp. 75-91, (2016); Fuentes A., Yoon S., Park J., Park D.S., Deep learning-based hierarchical cattle behavior recognition with spatio-temporal information, Comput. Electron. Agricult., 177, pp. 1-11, (2020); Long J., Shelhamer E., Darrell T., Fully convolutional networks for semantic segmentation, Proc. Ieee Conf. Comput. Vis. Pattern Recognit. (CVPR), pp. 3431-3440, (2015); Girshick R., Donahue J., Darrell T., Malik J., Region-based convolutional networks for accurate object detection and segmentation, Ieee Trans. Pattern Anal. Mach. Intell., 38, 1, pp. 142-158, (2016); Li Y., Qi H., Dai J., Ji X., Wei Y., Fully convolutional instanceaware semantic segmentation, Proc. Ieee Conf. Comput. Vis. Pattern Recognit. (CVPR), pp. 2359-2367, (2017); Xu B., Wang W., Falzon G., Kwan P., Guo L., Chen G., Tait A., Schneider D., Automated cattle counting using mask R-CNN in quadcopter vision system, Comput. Electron. Agricult., 171, pp. 1-12, (2020); Hariharan B., Arbelaez P., Girshick R., Malik J., Simultaneous detection and segmentation, Proc. Eur. Conf. Comput. Vis., pp. 297-312, (2014); He K., Zhang X., Ren S., Sun J., Deep residual learning for image recognition, Proc. Ieee Conf. Comput. Vis. Pattern Recognit. (CVPR), pp. 770-778, (2016); Mennesson J., Saint-Jean C., Mascarilla L., Color object recognition based on a Clifford Fourier transform, Guide to Geometric Algebra in Practice., pp. 175-191, (2011); Lin T.-Y., Dollar P., Girshick R., He K., Hariharan B., Belongie S., Feature pyramid networks for object detection, Proc. Ieee Conf. Comput. Vis. Pattern Recognit. (CVPR), pp. 2117-2125, (2017); Wu X., Wen S., Xie Y.A., Improvement of mask-RCNN object segmentation algorithm, Proc. Int. Conf. Intell. Robot. Appl., pp. 582-591, (2019); Russell B.C., Torralba A., Murphy K.P., Freeman W.T., LabelMe: A database andWeb-based tool for image annotation, Int. J. Comput. Vis., 77, 1-3, pp. 157-173, (2008); Alshdaifat N.F.F., Talib A.Z., Osman M.A., Improved deep learning framework for fish segmentation in underwater videos, Ecol. Informat., 59, pp. 1-11, (2020); Qiao Y., Truman M., Sukkarieh S., Cattle segmentation and contour extraction based on Mask R-CNN for precision livestock farming, Comput. Electron. Agricult., 165, pp. 1-9, (2019); Kristan M., Matas J., Leonardis A., Vojir T., Pflugfelder R., Fernandez G., Nebehay G., Porikli F., Cehovin L., A novel performance evaluation methodology for single-target trackers, Ieee Trans. Pattern Anal. Mach. Intell., 38, 11, pp. 2137-2155, (2016); Deng J., Dong W., Socher R., Li L.-J., Li K., Fei-Fei L., ImageNet: A large-scale hierarchical image database, Proc. Ieee Conf. Comput. Vis. Pattern Recognit., pp. 248-255, (2009); Lin T.-Y., Maire M., Belongie S., Hays J., Perona P., Ramanan D., Dollar P., Zitnick C.L., Microsoft COCO: Common objects in context, Proc. Eur. Conf. Comput. Vis., pp. 740-755, (2014); Everingham M., Van Gool L., Williams C.K.I., Winn J., Zisserman A., The Pascal visual object classes (VOC) challenge, Int. J. Comput. Vis., 88, 2, pp. 303-338, (2010); Redmon J., Farhadi A., YOLO9000: Better, faster, stronger, Proc. Ieee Conf. Comput. Vis. Pattern Recognit. (CVPR), pp. 6517-6525, (2017); Zhang H., Tian Y., Wang K., Zhang W., Wang F.-Y., Mask SSD: An effective single-stage approach to object instance segmentation, Ieee Trans. Image Process., 29, pp. 2078-2093, (2020); Dai J., He K., Sun J., Instance-aware semantic segmentation via multi-task network cascades, Proc. Ieee Conf. Comput. Vis. Pattern Recognit. (CVPR), pp. 3150-3158, (2016); Simonyan K., Zisserman A., Very Deep Convolutional Networks for Large-scale Image Recognition, (2014)","A.S.A. Mohamed; School of Computer Sciences, Universiti Sains Malaysia, Pulau Pinang, 11800, Malaysia; email: sufril@usm.my","","Institute of Electrical and Electronics Engineers Inc.","","","","","","21693536","","","","English","IEEE Access","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85104249692"
"Younis H.A.; Mohamed A.S.A.; Jamaludin R.; Ab Wahab M.N.","Younis, Hussain A. (57210408507); Mohamed, A.S.A. (57190968285); Jamaludin, R. (54401335900); Ab Wahab, M.N. (36471236100)","57210408507; 57190968285; 54401335900; 36471236100","Survey of robotics in education, taxonomy, applications, and platforms during COVID-19","2021","Computers, Materials and Continua","67","1","","687","707","20","11","10.32604/cmc.2021.013746","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099334783&doi=10.32604%2fcmc.2021.013746&partnerID=40&md5=43113133f6b79fd33e1266ad8985eca1","College of Education for Women University of Basrah, Basrah, Iraq; School of Computer Sciences, Universiti Sains Malaysia, Minden, Penang, 11800, Malaysia; Centre For Instructional Technology & Multimedia, Universiti Sains Malaysia, Minden, Penang, 11800, Malaysia","Younis H.A., College of Education for Women University of Basrah, Basrah, Iraq, School of Computer Sciences, Universiti Sains Malaysia, Minden, Penang, 11800, Malaysia; Mohamed A.S.A., School of Computer Sciences, Universiti Sains Malaysia, Minden, Penang, 11800, Malaysia; Jamaludin R., Centre For Instructional Technology & Multimedia, Universiti Sains Malaysia, Minden, Penang, 11800, Malaysia; Ab Wahab M.N., School of Computer Sciences, Universiti Sains Malaysia, Minden, Penang, 11800, Malaysia","The coronavirus disease 2019 (COVID-19) is characterized as a disease caused by a novel coronavirus known as severe acute respiratory coronavirus syndrome 2 (SARS-CoV-2; formerly known as 2019-nCoV). In December 2019, COVID-19 began to appear in a few countries. By the beginning of 2020, it had spread to most countries across the world. This is when education challenges began to arise. The COVID-19 crisis led to the closure of thousands of schools and universities all over the world. Such a situation requires reliance on e-learning and robotics education for students to continue their studies to avoid the mingling between people and students. In relation to this alternative learning solution, the present study was conducted. A systematic literature review on educational robotics (ER) keywords between 2015–2020 was carried out for the purpose to review a total of 2253 articles from the selected sources; Scopus (452), Taylor & Francis (311), Science Direct (427), IEEE Xplore (221), and Web of Science (842). This review procedure was labelled as Taxonomy 1. After filtering Taxonomy 1, it was found that 98 scientific articles formed the so-called Taxonomy II that was categorized into six categories: (i) Robotics concepts, (ii) Device, (iii) Robotic applications, (iv) Manufacturing robots, (v) Robotics analysis, and (vi) Education/taxonomy. For this study, only 35 articles in this specific field were selected, of which were then assigned into three categories: (i) Application, (ii) Platform, and (iii) Educational. The results show that the application category carries 17.4%, platform 20%, and education 22.85%. This study serves as the application platform to help students, academics, and researchers. © 2021 Tech Science Press. All rights reserved.","Application platform; COVID-19; Educational; Educational robotics; Taxonomy","Diseases; Robotics; Students; Tantalum compounds; Taxonomies; Application platforms; Educational robotics; Robotic applications; Robotics education; Scientific articles; Systematic literature review; Three categories; Web of Science; Educational robots","","","","","Division of Research and Innovation, Universiti Sains Malaysia","Funding Statement: The authors received funding from Division of Research and Innovation, Universiti Sains Malaysia for this study.","Ahmed H., La H. M., Education-robotics symbiosis: An evaluation of challenges and proposed recommendations, 9th IEEE Integrated STEM Education Conf., USA, pp. 222-229, (2019); Miller D. P., Nourbakhsh I., Robotics for education, Springer Handbook of Robotics, pp. 2115-2134, (2016); Santos C. B., Ferreira D. J., De Souza M. C. B. D. N. R., Martins A. R., Robotics and programming: Attracting girls to technology, Int. Conf. on Advances in Computing, Communications and Informatics, ICACCI, Jaipur, India, pp. 2052-2056, (2016); Younis Hussain A., Jamaludin R., Wahab M. N. A., Mohamed A. S. A., The review of NAO robotics in educational 2014–2020 in COVID-19 virus (pandemic era): Technologies, type of application, advantage, disadvantage and motivation, Journal of Physics: IOP, (2020); Sharkey A. J. C., Should we welcome robot teachers?, Ethics and Information Technology, 18, 4, pp. 283-297, (2016); Mohan R., Robotics: Its components, sensing, laws and applications, International Journal of Engineering in Computer Science, 1, 1, pp. 16-20, (2019); Peternel L., Tsagarakis N., Caldwell D., Ajoudani A., Robot adaptation to human physical fatigue in human-robot co-manipulation, Autonomous Robots, 42, 5, pp. 1011-1021, (2018); Chatzilygeroudis K., Vassiliades V., Stulp F., Calinon S., Mouret J. B., A survey on policy search algorithms for learning robot controllers in a handful of trials, IEEETransactionsonRobotics, 36, 2, pp. 328-347, (2020); Abdulkareem K. H., Mohammed M. A., Gunasekaran S. S., Al-Mhiqani M. N., Mutlag A. A., Et al., A review of fog computing and machine learning: Concepts, applications, challenges, and open issues, IEEE Access, 7, pp. 153123-153140, (2019); Atmatzidou S., Demetriadis S., Advancing students’ computational thinking skills through educational robotics: A study on age and gender relevant differences, Robotics and Autonomous Systems, 75, pp. 661-670, (2016); Istikomah I., Budiyanto C., The contribution of educational robotics and constructivist approach to computational thinking in the 21st century, 1st Int. Conf. on Computer Science and Engineering, Indonesia, pp. 610-616, (2019); Schopping T., Korthals T., Hesse M., AMiRo: A mini robot as versatile teaching platform, Journal of Intelligent & Robotic Systems, 81, 1, (2019); Feng T., Zou L., Yan J., Shi W., Liu Y., Et al., Brazilian robotics olympiad: A successful paradigm for science and technology dissemination, International Journal of Advanced Robotic Systems, 13, 5, pp. 1-8, (2016); Krajcsi A., Csapodi C., Stettner E., Algotaurus: An educational computer programming game for beginners, Interactive Learning Environments, 6, 2, pp. 1-14, (2019); Vega J., Canas J. M., Open vision system for low-cost robotics education, Electronics, 8, 11, pp. 1-20, (2019); Luciano S. C., Kost A. R., Robotic laboratory for distance education, Optics Education and Outreach, 9946, (2016); Bachiller-Burgos P., Barbecho I., Calderita L. V., Bustos P., Manso L. J., Learn block: A robot-agnostic educational programming tool, IEEE Access, 8, pp. 30012-30026, (2020); Dayoub F., Morris T., Corke P., Rubbing shoulders with mobile service robots, IEEE Access, 3, pp. 333-342, (2015); Tan N., Hayat A. A., Elara M. R., Wood K. L., A framework for taxonomy and evaluation of self-reconfigurable robotic systems, IEEE Access, 8, pp. 13969-13986, (2020); Cehovin Zajc L., Rezelj A., Skocaj D., Teaching with open-source robotic manipulator, Advances in Intelligent Systems and Computing, 829, pp. 189-198, (2019); Ferreira N. M. F., Moita F., Santos V. D. N., Ferreira J., Santos J., Et al., Education with robots inspired in biological systems, Advances in Intelligent Systems and Computing, 829, pp. 207-213, (2019); Pinto J. G. V. H., Monteiro J. M., Costa P., Prototyping and Programming a Multipurpose Educational Mobile Robot—NaSSIE, pp. 199-206, (2018); Lopez-Caudana E., Ponce P., Cervera L., Iza S., Mazon N., Robotic platform for teaching maths in junior high school, International Journal on Interactive Design and Manufacturing, 12, 4, pp. 1349-1360, (2018); Hameed I. A., Strazdins G., Hatlemark H. A. M., Jacobsen I. S., Damdam J. O., Robots that can mix serious with fun, Int. Conf. on Advanced Machine Learning Technologies and Applications, pp. 595-604, (2018); Schiffer S., Ferrein A., Erika—early robotics introduction at kindergarten age, Multimodal Technologies and Interaction, 2, 4, pp. 1-20, (2018); Chiazzese G., Arrigo M., Chifari A., Lonati V., Tosto C., Educational robotics in primary school: Measuring the development of computational thinking skills with the bebras tasks, InformaticsJournal, 6, 4, pp. 1-12, (2019); Negrini L., Teacher training in educational robotics an experience in southern switzerland: The PReSO project, Robotics in Educationc, pp. 92-97, (2019); Keller L., John I., Motivating female students for computer science by means of robot workshops, International Journal of Engineering Pedagogy, 10, 1, pp. 94-108, (2020); Mondada F., Bonnet E., Davrajh S., Johal W., Stopforth R., R2T2: Robotics to integrate educational efforts in south africa and europe, International Journal of Advanced Robotic Systems, 13, 5, pp. 1-13, (2016); Alex Polishuk I. V., Student-robot interactions in museum workshops: Learning activities and outcomes, Robotics in Education, pp. 233-244, (2017); Marios Xenos C. K., Yiannoutsou N., Grizioti M., Nikitopoulou S., Learning programming with educational robotics: Towards an integrated approach, International Conference EduRobotics, 560, pp. 215-222, (2017); Kandlhofer M., Steinbauer G., Evaluating the impact of educational robotics on pupils’ technical- and social-skills and science related attitudes, Robotics and Autonomous Systems, 75, pp. 679-685, (2016); Georg Jaggle A. W., Vincze Markus, Gottfried Koppensteiner W. L., Merdan M., Ibridge—Participative Cross-Generational Approach with Educational Robotics, pp. 263-274, (2019); Kantosalo A., Riihiaho S., Experience evaluations for human-computer co-creative processes–planning and conducting an evaluation in practice, Connection Science, Taylor & Francis Group, 31, 1, pp. 60-81, (2019); Bargagna S., Castro E., Cecchi F., Cioni G., Dario P., Et al., Educational robotics in down syndrome: A feasibility study, Technology Knowledge and Learning, 24, 2, pp. 315-323, (2019); Conchinha C., Osorio P., De Freitas J. C., Playful learning: Educational robotics applied to students with learning disabilities, Int. Sym. on Computers in Education, SIIE 2015, pp. 167-171, (2016); Urlings C. C., Coppens K. M., Borghans L., Measurement of executive functioning using a playful robot in kindergarten, Computers in the Schools, 36, 4, pp. 255-273, (2019); Alhaddad B. B. A. Y., Javed H., Connor O., Dena Al Thani J. J. C., RoboticTrainsasanEducational andTherapeuticToolforAutismSpectrumDisorderIntervention, pp. 249-262, (2019); Amin M. Z., Zamin N., Rahim H. A., Hassan N. I., Kamarudin N. D., Robo therapist: A sustainable approach to teach basic expressions for special needs children in malaysia, International Journal of Engineering and Technology(UAE), 7, 3, pp. 103-106, (2018); Francis K., Bruce C., Davis B., Drefs M., Hallowell D., Multidisciplinary perspectives on a video case of children designing and coding for robotics, Mathematics and Technology Education, 17, 3, pp. 165-178, (2017); Macgilchrist F., Allert H., Bruch A., Students and society in the 2020s. Three future ‘histories’ of education and technology, Learning Media and Technology, 45, 1, pp. 76-89, (2020); Patil S., Mane V., Patil P., Social innovation in education system by using robotic process automation (Rpa), International Journal of Innovative Technology and Exploring Engineering, 8, 11, pp. 3757-3760, (2019); Mohammed M. A., Abdulkareem K. H., Al-Waisy A. S., Mostafa S. A., Al-Fahdawi S., Et al., Benchmarking methodology for selection of optimal COVID-19 diagnostic model based on entropy and TOPSIS methods, IEEE Access, 8, pp. 99115-99131, (2020)","A.S.A. Mohamed; School of Computer Sciences, Universiti Sains Malaysia, Minden, Penang, 11800, Malaysia; email: sufril@usm.my","","Tech Science Press","","","","","","15462218","","","","English","Comput. Mater. Continua","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85099334783"
"Ayounis H.; Jamaludin R.; Wahab M.N.A.; Mohamed A.S.A.","Ayounis, Hussain (57210408507); Jamaludin, R. (54401335900); Wahab, M.N.A. (36471236100); Mohamed, A.S.A. (57190968285)","57210408507; 54401335900; 36471236100; 57190968285","The review of NAO robotics in Educational 2014-2020 in COVID-19 Virus (Pandemic Era): Technologies, type of application, advantage, disadvantage and motivation","2020","IOP Conference Series: Materials Science and Engineering","928","3","032014","","","","3","10.1088/1757-899X/928/3/032014","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097147382&doi=10.1088%2f1757-899X%2f928%2f3%2f032014&partnerID=40&md5=14d86cccb2dcdfbbe6ff53d583eef4ef","College of Education for Women, University of Basrah, Basrah, Iraq; School of Computer Sciences, Universiti Sains Malaysia, Pulau, Pinang, 11800, Malaysia","Ayounis H., College of Education for Women, University of Basrah, Basrah, Iraq, School of Computer Sciences, Universiti Sains Malaysia, Pulau, Pinang, 11800, Malaysia; Jamaludin R., School of Computer Sciences, Universiti Sains Malaysia, Pulau, Pinang, 11800, Malaysia; Wahab M.N.A., School of Computer Sciences, Universiti Sains Malaysia, Pulau, Pinang, 11800, Malaysia; Mohamed A.S.A., School of Computer Sciences, Universiti Sains Malaysia, Pulau, Pinang, 11800, Malaysia","The use of robotics in education is a very important issue for disposal and galaxies in this era of the pandemic (COVID-19). Where this study examines the topic of robotics in education (RIE) by using modern and specific query methods extracted from different research sites and based on judicious scholar's standards. These sites are Web of Science, Taylor and Francis and Science Direct. After careful investigation and deep research, the following titles should be taken which are (a)Educational robots, (b)education in robots, (c) human-robot interaction, (d) Higher Education, (e) academic, (f) smart pedagogy, (j)student, and (h) tutors. The retrieved articles were filtered according to the Use of robotics in Education. A total of 98 articles were selected and examined. Finally, we examined the taxonomy of these articles of robotics in Education base on faith and guidance, according to specific criteria, into six groups, which include Faith and Guidance, Concepts, Device, Application, Manufacturing, Studies Analysis and educational. Therefore, this work will be the platform and the guide for student, researcher, educators, anyone how interest in this field. The current focus in this area is on employing papers containing NAO robots and that 17 articles. © 2020 Published under licence by IOP Publishing Ltd.","academic; Education in robots; NAO robot; smart pedagogy; student","","","","","","","","Rodriguez I., Astigarraga A., Jauregi E., Ruiz T., Lazkano E., Humanizing NAO robot teleoperation using ROS, IEEE-RAS Int. Conf. Humanoid Robot, pp. 179-186, (2015); Ali S., Et al., Hand gesture based control of NAO robot using myo armband 953, (2020); Mubin O., Cappuccio M., Alnajjar F., Ahmad M. I., Shahid S., Can a robot invigilator prevent cheating?, AI Soc, (2020); Hameed I. A., Strazdins G., Hatlemark H. A. M., Jakobsen I. S., Damdam J. O., Robots that can mix serious with fun, International Conference on Advanced Machine Learning Technologies and Applications, pp. 595-604, (2018); Caudana E. L., Baltazar Reyes G., Acevedo R. G., Ponce P., Mazon N., Hernandez J. M., RoboTICs: Implementation of a Robotic Assistive Platform in a Mathematics High School Class, IEEE Int. Symp. Ind. Electron, pp. 1589-1594, (2019); Wan Z., Qin W., Song K., Wang B., Design and Implementation of Virtual Instructor Based on NAO Robot, 690, pp. 1207-1212, (2018); Mubin O., Ahmad M. I., Kaur S., Shi W., Khan A., Social Robots in Public Spaces: A Meta-review, International Conference on Social Robotics, pp. 213-220, (2018); Pohner N., Hennecke M., Evaluation of a robotics course with the humanoid Robot NAO in CS teacher education, ACM Int. Conf. Proceeding Ser, pp. 2-3, (2018); Michieletto S., Tosello E., Pagello E., Menegatti E., Teaching humanoid robotics by means of human teleoperation through RGB-D sensors, Rob. Auton. Syst, 75, pp. 671-678, (2016); Hawley L., Suleiman W., Control framework for cooperative object transportation by two humanoid robots, Rob. Auton. Syst, 115, pp. 1-16, (2019); Rosanda V., Starcic A. I., A review of social robots in classrooms: Emerging educational technology and teacher education, Educ. Self Dev, 14, pp. 93-106, (2019); Hong N. W. W., Chew E., Sze-Meng J. W., The review of educational robotics research and the need for real-world interaction analysis 2016, 14th Int. Conf. Control. Autom. Robot. Vision, ICARCV 2016, (2016); Kossewska J., Klosowska J., Acceptance of Robot-Mediated Teaching and Therapy for Children With Atypical Development by Polish Professionals, J. Policy Pract. Intellect. Disabil, 17, pp. 21-30, (2020); Keller L., John I., Motivating female students for computer science by means of robot workshops, Int. J. Eng. Pedagog, 10, pp. 94-108, (2020); Santos C. B., Ferreira D. J., De Souza M. C. B. D. N. R., Martins A. R., Robotics and programming: Attracting girls to technology 2016, Int. Conf. Adv. Comput. Commun. Informatics, ICACCI, 2016, pp. 2052-2056, (2016); Wright J. R., Ginter E. S., David B. G., Kilbourne B. J., Wells J. R., Intermediate programming methodologies for manipulating Modern Humanoid Robots Univers, J. Electr. Electron. Eng, 6, pp. 214-222, (2019); The Next Wave of Learning with Humanoid Robot: Learning Innovation Design starts with 'Hello NAO,, Accid. Anal. Prev, 19, pp. 501-502, (2019); Giaretta A., De Donno M., Dragoni N., Secure Coding and Ethical Hacking Workshops with NAO for Engaging K-12 Female Students, CS ACM Int. Conf. Proceeding Ser, (2018); Alex Polishuk I. V., Student-Robot Interactions in Museum Workshops: Learning Activities and Outcomes, (2017)","H. Ayounis; College of Education for Women, University of Basrah, Basrah, Iraq; email: hussain.younis@uobasrah.edu.iq","Shafik S.S.; Roomi A.B.; Sharrad F.I.","IOP Publishing Ltd","","2nd International Scientific Conference of Al-Ayen University, ISCAU 2020","15 July 2020 through 17 July 2020","Thi-Qar, Virtual","165187","17578981","","","","English","IOP Conf. Ser. Mater. Sci. Eng.","Conference paper","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85097147382"
"Yang L.; Mohamed A.S.A.; Ali M.K.M.","Yang, Lu (58514779800); Mohamed, Ahmad Sufril Azlan (57190968285); Ali, Majid Khan Majahar (57190937034)","58514779800; 57190968285; 57190937034","An Improved Object Detection and Trajectory Prediction Method for Traffic Conflicts Analysis","2023","Promet - Traffic and Transportation","35","4","","462","484","22","0","10.7307/ptt.v35i4.173","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171261200&doi=10.7307%2fptt.v35i4.173&partnerID=40&md5=3d9774b9ce0b7062dcdd2aa0f4a1f688","School of Computer Sciences, Universiti Sains, Malaysia; School of Mathematical Sciences, Universiti Sains, Malaysia","Yang L., School of Computer Sciences, Universiti Sains, Malaysia; Mohamed A.S.A., School of Computer Sciences, Universiti Sains, Malaysia; Ali M.K.M., School of Mathematical Sciences, Universiti Sains, Malaysia","Although computer vision-based methods have seen broad utilisation in evaluating traffic situations, there is a lack of research on the assessment and prediction of near misses in traf-fic. In addition, most object detection algorithms are not very good at detecting small targets. This study proposes a combination of object detection and tracking algorithms, Inverse Perspective Mapping (IPM), and trajectory prediction mechanisms to assess near-miss events. First, an instance segmentation head was proposed to improve the accuracy of the object frame box detection phase. Secondly, IPM was applied to all detection results. The relation-ship between them is then explored based on their distance to determine whether there is a near-miss event. In this process, the moving speed of the target was considered as a parame-ter. Finally, the Kalman filter is used to predict the object's trajectory to determine whether there will be a near-miss in the next few seconds. Experiments on Closed-Circuit Television (CCTV) datasets showed results of 0.94 mAP compared to other state-of-the-art methods. In addition to improved detection accuracy, the advantages of instance segmentation fused object detection for small target detection are validated. Therefore, the results will be used to analyse near misses more accurately. © 2023, Faculty of Transport and Traffic Engineering. All rights reserved.","near-miss; object detection; object tracking; trajectory prediction","computer vision; detection method; Kalman filter; prediction; segmentation; tracking; traffic management; trajectory","","","","","Fujian Provincial Education and Research Foundation for Youths in 2020, (JAT201039)","The authors express their sincere gratitude and appreciation to the Fujian Provincial Education and Research Foundation for Youths in 2020 (No. JAT201039) for supporting this study.","Loo BPY, Huang Z., Delineating traffic congestion zones in cities: An effective approach based on GIS, J Transp Geogr, 94, 6, (2021); Kitamura Y, Hayashi M, Yagi E., Traffic problems in Southeast Asia featuring the case of Cambodia’s traffic accidents involving motorcycles, IATSS Res, 42, 4, pp. 163-170, (2018); Zwetsloot G, Leka S, Kines P., Vision zero: From accident prevention to the promotion of health, safety and well-being at work, Policy and Practice in Health and Safety, 15, pp. 88-110, (2017); Zheng L, Sayed T, Mannering F., Modeling traffic conflicts for use in road safety analysis: A review of analytic methods and future directions, Anal Methods Accid Res, 29, pp. 100-142, (2021); Ge J, Et al., Accident causation models developed in China between 1978 and 2018: Review and comparison, Saf Sci, 148, 12, (2022); Heinrich HW, Stone RW., Industrial accident prevention, Soc Serv Rev, 5, 2, pp. 323-324, (1931); Terum JA, Svartdal F., Lessons learned from accident and near-accident experiences in traffic, Saf Sci, 120, 6, pp. 672-678, (2019); Wu LS, Dong Y, Wu, A case study of road traffic accidents based on Heinrich’s accident causation theory, Chinese Journal of Ergonomics, 24, 2, pp. 60-64, (2018); Hajjami LE, Mellouli EM, Berrada M., Neural network based sliding mode lateral control for autonomous vehicle, 2020 1st International Conference on Innovative Research in Applied Science, Engineering and Technology (IRASET), pp. 1-6, (2020); Lhoussain EH, Mellouli EM, Berrada M., Robust adaptive non-singular fast terminal sliding-mode lateral control for an uncertain ego vehicle at the lane-change maneuver subjected to abrupt change, Int. J. Dynam. Control, 9, pp. 1765-1782, (2021); Lhoussain EH, Et al., A robust intelligent controller for autonomous ground vehicle longitudinal dynamics, Applied Sciences, 13, 1, (2023); Kataoka H, Et al., Drive video analysis for the detection of traffic near-miss incidents, Proc.-IEEE Int. Conf. Robot. Autom, pp. 3421-3428, (2018); Suzuki T, Aoki Y, Kataoka H., Pedestrian near-miss analysis on vehicle-mounted driving recorders, Proc. 15th IAPR Int. Conf, pp. 416-419, (2017); Ospina MH, Et al., Extraction of decision rules using genetic algorithms and simulated annealing for prediction of severity of traffic accidents by motorcyclists, J. Ambient Intell. Humaniz. Comput, 12, pp. 10051-10072, (2021); Uchida N, Kawakoshi M, Tagawa T, Mochida T., An investigation of factors contributing to major crash types in Japan based on naturalistic driving dana, IATSS Res, 34, pp. 22-30, (2010); Chen W, Qiao Y, Li Y., Inception-SSD: An improved single shot detector for vehicle detection, J. Ambient Intell. Humaniz. Comput, (2020); Yang B, Et al., A vehicle tracking algorithm combining detector and tracker, Eurasip J. Image Video Process, 1, (2020); Wan J., An efficient small traffic sign detection method based on YOLOv3, J. Signal Process. Syst, (2020); Tran AC, Et al., A model for real-time traffic signs recognition based on the YOLO algorithm – A case study using vietnamese traffic signs, Futur. Data Secur. Eng, 11814, pp. 104-116, (2019); Kurniawan J, Dewa CK, Afiahayati, Traffic congestion detection: Learning from CCTV monitoring images using convolutional neural network, Procedia Comput Sci, 144, pp. 291-297, (2018); Abdel-Aty M, Wu Y, Zheng O, Yuan J., Using closed-circuit television cameras to analyze traffic safety at intersections based on vehicle key points detection, Accid Anal Prev, 176, (2022); Bertozzi M, Broggi A, Fascioli A., Stereo inverse perspective mapping: Theory and applications, Image Vis Comput, 16, 8, pp. 585-590, (1998); Zhao ZQ, Zheng P, Xu ST, Wu X., Object detection with deep learning: A Review, IEEE Trans Neural Networks Learn Syst, 30, 11, pp. 3212-3232, (2019); Galoogahi HK, Sim T, Lucey S., Correlation filters with limited boundaries, Proc IEEE Comput Soc Conf Comput Vis Pattern Recognit, 7, 6, pp. 4630-4638, (2015); Hurtik P, Et al., Poly-YOLO: Higher speed, more precise detection and instance segmentation for YOLOv3, Neural Comput Appl, 34, 10, pp. 8275-8290, (2022); Zhang Q, Chang X, Bian SB., Vehicle-damage-detection segmentation algorithm based on improved Mask RCNN, IEEE Access, 8, pp. 6997-7004, (2020); Khan MA, Akram T, Zhang YD, Sharif M., Attributes based skin lesion detection and recognition: A mask RCNN and transfer learning-based deep learning framework, Pattern Recognit Lett, 143, pp. 58-66, (2021); Wang CY, Bochkovskiy A, Liao H-YM., YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors, (2022); Pham V, Pham C, Dang T., Road damage detection and classification with Detectron2 and Faster R-CNN, Proc-2020 IEEE Int Conf Big Data, Big Data 2020, pp. 5592-5601, (2020); Luo W, Et al., Multiple object tracking: A literature review, Artif Intell, 293, pp. 1-49, (2021); Mangalam K, Et al., It is not the journey but the destination: Endpoint conditioned trajectory prediction, Lect Notes Comput Sci (including Subser Lect Notes Artif Intell Lect Notes Bioinformatics), 12347, pp. 759-776, (2020); Galoogahi HK, Fagg A, Lucey S., Learning background-aware correlation filters for visual tracking, Proc IEEE Int Conf Comput Vis, 2017, 10, pp. 1144-1152, (2017); Wojke N, Bewley A, Paulus D., Simple online and realtime tracking with a deep association metric, 2017 IEEE International Conference on Image Processing (ICIP), pp. 3645-3649, (2017); Du Y, Song Y, Yang B, Zhao Y., StrongSORT: Make DeepSORT great again, (2022); Luo H, Et al., A strong baseline and batch normalization neck for deep person re-identification, IEEE Trans Multimed, 22, 10, pp. 2597-2609, (2020); Bruls T, Porav H, Kunze L, Newman P., The right (angled) perspective: Improving the understanding of road scenes using boosted inverse perspective mapping, IEEE Intell Veh Symp Proc, 2019, 6, pp. 302-309, (2019); Tanveer MH, Sgorbissa A., An inverse perspective mapping approach using monocular camera of pepper humanoid robot to determine the position of other moving robot in plane, ICINCO 2018-Proc 15th Int Conf Informatics Control Autom Robot, 2, pp. 219-225, (2018); Ivanovic B, Pavone M., Rethinking trajectory forecasting evaluation; Mahmud SMS, Luis FL, Hoque MS, Tavassoli A., Application of proximal surrogate indicators for safety evaluation: A review of recent developments and research needs, IATSS Research, 41, 4, pp. 153-163, (2017); Uno N, Iida Y, Itsubo S, Yasuhara S., A microscopic analysis of traffic conflict caused by lane-changing vehicle at weaving section, Proceedings of the 13th Mini-EURO Conference-Handling Uncertainty in the Analysis of Traffic and Transportation Systems, pp. 10-13, (2002); Allen BL, Shin TB, Cooper PJ., Analysis of traffic conflicts and collisions, (1978); Chen QH, Et al., Modeling accident risks in different lane-changing behavioral patterns, Analytic Methods in Accident Research, 30, (2021)","A.S.A. Mohamed; School of Computer Sciences, Universiti Sains, Malaysia; email: sufril@usm.my","","Faculty of Transport and Traffic Engineering","","","","","","03535320","","POMEE","","English","Promet Traffic Traffico","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85171261200"
"Bello R.-W.; Mohamed A.S.A.; Talib A.Z.","Bello, Rotimi-Williams (57209469141); Mohamed, Ahmad Sufril Azlan (57190968285); Talib, Abdullah Zawawi (35570816900)","57209469141; 57190968285; 35570816900","Enhanced mask r-cnn for herd segmentation","2021","International Journal of Agricultural and Biological Engineering","14","4","","238","244","6","6","10.25165/j.ijabe.20211404.6398","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113307773&doi=10.25165%2fj.ijabe.20211404.6398&partnerID=40&md5=28210c2841a67b66515b191f6c8e4e60","School of Computer Sciences, Universiti Sains Malaysia, Pulau Pinang, 11800, Malaysia; Department of Mathematics/Computer Sciences, University of Africa, Toru-Orua, Bayelsa State, Nigeria","Bello R.-W., School of Computer Sciences, Universiti Sains Malaysia, Pulau Pinang, 11800, Malaysia, Department of Mathematics/Computer Sciences, University of Africa, Toru-Orua, Bayelsa State, Nigeria; Mohamed A.S.A., School of Computer Sciences, Universiti Sains Malaysia, Pulau Pinang, 11800, Malaysia; Talib A.Z., School of Computer Sciences, Universiti Sains Malaysia, Pulau Pinang, 11800, Malaysia","Livestock image segmentation is an important task in the field of vision and image processing. Since utilizing the concentration of forage in the grazing area with shielding the surrounding farm plants and crops is necessary for making effective cattle ranch arrangements, there is a need for a segmentation method that can handle multiple objects segmentation. Moreover, the indistinct boundaries and irregular shapes of cattle bodies discourage the application of the existing Mask Region-based Convolutional Neural Network (Mask R-CNN) which was primarily modeled for the segmentation of natural images. To address this, an enhanced Mask R-CNN model was proposed for multiple objects instance segmentation to support indistinct boundaries and irregular shapes of cattle bodies for precision livestock farming. The contributions of this method are in multiple folds: 1) optimal filter size smaller than a residual network for extracting smaller and composite features; 2) region proposals for utilizing multiscale semantic features; 3) Mask R-CNN’s fully connected layer integrated with sub-network for an enhanced segmentation. The experiment conducted on pre-processed datasets produced a mean average precision (mAP) of 0.93, which was higher than the results from the existing state-of-the-art models. © 2021, Chinese Society of Agricultural Engineering. All rights reserved.","Enhancement; Herd; Image segmentation; Livestock farming; Mask R-CNN","","","","","","Division of Research and Innovation; RCMO; Universiti Sains Malaysia","The authors received funding from the Division of Research and Innovation (RCMO), Universiti Sains Malaysia for the publication of this work.","The future of livestock in Nigeria: Opportunities and challenges in the face of uncertainty, (2019); Bello R, Talib A, Mohamed A., Deep learning-based Architectures for recognition of cow using cow nose image pattern, Gazi University Journal of Science, 33, 3, pp. 831-844, (2020); Bello R W, Olubummo D A, Seiyaboh Z, Enuma O C, Talib A Z, Mohamed A S A., Cattle identification: the history of nose prints approach in brief, IOP Conference Series: Earth and Environmental Science, 594, 1, pp. 1-9, (2020); Shao W, Kawakami R, Yoshihashi R, You S, Kawase H, Naemura T., Cattle detection and counting in UAV images based on convolutional neural networks, International Journal of Remote Sensing, 41, 1, pp. 31-52, (2020); Mao Y, He D, Song H., Automatic detection of ruminant cows’ mouth area during rumination based on machine vision and video analysis technology, Int J Agric & Biol Eng, 12, 1, pp. 186-191, (2019); He D, Liu D, Zhao K., Review of perceiving animal information and behavior in precision livestock farming, Transaction of the CSAM, 47, pp. 231-244, (2016); Bos J M, Bovenkerk B, Feindt P H, Van Dam Y K., The quantified animal: Precision livestock farming and the ethical implications of objectification, Food Ethics, 2, pp. 77-92, (2018); Xudong Z, Xi K, Ningning F, Gang L., Automatic recognition of dairy cow mastitis from thermal images by a deep learning detector, Computers and Electronics in Agriculture, 178, pp. 1-11, (2020); Gomes R A, Monteiro G R, Assis G J F, Busato K C, Ladeira M M, Chizzotti M L., Estimating body weight and body composition of beef cattle trough digital image analysis, Journal of Animal Science, 94, 12, pp. 5414-5422, (2016); Bello R W, Abubakar S., Development of a software package for cattle identification in Nigeria, J. Appl. Sci. Environ. Manag, 23, 10, pp. 1825-1828, (2019); Hansen M F, Smith M L, Smith L N, Jabbar K A, Forbes D., Automated monitoring of dairy cow body condition, mobility and weight using a single 3d video capture device, Comput. Ind, 98, pp. 14-22, (2018); Zhou C, Lin K, Xu D, Liu J, Zhang S, Sun C, Et al., Method for segmentation of overlapping fish images in aquaculture, Int J Agric & Biol Eng, 12, 6, pp. 135-142, (2019); Xiao D, Feng A, Liu J., Detection and tracking of pigs in natural environments based on video analysis, Int J Agric & Biol Eng, 12, 4, pp. 116-126, (2019); Tebug S F, Missohou A, Sourokou S S, Juga J, Poole E J, Tapio M, Et al., Using body measurements to estimate live weight of dairy cattle in low-input systems in Senegal, J. Appl. Anim. Res, 46, pp. 87-93, (2018); Chen F E, Liang X M, Chen L H, Liu B Y, Lan Y B., Novel method for real-time detection and tracking of pig body and its different parts, Int J Agric & Biol Eng, 13, 6, pp. 144-149, (2020); Liu H, Reibman A R, Boerman J P., A cow structural model for video analytics of cow health, pp. 1-13, (2020); Ter-Sarkisov A, Ross R, Kelleher J, Earley B, Keane M., Beef cattle instance segmentation using fully convolutional neural network, pp. 1-11, (2018); Lyu S, Noguchi N, Ospina R, Kishima Y., Development of phenotyping system using low altitude UAV imagery and deep learning, Int J Agric & Biol Eng, 14, 1, pp. 207-215, (2021); Bello R W, Talib A Z, Mohamed A S A, Olubummo D A, Otobo F N., Image-based individual cow recognition using body patterns, International Journal of Advanced Computer Science and Applications, 11, 3, pp. 92-98, (2020); Salau J, Krieter J., Instance segmentation with Mask R-CNN applied to loose-housed dairy cows in a multi-camera setting, Animals, 10, 12, pp. 1-19, (2020); Bello R W, Talib A Z H, Mohamed A S A B., Deep belief network approach for recognition of cow using cow nose image pattern, Walailak Journal of Science and Technology, 18, 5, pp. 1-14, (2021); He K, Gkioxari G, Dollar P, Girshick R., Mask R-CNN, Proceedings of the IEEE International Conference on Computer Vision, pp. 2961-2969, (2017); Li K, Hariharan B, Malik J., Iterative instance segmentation, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 3659-3667, (2016); Zhang H, Tian Y, Wang K, Zhang W, Wang F Y., Mask SSD: An effective single-stage approach to object instance segmentation, IEEE Transactions on Image Processing, 29, 1, pp. 2078-2093, (2019); Pinheiro P O, Collobert R, Dollar P., Learning to segment object candidates, (2015); Pinheiro P O, Lin T Y, Collobert R, Dollar P., Learning to refine object segments, pp. 1-18, (2016); Ren S, He K, Girshick R, Sun J., Faster R-CNN: Towards real-time object detection with region proposal networks, IEEE transactions on pattern analysis and machine intelligence, 39, 6, pp. 1137-1149, (2016); Girshick R, Donahue J, Darrell T, Malik J., Region-based convolutional networks for accurate object detection and segmentation, IEEE Trans. Pattern Anal. Mach. Intell, 38, pp. 142-158, (2015); Girshick R., Fast R-CNN, Proceedings of the IEEE International Conference on Computer Vision, pp. 1440-1448, (2015); Bello R W, Mohamed A S A, Talib A Z., Contour extraction of individual cattle from an image using enhanced Mask R-CNN instance segmentation method, IEEE Access, 9, pp. 56984-57000, (2021); Chen L C, Hermans A, Papandreou G, Schroff F, Wang P, Adam H., Masklab: Instance segmentation by refining object detection with semantic and direction features, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 4013-4022, (2018); Zhao K, Kang J, Jung J, Sohn G., Building extraction from satellite images using mask R-CNN with building boundary regularization, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, pp. 247-251, (2018); Russell B C, Torralba A, Murphy K P, Freeman W T., Labelme: A database and web-based tool for image annotation, Int. J. Comput. Vision, 77, pp. 157-173, (2008); Lin T Y, Maire M, Belongie S, Hays J, Perona P, Ramanan D, Et al., Microsoft coco: Common objects in context, European Conference on Computer Vision, pp. 740-755, (2014); Abadi M, Barham P, Chen J, Chen Z, Davis A, Dean J, Et al., Tensorflow: A system for large-scale machine learning, 12th Symposium on Operating Systems Design and Implementation, pp. 265-283, (2016); Shelhamer E, Long J, Darrell T., Fully convolutional networks for semantic segmentation, IEEE Transactions on Pattern Analysis and Machine Intelligence, 39, 4, pp. 640-651, (2017); Redmon J, Farhadi A., YOLO9000: better, faster, stronger, IEEE Conference on Computer Vision and Pattern Recognition, pp. 6517-6525, (2017); Dai J, He K, Sun J., Instance-aware semantic segmentation via multi-task network cascades, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 3150-315, (2016); Simonyan K, Zisserman A., Very deep convolutional networks for large-scale image recognition, (2015)","A.S.A. Mohamed; School of Computer Sciences, Universiti Sains Malaysia, Pulau Pinang, 11800, Malaysia; email: sufril@usm.my","","Chinese Society of Agricultural Engineering","","","","","","19346344","","","","English","Int. J. Agric. Biol. Eng.","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85113307773"
"Muhammad A.; Addenan M.F.; Latiff M.M.; Haris B.; Surip S.S.; Mohamed A.S.A.","Muhammad, A. (57197347537); Addenan, M.F. (57201709304); Latiff, M.M. (57192170886); Haris, B. (57192171103); Surip, S.S. (57192173209); Mohamed, A.S.A. (57190968285)","57197347537; 57201709304; 57192170886; 57192171103; 57192173209; 57190968285","Interactive sign language interpreter using skeleton tracking","2016","Journal of Telecommunication, Electronic and Computer Engineering","8","6","","137","140","3","1","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84999114737&partnerID=40&md5=75c02ef77d06f94f371fb4d2555cc81f","School of Computer Sciences, Universiti Sains Malaysia, Penang, 11800, Malaysia; Faculty of Business and Management, Universiti Teknologi MARA, Shah Alam, Selangor, 40450, Malaysia; School of The Arts, Universiti Sains Malaysia, Penang, 11800, Malaysia","Muhammad A., School of Computer Sciences, Universiti Sains Malaysia, Penang, 11800, Malaysia; Addenan M.F., School of Computer Sciences, Universiti Sains Malaysia, Penang, 11800, Malaysia; Latiff M.M., School of Computer Sciences, Universiti Sains Malaysia, Penang, 11800, Malaysia; Haris B., Faculty of Business and Management, Universiti Teknologi MARA, Shah Alam, Selangor, 40450, Malaysia; Surip S.S., School of The Arts, Universiti Sains Malaysia, Penang, 11800, Malaysia; Mohamed A.S.A., School of Computer Sciences, Universiti Sains Malaysia, Penang, 11800, Malaysia","The aim of this paper is to introduce an interactive communication system that will benefit both people with hearing and verbal difficulties to convey in the form of the sign language naturally. The idea is to provide two ways of communication between two users by converting sign language to voice and text and provides means of returning communication feedback whereby the other party can speak or key-in text and translates it into sign language movement performed by a three-dimensional (3D) model. A Microsoft Kinect device is used to captures the sign movements by optimizing the skeleton tracking algorithm to understand specific hands movements and dictates using the pre-recorded gesture library to digitized voice and using the same apparatus, speech is translated back into sign language. Research leads in helping the disables have been carried out extensively and majority focuses on only using single type of motion sensing technology such as TOBII (eye tracking) and LEAP (leap motion) which are either costly or limited to a small workable space. Microsoft Kinect technology would be a genuinely equipment used to create a cost-effective and capable technology prototype that enables sign-language communication between signer and non-signer, thus, offers translation into Bahasa Malaysia text.","Kinect; Language translator; Motion; Sign language; Skeleton tracking","","","","","","","","Stokoe W.C., Casterline D.C., Croneberg C.G., A Dictionary of American Sign Language on Linguistic Principles, (1976); Stokoe W.C., Sign Language Structure [microform], (1978); Yin-Poole W., Source: MS Quadrupling Kinect Accuracy, (2010); Woodcock K., Fisher S.L., Occupational Health and Safety for Sign Lan-guage Interpreters, (2008); Huang F., Huang S., Interpreting American sign language with kinect, Journal of Deaf Studies and Deaf Education, (2011); Roy C., Metzger M., Researching signed language interpreting research through a sociolinguistic lens, The International Journal for Translation & Interpreting Research, (2014); Isard M., Black A., Condensation-conditional density propagation for visual tracking, International Journl of Computer Vision, 29, pp. 5-28, (1998); Sidenbladh H., Black M.J., Fleet D.J., Stochastic tracking of 3d human gestures using 2d image motion, European Conference on Computer Vision, (2000); Choo K., Fleet D., People tacking using hybrid monte carlo filtering, International conference on computer vision, (2001); Dong C., Leu M.C., Yin Z.Z., American sign language alphabet recognition using microsoft kinect computer vision, Pattern Recognition Workshops (CVPRW) 2015 IEEE Conference on, (2005)","","","Universiti Teknikal Malaysia Melaka","","","","","","21801843","","","","English","J. Telecommun. Electron. Comput. Eng.","Article","Final","","Scopus","2-s2.0-84999114737"
"Ab Wahab M.N.; Tay S.C.; Abdull Sukor A.S.; Mohamed A.S.A.; Mahinderjit Singh M.","Ab Wahab, Mohd Nadhir (57223397087); Tay, Shiek Chi (57286197800); Abdull Sukor, Abdul Syafiq (57209073616); Mohamed, Ahmad Sufril Azlan (57190968285); Mahinderjit Singh, Manmeet (57222728954)","57223397087; 57286197800; 57209073616; 57190968285; 57222728954","Smart Waste Management System","2022","Lecture Notes in Electrical Engineering","770","","","713","724","11","2","10.1007/978-981-16-2406-3_55","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116481281&doi=10.1007%2f978-981-16-2406-3_55&partnerID=40&md5=05464e85298a65e8a91329955aa26716","School of Computer Science, Universiti Sains Malaysia, Minden, Pulau Pinang, 11800, Malaysia; Faculty of Mechanical Engineering Technology, Universiti Malaysia Perlis (UniMAP), Arau, Perlis, 02600, Malaysia","Ab Wahab M.N., School of Computer Science, Universiti Sains Malaysia, Minden, Pulau Pinang, 11800, Malaysia; Tay S.C., School of Computer Science, Universiti Sains Malaysia, Minden, Pulau Pinang, 11800, Malaysia; Abdull Sukor A.S., Faculty of Mechanical Engineering Technology, Universiti Malaysia Perlis (UniMAP), Arau, Perlis, 02600, Malaysia; Mohamed A.S.A., School of Computer Science, Universiti Sains Malaysia, Minden, Pulau Pinang, 11800, Malaysia; Mahinderjit Singh M., School of Computer Science, Universiti Sains Malaysia, Minden, Pulau Pinang, 11800, Malaysia","The increasing amount of waste in landfill has created a serious environmental problem which demands a more reliable solution in handling the collection of wastes. To this date, recycling is one of the solutions to manage the waste as it collects and processes recyclable materials into new products instead of throwing the trash to the landfill. However, the consciousness of recycling in our society is still devastatingly lower than expected as people are faced with many challenges that impede them to recycle. One of the challenges is to segregate the waste according to its group. People are still having difficulty to clearly distinguish recyclable materials due to the lack of recycling knowledge. Thus, this paper aims to develop a system that can separate the waste automatically and channel them to the proper bins. To do that, a camera is used to capture the image of the waste. Then, image classification using deep learning model is used to classify different types of wastes. The developed model is then embedded in Raspberry Pi and a servo motor is used to direct the waste to the respective bins for real-world implementation. Experimental results show that the proposed system can identify the categories of waste within the accuracy of 77–85%. This system is expected to deliver the importance of recycling and cultivate recycling practices to the public and finally reduced waste generation on land. © 2022, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.","Deep learning; Image recognition; Waste management; Waste sorting","Bins; Deep learning; Land fill; Recycling; Deep learning; Developed model; Environmental problems; Images classification; Learning models; Real-world implementation; Recyclable material; Servo-motor; Waste management systems; Waste sorting; Image recognition","","","","","SAHOMASI; Universiti Sains Malaysia; Universiti Malaysia Perlis","Acknowledgements This project is supported by USM Short Term Grant (PKOMP/6315262), implemented in Smart Home Using MEM Sensors and Ambient Intelligence Lab (SAHOMASI), Universiti Sains Malaysia (USM) and part of the collaboration projects between Robotics, Computer Vision, and Image Processing (RCVIP) Research Group of Universiti Sains Malaysia (USM) and Centre of Excellence for Advanced Sensor Technology (CEASTech), Universiti Malaysia Perlis (UniMAP).","Pardini K., Rodrigues J.J.P.C., Diallo O., Das A.K., de Albuquerque V.H.C., Kozlov S.A., A smart waste management solution geared towards citizens, Sensors, 20, 8, pp. 1-15, (2020); Omar M.F., Termizi A.A.A., Zainal D., Wahap N.A., Ismail N.M., Ahmad N., Implementation of spatial smart waste management system in Malaysia, IOP Conference Series: Earth and Environmental Science 2016, 37, pp. 1-9, (2016); Sin T.J., Chen G.K., Long K.S., Goh I., Hwang H., Current practice of waste management system in Malaysia: Towards sustainable waste management, FPTP Postgraduate Seminar Towards Sustainable Management 2013, 1106, pp. 1-19, (2013); Ahsan A., Alamgir M., El-Sergany M.M., Shams S., Rowshon M.K., Daud N.N.N., Assessment of municipal solid waste management system in a developing country, Chin J Eng, 2014, 56, pp. 1-11, (2014); Sarbassov Y., Sagalova T., Tursunov O., Venetis C., Xenarios S., Inglezakis V., Survey on household solid waste sorting at source in developing economies: A case study of Nur-Sultan City in Kazakhstan, Sustain, 11, 22, pp. 1-17, (2019); Rohan V.V.J., Bhagavat G., Pradip B., Shivam B., Waste segregation using smart robotic arm, Int Res J Eng Technol, 2, 1, pp. 1833-1835, (2017); Malinauskaite J., Jouhara H., Czajczynska D., Stanchev P., Katsou E., Rostkowski P., Thorne R.J., Colon J., Ponsa S., Al-Mansour F., Anguilano L., Krzyzynska R., Lopez I.C., Municipal waste management systems for domestic use, Energy, 141, c, pp. 2013-2044, (2017); Nasee N., Shokar A., Afza G., Mazhr N., Smart solid waste management system, International Conference on Technology and Entrepreneurship in Digital Society, 2019, pp. 1-5, (2019); Sukor A.S.A., Zakaria A., Rahim N.A., Setchi R., Semantic knowledge base in support of activity recognition in smart home environments, Int J Eng Technol, 7, 4, pp. 67-72, (2018); Joseph J., Smart waste management using deep learning with IoT, Int J Netw Syst, 8, 3, pp. 37-40, (2019); Sukor A.S.A., Zakaria A., Rahim N.A., Kamarudin L.M., Nishizaki H., Abnormality detection approach using deep learning models in smart home environments, Proceedings of the 7Th International Conference on Communications and Broadband Networking, pp. 22-27, (2019); Wahab M.N.A., Mohamed A.S.A., Sukor A.S.A., Teng O.C., Text reader for visually impaired person, International Conference on Electronic Design (ICED, 2020, pp. 1-6, (2020); Chu Y., Huang C., Xie X., Tan B., Kamal S., Xiong X., Multilayer hybrid deep-learning method for waste classification and recycling, Comput Intell Neurosci, 2018, pp. 1-9, (2018); Rutqvist D., Kleyko D., Blomstedt F., An automated machine learning approach for smart waste management systems, IEEE Trans Ind Informatics, 16, 1, pp. 384-392, (2020)","M.N. Ab Wahab; School of Computer Science, Universiti Sains Malaysia, Minden, Pulau Pinang, 11800, Malaysia; email: mohdnadhir@usm.my","Isa K.; Md. Zain Z.; Mohd-Mokhtar R.; Mat Noh M.; Ismail Z.H.; Yusof A.A.; Mohamad Ayob A.F.; Azhar Ali S.S.; Abdul Kadir H.","Springer Science and Business Media Deutschland GmbH","","12th National Technical Seminar on Unmanned System Technology, NUSYS 2020","24 November 2020 through 25 November 2020","Virtual, Online","266059","18761100","978-981162405-6","","","English","Lect. Notes Electr. Eng.","Conference paper","Final","","Scopus","2-s2.0-85116481281"
"Sumari P.; Wan Ahmad W.M.A.; Hadi F.; Mazlan M.; Liyana N.A.; Bello R.-W.; Mohamed A.S.A.; Talib A.Z.","Sumari, Putra (6602619198); Wan Ahmad, Wan Muhammad Azimuddin (57359018500); Hadi, Faris (57358897800); Mazlan, Muhammad (57358897900); Liyana, Nur Anis (57359018600); Bello, Rotimi-Williams (57209469141); Mohamed, Ahmad Sufril Azlan (57190968285); Talib, Abdullah Zawawi (35570816900)","6602619198; 57359018500; 57358897800; 57358897900; 57359018600; 57209469141; 57190968285; 35570816900","A Precision Agricultural Application: Manggis Fruit Classification Using Hybrid Deep Learning","2021","Revue d'Intelligence Artificielle","35","5","","375","381","6","1","10.18280/ria.350503","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120428842&doi=10.18280%2fria.350503&partnerID=40&md5=8b2d9c6b1deb77064beecaa143125e38","School of Computer Sciences, Universiti Sains Malaysia, Pulau Pinang, 11800, Malaysia; Department of Mathematics/Computer Sciences, University of Africa, Toru-Orua, Bayelsa State, 561101, Nigeria","Sumari P., School of Computer Sciences, Universiti Sains Malaysia, Pulau Pinang, 11800, Malaysia; Wan Ahmad W.M.A., School of Computer Sciences, Universiti Sains Malaysia, Pulau Pinang, 11800, Malaysia; Hadi F., School of Computer Sciences, Universiti Sains Malaysia, Pulau Pinang, 11800, Malaysia; Mazlan M., School of Computer Sciences, Universiti Sains Malaysia, Pulau Pinang, 11800, Malaysia; Liyana N.A., School of Computer Sciences, Universiti Sains Malaysia, Pulau Pinang, 11800, Malaysia; Bello R.-W., School of Computer Sciences, Universiti Sains Malaysia, Pulau Pinang, 11800, Malaysia, Department of Mathematics/Computer Sciences, University of Africa, Toru-Orua, Bayelsa State, 561101, Nigeria; Mohamed A.S.A., School of Computer Sciences, Universiti Sains Malaysia, Pulau Pinang, 11800, Malaysia; Talib A.Z., School of Computer Sciences, Universiti Sains Malaysia, Pulau Pinang, 11800, Malaysia","Fruits come in different variants and subspecies. While some subspecies of fruits can be easily differentiated, others may require an expertness to differentiate them. Although farmers rely on the traditional methods to identify and classify fruit types, the methods are prone to so many challenges. Training a machine to identify and classify fruit types in place of traditional methods can ensure precision fruit classification. By taking advantage of the state-of-the-art image recognition techniques, we approach fruits classification from another perspective by proposing a high performing hybrid deep learning which could ensure precision mangosteen fruit classification. This involves a proposed optimized Convolutional Neural Network (CNN) model compared to other optimized models such as Xception, VGG16, and ResNet50 using Adam, RMSprop, Adagrad, and Stochastic Gradient Descent (SGD) optimizers on specified dense layers and filters numbers. The proposed CNN model has three types of layers that make up its model, they are: 1) the convolutional layers, 2) the pooling layers, and 3) the fully connected (FC) layers. The first convolution layer uses convolution filters with a filter size of 3x3 used for initializing the neural network with some weights prior to updating to a better value for each iteration. The CNN architecture is formed from stacking these layers. Our self-acquired dataset which is composed of four different types of Malaysian mangosteen fruit, namely Manggis Hutan, Manggis Mesta, Manggis Putih and Manggis Ungu was employed for the training and testing of the proposed CNN model. The proposed CNN model achieved 94.99% classification accuracy higher than the optimized Xception model which achieved 90.62% accuracy in the second position. © 2021 Lavoisier. All rights reserved.","CNN; Hybrid deep learning; Mangosteen fruit; Resnet; SGD; Transfer learning; VGG16; Xception","","","","","","","","Mettleq A.S.A., Dheir I.M., Elsharif A.A., Abu-Naser S.S., Mango classification using deep learning, International Journal of Academic Engineering Research, 3, 12, pp. 22-29, (2020); Gullapelly A., Banik B.G., Classification of rigid and non-rigid objects using CNN, Revue d'Intelligence Artificielle, 35, 4, pp. 341-347, (2021); Thenmozhi K., Reddy U.S., Crop pest classification based on deep convolutional neural network and transfer learning, Computers and Electronics in Agriculture, 164, (2019); Azizah L.M.R., Umayah S.F., Riyadi S., Damarjati C., Utama N.A., Deep learning implementation using convolutional neural network in mangosteen surface defect detection, 7th IEEE International Conference on Control System, Computing and Engineering, pp. 242-246, (2017); Bello R.W., Mohamed A.S.A., Talib A.Z., Enhanced mask R-CNN for herd segmentation, International Journal of Agricultural and Biological Engineering, 14, 4, pp. 238-244, (2021); Femling F., Olsson A., Alonso-Fernandez F., Fruit and vegetable identification using machine learning for retail applications, 14th International Conference on Signal-Image Technology & Internet-Based Systems, pp. 9-15, (2018); Chollet F., Xception: Deep learning with depthwise separable convolutions, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 1251-1258, (2017); Simonyan K., Zisserman A., Very deep convolutional networks for large-scale image recognition, (2014); Gupta U., Detailed guide to understand and implement ResNets, (2017); Alkan A., Abdullah M.U., Abdullah H.O., Assaf M., Zhou H., A smart agricultural application: automated detection of diseases in vine leaves using hybrid deep learning, Turkish Journal of Agriculture and Forestry, 45, pp. 1-13, (2021); Paranavithana I.R., Kalansuriya V.R., Deep convolutional neural network model for tea bud(s) classification, IAENG International Journal of Computer Science, 48, 3, pp. 1-6, (2021); Nasir I.M., Bibi A., Sha J.H., Khan M.A., Sharif M., Iqbal K., Nam Y., Kadry S., Deep learning-based classification of fruit diseases: An application for precision agriculture, CMC-Computers Materials and Continua, 66, 2, pp. 1949-1962, (2021); Palakodati S.S.S., Chirra V.R., Yakobu D., Bulla S., Fresh and rotten fruits classification using CNN and transfer learning, Revue d'Intelligence Artificielle, 34, 5, pp. 617-622, (2021)","P. Sumari; School of Computer Sciences, Universiti Sains Malaysia, Pulau Pinang, 11800, Malaysia; email: putras@usm.my","","International Information and Engineering Technology Association","","","","","","0992499X","","","","English","Rev. Intell. Artif.","Article","Final","All Open Access; Bronze Open Access","Scopus","2-s2.0-85120428842"
"Yunus M.N.H.; Jaafar M.H.; Mohamed A.S.A.; Azraai N.Z.; Amil N.; Zein R.M.","Yunus, Muhamad Nurul Hisyam (57226543249); Jaafar, Mohd Hafiidz (57210229668); Mohamed, Ahmad Sufril Azlan (57190968285); Azraai, Nur Zaidi (57191244519); Amil, Norhaniza (56891475100); Zein, Remy Md (57959512600)","57226543249; 57210229668; 57190968285; 57191244519; 56891475100; 57959512600","Biomechanics Analysis of the Firefighters’ Thorax Movement on Personal Protective Equipment during Lifting Task Using Inertial Measurement Unit Motion Capture","2022","International Journal of Environmental Research and Public Health","19","21","14232","","","","0","10.3390/ijerph192114232","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141636988&doi=10.3390%2fijerph192114232&partnerID=40&md5=6c9aa20257399b6be3d2bfdc0a124a5c","School of Industrial Technology, Universiti Sains Malaysia (USM), Penang 11800, Malaysia; School of Computer Sciences, Universiti Sains Malaysia (USM), Penang 11800, Malaysia; School of the Arts, Universiti Sains Malaysia (USM), Penang 11800, Malaysia; National Institute of Occupational Safety and Health (NIOSH), Bangi, 43650, Malaysia","Yunus M.N.H., School of Industrial Technology, Universiti Sains Malaysia (USM), Penang 11800, Malaysia; Jaafar M.H., School of Industrial Technology, Universiti Sains Malaysia (USM), Penang 11800, Malaysia; Mohamed A.S.A., School of Computer Sciences, Universiti Sains Malaysia (USM), Penang 11800, Malaysia; Azraai N.Z., School of the Arts, Universiti Sains Malaysia (USM), Penang 11800, Malaysia; Amil N., School of Industrial Technology, Universiti Sains Malaysia (USM), Penang 11800, Malaysia; Zein R.M., National Institute of Occupational Safety and Health (NIOSH), Bangi, 43650, Malaysia","Back injury is a common musculoskeletal injury reported among firefighters (FFs) due to their nature of work and personal protective equipment (PPE). The nature of the work associated with heavy lifting tasks increases FFs’ risk of back injury. This study aimed to assess the biomechanics movement of FFs on personal protective equipment during a lifting task. A set of questionnaires was used to identify the prevalence of musculoskeletal pain experienced by FFs. Inertial measurement unit (IMU) motion capture was used in this study to record the body angle deviation and angular acceleration of FFs’ thorax extension. The descriptive analysis was used to analyze the relationship between the FFs’ age and body mass index with the FFs’ thorax movement during the lifting task with PPE and without PPE. Sixty-three percent of FFs reported lower back pain during work, based on the musculoskeletal pain questionnaire. The biomechanics analysis of thorax angle deviation and angular acceleration has shown that using FFs PPE significantly causes restricted movement and limited mobility for the FFs. As regards human factors, the FFs’ age influences the angle deviation while wearing PPE and FFs’ BMI influences the angular acceleration without wearing PPE during the lifting activity. © 2022 by the authors.","angular kinematic; biomechanics; ergonomic risk assessment; motion capture; personal protective equipment","Back Injuries; Biomechanical Phenomena; Firefighters; Humans; Lifting; Musculoskeletal Pain; Personal Protective Equipment; Thorax; biomechanics; equipment; injury; movement; physical activity; posture; risk assessment; adult; age; Article; biomechanics; body composition; body mass; clinical article; controlled study; descriptive research; ergonomics; fire fighter; human; kinematics; lifting effort; limited mobility; low back pain; male; middle aged; musculoskeletal pain; physical parameters; prevalence; questionnaire; risk assessment; thorax examination; young adult; biomechanics; building; injury; musculoskeletal pain; protective equipment; thorax","","","","","Ministry of Higher Education, Malaysia, MOHE, (FRGS/1/2019/STG02/USM/03/2)","This research was funded by the Ministry of Higher Education, Malaysia, under the Fundamental Research Grant Scheme with the reference number FRGS/1/2019/STG02/USM/03/2.","Kim M.G., Kim K.S., Ryoo J.H., Yoo S.W., Relationship between Occupational Stress and Work-related Musculoskeletal Disorders in Korean Male Firefighters, Ann. Occup. Environ. Med, 25, pp. 1-7, (2013); Campbell R.U.S., Firefighter injuries on the fireground, 2010–2014, Fire Technol, 54, pp. 461-477, (2018); DeJoy D.M., Smith T.D., Dyal M.A., Safety climate and firefighting: Focus group results, J. Safety Res, 62, pp. 107-116, (2017); Kim M.G., Seo J il Kim K., Ahn Y., Nationwide firefighter survey: The prevalence of lower back pain and its related psychological factors among Korean Firefighters, Int. J. Occup. Saf. Ergon, 23, pp. 447-456, (2017); Kahn S.A., Palmieri T.L., Sen S., Woods J., Gunter O.L., Factors Implicated in Safety-related Firefighter Fatalities, J. Burn Care Res, 38, pp. e83-e88, (2017); Le A.B., Smith T.D., McNulty L.A., Dyal M.A., Dejoy D.M., Firefighter overexertion: A continuing problem found in an analysis of non-fatal injury among career firefighters, Int. J. Environ. Res. Public Health, 17, (2020); Orr R., Simas V., Canetti E., Schram B., A profile of injuries sustained by firefighters: A critical review, Int. J. Environ. Res. Public Health, 16, (2019); McQuerry M., DenHartog E., Barker R., Impact of reinforcements on heat stress in structural firefighter turnout suits, J. Text. Inst, 109, pp. 1367-1373, (2018); Lee J.Y., Park J., Park H., Coca A., Kim J.H., Taylor N.A., Son S.Y., Tochihara Y., What do firefighters desire from the next generation of personal protective equipment? Outcomes from an international survey, Ind. Health, 53, pp. 434-444, (2015); Wang S., Park J., Wang Y., Cross-cultural comparison of firefighters’ perception of mobility and occupational injury risks associated with personal protective equipment, Int. J. Occup. Saf. Ergon, 27, pp. 664-672, (2021); Park K., Rosengren K.S., Horn G.P., Smith D.L., Hsiao-Wecksler E.T., Assessing gait changes in firefighters due to fatigue and protective clothing, Saf. Sci, 49, pp. 719-726, (2011); Sobeih T.M., Davis K.G., Succop P.A., Jetter W.A., Bhattacharya A., Postural balance changes in on-duty firefighters: Effect of gear and long work shifts, J. Occup. Environ. Med, 48, pp. 68-75, (2006); Szubert Z., Sobala W., Work-related injuries among firefighters: Sites and circumstances of their occurrence, Int. J. Occup. Med. Environ. Health, 15, pp. 49-55, (2002); Soteriades E.S., Psalta L., Leka S., Spanoudis G., Occupational stress and musculoskeletal symptoms in firefighters, Int. J. Occup. Environ. Med, 32, pp. 341-352, (2019); Frost D.M., Beach T.A.C., Crosby I., McGill S.M., Firefighter injuries are not just a fireground problem, Work, 52, pp. 835-842, (2015); Lavender S.A., Sommerich C.M., Bigelow S., Weston E.B., Seagren K., Pay N.A., Sillars D., Ramachandran V., Sun C., Xu Y., Et al., A biomechanical evaluation of potential ergonomic solutions for use by firefighter and EMS providers when lifting heavy patients in their homes, Appl. Ergon, 82, pp. 102910-102918, (2020); Vahdat I., Rostami M., Tabatabai Ghomsheh F., Khorramymehr S., Tanbakoosaz A., Effects of external loading on lumbar extension moment during squat lifting, Int. J. Occup. Med. Environ. Health, 30, (2017); Weston E.B., Aurand A., Dufour J.S., Knapik G.G., Marras W.S., Biomechanically determined hand force limits protecting the low back during occupational pushing and pulling tasks, Ergonomics, 61, pp. 853-865, (2018); Kamat S.R., Md Zula N.E.N., Rayme N.S., Shamsuddin S., Husain K., The ergonomics body posture on repetitive and heavy lifting activities of workers in aerospace manufacturing warehouse, IOP Conf. Ser. Mater. Sci. Eng, 210, pp. 1-12, (2017); Hlavenka T.M., Christner V.F.K., Gregory D.E., Neck posture during lifting and its effect on trunk muscle activation and lumbar spine posture, Appl. Ergon, 62, pp. 28-33, (2017); Nath N.D., Akhavian R., Behzadan A.H., Ergonomic analysis of construction worker ’ s body postures using wearable mobile sensors, Appl. Ergon, 62, pp. 107-117, (2017); Yunus M.N.H., Jaafar M.H., Mohamed A.S.A., Azraai N.Z., Hossain M.S., Implementation of kinetic and kinematic variables in ergonomic risk assessment using motion capture simulation: A review, Int. J. Environ. Res. Public Health, 18, (2021); Nourollahi M., Afshari D., Dianat I., Awkward trunk postures and their relationship with low back pain in hospital nurses, Work, 59, pp. 317-323, (2018); Nott C.R., Zajac F.E., Neptune R.R., Kautz S.A., All joint moments significantly contribute to trunk angular acceleration, J. Biomech, 43, pp. 2648-2652, (2010); Fleron M.K., Ubbesen N.C.H., Battistella F., Dejtiar D.L., Oliveira A.S., Accuracy between optical and inertial motion capture systems for assessing trunk speed during preferred gait and transition periods, Sports Biomech, 18, pp. 366-377, (2019); Ribeiro T.H., Vieira M.L.H., Motion Capture Technology—Benefits and Challenges, Int. J. Innov. Res. Technol. Sci, 48, pp. 1156-2321, (2016); Bortolini M., Gamberi M., Pilati F., Regattieri A., Automatic assessment of the ergonomic risk for manual manufacturing and assembly activities through optical motion capture technology, Procedia CIRP, 72, pp. 81-96, (2018); Akhavian R., Behzadan A.H., Smartphone-based construction workers’ activity recognition and classification, Autom. Constr, 71, pp. 198-209, (2016); Park H., Park J., Lin S.H., Boorady L.M., Assessment of firefighters’ needs for personal protective equipment, Fash. Text, 1, pp. 1-13, (2014); Brown M.N., Char R.M., Henry S.O., Tanigawa J., Yasui S., The effect of firefighter personal protective equipment on static and dynamic balance dynamic balance, Ergonomics, 62, pp. 1193-1201, (2019); Park H., Kakar R.S., Pei J., Tome J.M., Stull J., Impact of Size of Fire boot and SCBA Cylinder on firefighters’ Mobility, Cloth. Text. Res. J, 37, pp. 1-16, (2019); Guidelines on Ergonomic Risk Assessment at Workplace, (2017); Mihcin S., Ciklacandir S., Kocak M., Tosun A., Wearable motion capture system evaluation for biomechanical studies for hip joints, J. Biomech. Eng, 143, pp. 1-15, (2021); Huysamen K., Power V., O'Sullivan L., Kinematic and kinetic functional requirements for industrial exoskeletons for lifting tasks and overhead lifting, Ergonomics, 63, pp. 818-830, (2020); Greenland K.O., Merryweather A.S., Bloswick D.S., The effect of lifting speed on cumulative and peak biomechanical loading for symmetric lifting tasks, Saf. Health Work, 4, pp. 105-110, (2013); Marras W.S., Lavender S.A., Leurgans S.E., Fathallah F.A., Ferguson S.A., Gary Allread W., Rajulu S.L., Biomechanical risk factors for occupationally related low back disorders, Ergonomics, 38, pp. 377-410, (1995); Hamm K., Angular Kinematic, Biomechanics of Human Movement, pp. 227-270, (2016); Zhao Y.S., Jaafar M.H., Mohamed A.S.A., Azraai N.Z., Amil N., Ergonomics Risk Assessment for Manual Material Handling of Warehouse Activities Involving High Shelf and Low Shelf Binning Processes: Application of Marker-Based Motion Capture, Sustainability, 14, (2022); Zhang T.T., Liu Z., Liu Y.L., Zhao J.J., Liu D.W., Tian Q.B., Obesity as a Risk Factor for Low Back Pain, Clin. Spine Surg, 31, pp. 22-27, (2018); Deros B.M., Daruis D.D.I., Basir I.M., A study on ergonomic awareness among workers performing manual material handling activities, Procedia-Soc. Behav. Sci, 195, pp. 1666-1673, (2015); Vieira E.R., Kumar S., Working postures: A literature review, J. Occup. Rehabil, 14, pp. 143-159, (2004); Herzog V.N., Buchmeister B., The Review of Ergonomics Analysis for Body Postures Assessment, DAAAM International Scientific Book, pp. 153-164, (2015); Brents C., Hischke M., Reiser R., Rosecrance J., Trunk posture during manual materials handling of beer kegs, Int. J. Environ. Res. Public Health, 18, (2021); Ulrey B.L., Fathallah F.A., Effect of a personal weight transfer device on muscle activities and joint flexions in the stooped posture, J. Electromyogr. Kinesiol, 23, pp. 195-205, (2013); Anita A.R., Yazdani A., Hayati K.S., Adon M.Y., Association between awkward posture and musculoskeletal disorders (MSD) among assembly line workers in an automotive industry, Malaysian J. Med. Health Sci, 10, pp. 23-28, (2014); Antwi-Afari M.F., Li H., Luo X.E., Edwards D.J., Owusu-Manu D., Darko A., Overexertion-related construction workers’ activity recognition and ergonomic risk assessment based on wearable insole pressure system, Proceedings of the West Africa Built Environment Research Conference, pp. 788-796; Cutlip K., Nimbarte A.D., Chowdhury S.K., Jaridi M., Evaluation of Shoulder Stability During Forceful Arm Exertions, Ind. Syst. Eng. Rev, 3, pp. 49-58, (2015); Kesler R.M., Bradley F.F., Deetjen G.S., Angelini M.J., Petrucci M.N., Rosengren K.S., Horn G.P., Hsiao-Wecksler E.T., Impact of SCBA Size and Fatigue from Different Firefighting Work Cycles on firefighter Gait, Ergonomics, 61, pp. 1208-1215, (2018); Waddell M.L., Amazeen E.L., Lift speed moderates the effects of muscle activity on perceived heaviness, Q. J. Exp. Psychol, 71, pp. 2174-2185, (2016); Negm A., MacDermid J., Sinden K., D'Amico R., Lomotan M., MacIntyre N.J., Prevalence and distribution of musculoskeletal disorders in firefighters are influenced by age and length of service, J. Mil. Veteran Fam. Health, 3, pp. 33-41, (2017); Punakallio A., Balance abilities of workers in physically demanding jobs: With special reference to firefighters of different ages, J. Sports Sci. Med, 4, pp. 1-47, (2005); Skovlund S.V., Blafoss R., Sundstrup E., Andersen L.L., Association between physical work demands and work ability in workers with musculoskeletal pain: Cross-sectional study, BMC Musculoskelet. Disord, 21, (2020); Tsang S.M.H., Szeto G.P.Y., Li L.M.K., Wong D.C.M., Yip M.M.P., Lee R.Y.W., The effects of bending speed on the lumbo-pelvic kinematics and movement pattern during forward bending in people with and without low back pain, BMC Musculoskelet. Disord, 18, (2017); Yoon J., Shiekhzadeh A., Nordin M., The effect of load weight vs. pace on muscle recruitment during lifting, Appl. Ergon, 43, pp. 1044-1050, (2012); Pasdar Y., Darbandi M., Mirtaher E., Rezaeian S., Najafi F., Hamzeh B., Associations between Muscle Strength with Different Measures of Obesity and Lipid Profiles in Men and Women: Results from RaNCD Cohort Study, Clin. Nutr. Res, 8, pp. 148-158, (2019)","M.H. Jaafar; School of Industrial Technology, Universiti Sains Malaysia (USM), Penang 11800, Malaysia; email: mhafiidz@usm.my","","MDPI","","","","","","16617827","","","36361112","English","Int. J. Environ. Res. Public Health","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85141636988"
"Yang L.; Mohamed A.S.A.; Ali M.K.M.","Yang, Lu (58514779800); Mohamed, Ahmad Sufril Azlan (57190968285); Ali, Majid Khan Majahar (57190937034)","58514779800; 57190968285; 57190937034","Traffic Conflicts Analysis in Penang Based on Improved Object Detection With Transformer Model","2023","IEEE Access","11","","","84061","84073","12","0","10.1109/ACCESS.2023.3299316","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85166319801&doi=10.1109%2fACCESS.2023.3299316&partnerID=40&md5=5bd9be02e55a27481601a3963bb4eec6","Universiti Sains Malaysia, George Town, School of Computer Sciences, Penang, 11800, Malaysia","Yang L., Universiti Sains Malaysia, George Town, School of Computer Sciences, Penang, 11800, Malaysia; Mohamed A.S.A., Universiti Sains Malaysia, George Town, School of Computer Sciences, Penang, 11800, Malaysia; Ali M.K.M., Universiti Sains Malaysia, George Town, School of Computer Sciences, Penang, 11800, Malaysia","Current road safety detection and analysis tends to focus on hit-and-run accidents that have already occurred, while ignoring near-misses that may pose a potential safety risk. A monitoring method for near-misses based on the improved YOLOv7 of transformers is proposed in this work. First, the backbone network of YOLOv7 is improved using a G3HN structure ( gn Conv) with recursive gate convolution. Second, the global attention mechanism (GAM) is added to the probe to improve recognition accuracy. Finally, the object detection results are inverted by inverse perspective mapping (IPM) to obtain the centroid of the object, and then the probability of near miss is calculated and analyzed using the DN-based, PICUD-based, and PSD-based methods. This experiment was based on the POL37 Closed-Circuit Television (CCTV) dataset from Penang, Malaysia. The experimental results show that the improved algorithm proposed in this paper can effectively identify small targets in the object detection phase with a detection accuracy of 94.8%, smaller models, and faster training speed, and can fulfill the surveillance task of near-miss events in CCTV surveillance scenes.  © 2013 IEEE.","G3HN; near-miss events; object detection; transformer mechanism","Accidents; Inverse problems; Job analysis; Object detection; Object recognition; Risk assessment; Closed circuit television; Computational modelling; G3HN; Near-miss event; Near-misses; Objects detection; Task analysis; Transformer; Transformer mechanism; Convolution","","","","","","","Heinrich H.W., Industrial Accident Prevention: A Scientific Approach, (1941); Park J.-I., Kim S., Kim J.-K., Exploring spatial associations between near-miss and police-reported crashes: The Heinrich’s law in traffic safety, Transp. Res. Interdiscipl. Perspect., 19, (2023); Ye Y., Fu L., Li B., Object detection and tracking using multi-layer laser for autonomous urban driving, Proc. IEEE 19th Int. Conf. Intell. Transp. Syst. (ITSC), pp. 259-264, (2016); Khorramshahi P., Shenoy V., Pack M., Chellappa R., Scalable and real-time multi-camera vehicle detection, re-identification, and tracking; Mandal V., Adu-Gyamfi Y., Object detection and tracking algorithms for vehicle counting: A comparative analysis, J. Big Data Anal. Transp., 2, 3, pp. 251-261; Lee H., Yoon J., Jeong Y., Yi K., Moving object detection and tracking based on interaction of static obstacle map and geometric model-free approachfor urban autonomous driving, IEEE Trans. Intell. Transp. Syst., 22, 6, pp. 3275-3284, (2021); Cho H., Seo Y.-W., Kumar B.V.K.V., Rajkumar R.R., A multisensor fusion system for moving object detection and tracking in urban driving environments, Proc. IEEE Int. Conf. Robot. Autom. (ICRA), pp. 1836-1843, (2014); Azimjonov J., Ozmen A., A real-time vehicle detection and a novel vehicle tracking systems for estimating and monitoring traffic flow on highways, Adv. Eng. Informat., 50, 2021; Gu F., Lu J., Cai C., RPformer: A robust parallel transformer for visual tracking in complex scenes, IEEE Trans. Instrum. Meas., 71, pp. 1-14; Yuan D., Chang X., Liu Q., Yang Y., Wang D., Shu M., He Z., Shi G., Active learning for deep visual tracking, IEEE Trans. Neural Netw. Learn. Syst.; Yuan D., Chang X., Li Z., He Z., Learning adaptive spatial–temporal context-aware correlation filters for UAV tracking, ACM Trans. Multimedia Comput., Commun., Appl., 18, 3, pp. 1-18, (2022); Gu F., Lu J., Cai C., A robust attention-enhanced network with transformer for visual tracking, Multimedia Tools Appl, pp. 1-22; Zhao Z., Yang X., Zhou Y., Sun Q., Ge Z., Liu D., Real-time detection of particleboard surface defects based on improved YOLOv5 target detection, Sci. Rep., 11, 1, (2021); Guo G., Zhang Z., Road damage detection algorithm for improved YOLOv5, Sci. Rep., 12, 1, pp. 1-12; Yang L., Zhang R.-Y., Li L., Xie X., SimAM: A simple, parameter-free attention module for convolutional neural networks, Proc. Int. Conf. Mach. Learn., 2021, pp. 11863-11874; Liu W., Quijano K., Crawford M.M., YOLOv5-tassel: Detecting tassels in RGB UAV imagery with improved YOLOv5 based on transfer learning, IEEE J. Sel. Topics Appl. Earth Observ. Remote Sens., 15, pp. 8085-8094; Liu Z., Lin Y., Cao Y., Hu H., Wei Y., Zhang Z., Lin S., Guo B., Swin transformer: Hierarchical vision transformer using shifted windows; Liu Z., Mao H., Wu C.-Y., Feichtenhofer C., Darrell T., Xie S., A ConvNet for the 2020s, Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), pp. 11976-11986, (2022); Li Y., Yao T., Pan Y., Mei T., Contextual transformer networks for visual recognition, IEEE Trans. Pattern Anal. Mach. Intell., 45, 2, pp. 1489-1500, (2023); Wang C.-Y., Bochkovskiy A., Liao H.-Y.M., YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors, Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., pp. 7464-7475, (2023); Rao Y., Zhao W., Tang Y., Zhou J., Lim S.N., Lu J., HorNet: Efficient high-order spatial interactions with recursive gated convolutions, Proc. Adv. Neural Inf. Process. Syst., 35, 2022, pp. 10353-10366; Liu Y., Shao Z., Hoffmann N., Global attention mechanism: Retain information to enhance channel-spatial interactions; Zhang Q.-L., Yang Y.-B., SA-Net: Shuffle attention for deep convolutional neural networks, Proc. IEEE Int. Conf. Acoust., Speech Signal Process. (ICASSP), pp. 2235-2239, (2021); Huang Z., Wang X., Huang L., Huang C., Wei Y., Liu W., CCNet: Criss-cross attention for semantic segmentation, Proc. IEEE/CVF Int. Conf. Comput. Vis. (ICCV), pp. 603-612, (2019); Yu T., Li X., Cai Y., Sun M., Li P., S2-MLP: Spatial-shift MLP architecture for vision, Proc. IEEE/CVF Winter Conf. Appl. Comput. Vis. (WACV), pp. 297-306, (2022); Li X., Wang W., Hu X., Yang J., Selective kernel networks, Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), pp. 510-519, (2019); Liu Y., Shao Z., Teng Y., Hoffmann N., NAM: Normalization-based attention module; Park J., Woo S., Lee J.-Y., Kweon I.S., BAM: Bottleneck attention module, (2018); Woo S., Park J., Lee J.-Y., Kweon I.S., CBAM: Convolutional block attention module, Proc. Eur. Conf. Comput. Vis. (ECCV), pp. 3-19, (2018); Hu J., Shen L., Albanie S., Sun G., Wu E., Squeeze-and-excitation networks, IEEE Trans. Pattern Anal. Mach. Intell., 42, 8, pp. 2011-2023, (2020); Gu R., Wang G., Song T., Huang R., Aertsen M., Deprest J., Ourselin S., Vercauteren T., Zhang S., CA-Net: Comprehensive attention convolutional neural networks for explainable medical image segmentation, IEEE Trans. Med. Imag., 40, 2, pp. 699-711, (2021); Wang Q., Wu B., Zhu P., Li P., Zuo W., Hu Q., ECA-Net: Efficient channel attention for deep convolutional neural networks, Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), pp. 11534-11542, (2020); Du Y., Zhao Z., Song Y., Zhao Y., Su F., Gong T., Meng H., StrongSORT: Make DeepSORT great again, IEEE Trans. Multimedia, 31; Bruls T., Porav H., Kunze L., Newman P., The right (angled) perspective: Improving the understanding of road scenes using boosted inverse perspective mapping, Proc. IEEE Intell. Vehicles Symp. (IV), pp. 302-309, (2019); Tanveer M.H., Sgorbissa A., An inverse perspective mapping approach using monocular camera of pepper humanoid robot to determine the position of other moving robot in plane, Proc. ICINCO, 2, pp. 229-235, (2018); Ljung M., Huang Y.-H., Aberg N., Johansson E., Close calls on the road—A study of drivers’ near-misses, Proc. 3rd Int. Conf. Traffic Transp. Psychol. (ICTTP), pp. 1-14, (2004); Xiong X., He Y., Gao X., Zhao Y., A multi-level risk framework for driving safety assessment based on vehicle trajectory, Promet, 34, 6, pp. 959-973; Chen Q., Huang H., Li Y., Lee J., Long K., Gu R., Zhai X., Modeling accident risks in different lane-changing behavioral patterns, Anal. Methods Accident Res., 30, 2021; Bokare P.S., Maurya A.K., Acceleration-deceleration behaviour of various vehicle types, Transp. Res. Proc., 25, pp. 4733-4749, (2017); Liu H., Li X., Zhou W., Chen Y., He Y., Xue H., Zhang W., Yu N., Spatial-phase shallow learning: Rethinking face forgery detection in frequency domain, Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), pp. 772-781, (2021)","A.S.A. Mohamed; Universiti Sains Malaysia, George Town, School of Computer Sciences, Penang, 11800, Malaysia; email: sufril@usm.my","","Institute of Electrical and Electronics Engineers Inc.","","","","","","21693536","","","","English","IEEE Access","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85166319801"
"Ruhaiyem N.I.R.; Mohamed A.S.A.; Belaton B.","Ruhaiyem, Nur Intan Raihana (57190964192); Mohamed, Ahmad Sufril Azlan (57190968285); Belaton, Bahari (6504014356)","57190964192; 57190968285; 6504014356","Optimized Segmentation of Cellular Tomography through Organelles' Morphology and Image Features","2016","Journal of Telecommunication, Electronic and Computer Engineering","8","3","","79","83","4","3","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84984846552&partnerID=40&md5=d733b548ab584b20283e495ece0bad7f","Universiti Sains Malaysia, 11800 USM Penang, Malaysia","Ruhaiyem N.I.R., Universiti Sains Malaysia, 11800 USM Penang, Malaysia; Mohamed A.S.A., Universiti Sains Malaysia, 11800 USM Penang, Malaysia; Belaton B., Universiti Sains Malaysia, 11800 USM Penang, Malaysia","Computational tracing of cellular images generally requires painstaking job in optimizing parameter(s). By incorporating prior knowledge about the organelle's morphology and image features, the required number of parameter tweaking can be reduced substantially. In practical applications, however, the general organelles' features are often known in advance, yet the actual organelles' morphology is not elaborated. Two primary contributions of this paper are firstly the classification of insulin granules based on its image features and morphology for accurate segmentation - mainly focused at pre-processing image segmentation and secondly the new hybrid meshing quantification is presented. The method proposed in this study is validated on a set of manually defined ground truths. The study of insulin granules in particular; the location, and its image features has also opened up other options for future studies.","","","","","","","","","Olofsson C.S., Salehi A., Holm C., Rorsman P., Palmitate increases l- Type ca2+ currents and the size of the readily releasable granule pool in mouse pancreatic beta- cells, Journal of Physiology, 557, pp. 935-948, (2004); Hutton J.C., The insulin secretory granule, Diabetologia, 32, pp. 271-281, (1989); Howell S.L., The molecular organization of the beta granule of the islets of langerhans, Advances in Cytopharmacology, 2, pp. 319-327, (1974); Ladinsky M.S., Wu C.C., Mcintosh S., Mcintosh J.R., Howell K.E., Structure of the golgi and distribution of reporter molecules at 20 degrees c reveals the complexity of the exit compartments, Molecular Biology of the Cell, 13, pp. 2810-2825, (2002); Marsh B.J., Lessons from tomographic studies of the mammalian golgi, Biochemical and Biophysics Acta, 1744, pp. 273-292, (2005); Russ J.C., Dehoff R.T., Practical Stereology, (2000); Derganc J., Mironov A.A., Svetina S., Physical factors that affect the number and size of golgi cisternae, Traffic, 7, pp. 85-96, (2006); Griffiths G., Fuller S.D., Back R., Hollinshead M., Pfeiffer S., Simons K., The dynamic nature of the golgi complex, Journal of Cell Biology, 108, pp. 277-297, (1989); Marsh B.J., Mastronarde D.N., Buttle K.F., Howell K.E., Mcintosh J.R., Organellar relationships in the golgi region of the pancreatic beta cell line, hit t15, visualized by high resolution electron tomography, Proceedings of the National Academy of Science U S a, 98, pp. 2399-2406, (2001); Ladinsky M.S., Mastronarde D.N., Mcintosh J.R., Howell K.E., Staehelin L.A., Golgi, structure in three dimensions: Functional insights from the normal rat kidney cell, Journal of Cell Biology, 144, pp. 1135-1149, (1999); Nurntan Raihana R., Multiple, Object-oriented Segmentation Methods of Mammalian Cell Tomograms, (2014); Rorsman P., Renstrom E., Insulin granule dynamics in pancreatic beta cells, Diabetologia, 46, pp. 1029-1045, (2003); Noske A.B., Costin A.J., Morgan G.P., Marsh B.J., Expedited approaches to whole cell electron tomography and organelle mark-up in situ in high-pressure frozen pancreatic islets, Journal of Structural, Biology, 161, pp. 298-313, (2008); Kremer J.R., Mastronarde D.N., Mcintosh J.R., Computer visualization of three-dimensional image data using imod, Journal of Structural Biology, 116, pp. 71-76, (1996); Nurntan Raihana R., Semi- Automated cellular tomogram segmentation workflow (ctsw): Towards an automaic target scoring system, The International Conference on Computer Graphics, Multimedia and Image Processing, (2014)","","","Universiti Teknikal Malaysia Melaka","","","","","","21801843","","","","English","J. Telecommun. Electron. Comput. Eng.","Article","Final","","Scopus","2-s2.0-84984846552"
"Lim L.M.; Ali M.K.M.; Ismail M.T.; Mohamed A.S.A.","Lim, Lek Ming (57915722400); Ali, Majid Khan Majahar (57190937034); Ismail, Mohd. Tahir (21742355400); Mohamed, Ahmad Sufril Azlan (57190968285)","57915722400; 57190937034; 21742355400; 57190968285","Data Safety Prediction Using Bird’s Eye View and Social Distancing Monitoring for Penang Roads","2022","Pertanika Journal of Science and Technology","30","4","","2563","2587","24","0","10.47836/pjst.30.4.15","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139217716&doi=10.47836%2fpjst.30.4.15&partnerID=40&md5=3e90ba808b7957f3b53b81b34fecf4b7","School of Mathematical Sciences, Universiti Sains Malaysia, Penang, 11800 USM, Malaysia; School of Computer Sciences, Universiti Sains Malaysia, Penang, 11800 USM, Malaysia","Lim L.M., School of Mathematical Sciences, Universiti Sains Malaysia, Penang, 11800 USM, Malaysia; Ali M.K.M., School of Mathematical Sciences, Universiti Sains Malaysia, Penang, 11800 USM, Malaysia; Ismail M.T., School of Mathematical Sciences, Universiti Sains Malaysia, Penang, 11800 USM, Malaysia; Mohamed A.S.A., School of Computer Sciences, Universiti Sains Malaysia, Penang, 11800 USM, Malaysia","In terms of fatalities, Malaysia ranks third among ASEAN countries. Every year, there is an increase in accidents and fatalities. The state of the road is one factor contributing to near misses. A near miss is an almost-caused accident, an unplanned situation that could result in injury or accidents. The Majlis Bandar Pulau Pinang (MBPP) has installed 1841 closed-circuit television (CCTV) cameras around Penang to monitor traffic and track near miss incidents. When installing CCTVs, the utilisation of video allows resources to be used and optimised in situations when maintaining video memories is difficult and costly. Highways, industrial regions, and city roads are the most typical places where accidents occur. Accidents occurred at 200 per year on average in Penang from 2015 to 2017. Near misses are what create accidents. One of the essential factors in vehicle detection is the “near miss.” In this study, You Only Look Once version 3 (YOLOv3) and Faster Region-based Convolutional Neural Network (Faster RCNN) are used to solve transportation issues. In vehicle detection, a faster RCNN was used. Bird’s Eye View and Social Distancing Monitoring are used to detect the only vehicle in image processing and observe how near misses occur. This experiment tests different video quality and lengths to compare test time and error detection percentage. In conclusion, YOLOv3 outperforms Faster RCNN. In high-resolution videos, Faster RCNN outperforms YOLOv3, while in low-resolution videos, YOLOv3 outperforms Faster RCNN. © Universiti Putra Malaysia Press.","Bird’s eye view; near miss; social distancing monitoring vehicle detection","","","","","","","","Albelwi S., Mahmood A., A framework for designing the architectures of deep convolutional neural networks, Entropy, 19, 242, pp. 1-20, (2017); Aldred R., Cycling near misses: Their frequency, impact and prevention, Transport Research Part A, 90, 1, pp. 69-83, (2016); Aldred R., Crosweller S., Investigating the rates and impacts of near misses and related incidents among UK cyclists, Journal of Transport & Health, 2, 3, pp. 379-393, (2015); Alganci U., Soydas M., Sertel E., Comparative research on deep learning approaches for airplane detection from very high-resolution satellite images, Remote sensing, 12, 3, (2020); Aqqa M., Mantini P., Shah S. K., Understanding how video quality affects object detection algorithms, VISIGRAPP (5: VISAPP), pp. 96-104, (2019); Arinaldi A., Pradana J., Gurusniaga A., Detection and classification of vehicles for traffic video analytics, Procedia Computer Science, 144, pp. 259-268, (2018); Behl A., Bhatia A., Puri A., Convolution and applications of convolution, International Journal of Innovative Research in Technology (IJIRT), 1, 6, pp. 2123-2126, (2014); Bull C., Hagen B., Lubin A., Shivaraman G., Chibbaro D., Predictable is preventable: Tracking pedestrian near-miss incidents, (2017); Calles M., Nelson T., Winters M., Comparing crowdsourced near-miss and collision cycling data and official bike safety reporting, Transportation Research Record: Journal of the Transportation Research Board, 2662, 1, pp. 1-11, (2017); Cao C., Wang B., Zhang W., Zeng X., Yan X., Feng Z., Liu Y., Wu Z., An improved faster R-CNN for small object detection, IEEE Access, 7, pp. 106838-106846, (2019); Cepni S., Atik M. E., Druan Z., Vehicle detection using different deep learning algorithms from image sequence, Baltic Journal of Modern Computing, 8, 2, pp. 347-358, (2020); Ciberlin J., Grbic R., Teslic N., Pilipovic M., Object detection and object tracking in front of the vehicle using front view camera, 2019 Zooming Innovation in Consumer Technologies Conference (ZINC), pp. 27-32, (2019); Ding X., Yang R., Vehicle and parking space detection based on improved YOLO network model, Journal of Physics: Conference Series, 1325, (2019); Dixit K. S., Chadaga M. G., Savalgimath S. S., Rakshith G. R., Kumar M. N., Evaluation and evolution of object detection techniques YOLO and R-CNN, International Journal of Recent Technology and Engineering (IJRTE), 8, 2S3, pp. 824-829, (2019); Gad A. F., Faster R-CNN explained for object detection tasks, PaperspaceBlog, (2020); Girotto E., Andrade S., Gonzalez A. D., Professional experience and traffic accidents/ near-miss accidents among truck drivers, ELSEVIER: Accidents Analysis and Prevention, 95, pp. 299-304, (2016); Huang Y. Q., Zheng J. C., Sun S. D., Yang C. F., Liu J., Optimized YOLOv3 algorithm and its application in traffic flow detections, Applied Sciences, 10, 9, (2020); Johnson K. D., Patel S. R., Baur D. M., Edens E., Sherry P., Malhotra A., Kales S., Association of sleep habits with accidents and near misses in United States transportation operators, Journal of Occupational & Environmental Medicine, 56, 5, pp. 510-515, (2014); Kataoka H., Suzuki T., Oikawa S., Matsui Y., Satoh Y., Drive video analysis for the detection of traffic near-miss incidents, IEEE International Conference on Robotics and Automation (ICRA), pp. 3421-3428, (2018); Ke R., Lutin J., Spears J., Wang Y., A cost-effective framework for automated vehicle-pedestrian near-miss detection through onboard monocular vision, IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), pp. 25-32, (2017); Mahdi N. N. R. N., Bachok N., Mohamed N., Shafei M. N., Risk factors for near miss incident among long distance bus drivers in Malaysia, Iranian Journal of Public Health, 43, 3, pp. 117-124, (2014); Makizako H., Shimada H., Hotta R., Doi T., Tsutsumimoto K., Nakakubo S., Makino K., Associations of Near-Miss traffic incidents with attention and executive function among older Japanese drivers, Gerontology, 64, pp. 495-502, (2018); Matsui Y., Hitosugi M., Doi T., Oikawa S., Takahashi K., Ando K., Features of pedestrian behavior in car-to-pedestrian contact situations in Near-Miss incidents in Japan, Traffic Injury Prevention, 14, 1, pp. 58-63, (2013); Matsui Y., Takahashi K., Imaizumi R., Ando K., Car-to-pedestrian contact situations in near-miss incidents and real-world accidents in Japan, 22nd International Technical Conference on the Enhanced Safety of Vehicles (No. 110164), (2011); Nadai S. D., Parodi F., Pizzorni D., A system of systems to Near Miss accidents in dangerous goods road transportation, IEEE 2012 7th International Conference on System of Systems Engineering (SoSE), pp. 219-222, (2012); Nostikasari D., Shelton K., Learning from close calls: A glimpse into near-miss experiences, (2017); Ong Y., Near miss vehicle collisions estimation using YOLO, (2020); Poulos R., Hatfield J., Rissel C., Flack L., Shaw L., Grzebieta R., McIntosh A., Near miss experiences of transport and recreational cyclists in New South Wales, Australia. Findings from a prospective cohort study, Accidents Analysis and Prevention, 101, pp. 143-153, (2017); Qin Z., Yu F., Liu C., Chen X., How convolutional neural networks see the world-A survey of convolutional neural network visualization methods, Mathematical Foundations Computing, 1, 2, pp. 149-180, (2018); Rahman A., Salam A., Islam M., Sarker P., An image based approach to compute object distance, International Journal of Computational Intelligence Systems, 1, 4, pp. 304-312, (2012); Rawat W., Wang Z., Deep convolutional neural networks for image classification: A comprehensive review, Neural Computation, 29, 9, pp. 2352-2449, (2017); Rome L. D., Brown J., Baldock M., Fitzharris M., Near-miss crashes and other predictors of motorcycle crashes: Findings from a population-based survey, Traffic Injury Prevention, 19, 2, pp. S20-S26, (2018); Sanders R., Perceived traffic risk for cyclists: The impact of near miss and collision experiences, Accidents Analysis and Prevention, 75, pp. 26-34, (2015); Using near miss reporting in security, (2016); Siregar M. L., Agah H. R., Hidayatullah F., Near-miss accident analysis for traffic safety improvement at a “channelized” junction with U-turn, International Journal of Safety and Security Engineering, 8, 1, pp. 31-38, (2018); Sonnleitner E., Barth O., Palmanshofer A., Kurz M., Traffic measurement and congestion detection based on real-time highway video data, Applied Sciences, 10, 18, (2020); Srivastava S., Divekar A., Anilkumar C., Naik I., Kulkarni V., Pattabiraman V., Comparative analysis of deep learning image detection algorithms, Journal of Big Data volume, 8, 66, pp. 1-31, (2021); Storgard J., Erdogan I., Lappalainen J., Tapanien U., Developing incident and near miss reporting in the maritime industry-A case study on the Baltic Sea, Procedia Social and Behavioral Sciences, 48, pp. 1010-1021, (2012); Uchida N., Kawakoshi M., Tagawa T., Mochida T., An investigation of factors contributing to major crash types in Japan based on naturalistic driving data, International Association of Traffic and Safety Sciences (IATSS), 34, 1, pp. 22-30, (2010); Vinitha V., Velantina V., Social distancing detection system with artificial intelligence using computer vision and deep learning, International Research Journal of Engineering and Technology (IRJET), 7, 8, pp. 4049-4053, (2020); Wang C., Dai Y., Zhou W., Geng Y., A vision-based video crash detection framework for mixed traffic flow environment considering low-visibility condition, Hindawi, Journal of Advanced Transportation, 2020, (2020); World Health Statistics 2015, (2015); Zhang Z., Trivedi C., Liu X., Automated detection of grade-crossing-trespassing near misses based on computer vision analysis of surveillance video data, Safety Science, 110, pp. 276-285, (2018); Zohra A., Kamilia S., Souad S., Detection and classification of vehicles using deep learning, International Journal of Computer Science Trends and Technology (IJCST), 6, 3, pp. 23-29, (2018)","M.K.M. Ali; School of Mathematical Sciences, Universiti Sains Malaysia, Penang, 11800 USM, Malaysia; email: majidkhanmajaharali@usm.my","","Universiti Putra Malaysia Press","","","","","","01287680","","","","English","Pertanika J. Sci. Technol.","Article","Final","All Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85139217716"
